diff --git a/numpy/core/einsumfunc.py b/numpy/core/einsumfunc.py
index e242363a486..1ea3e598c0e 100644
--- a/numpy/core/einsumfunc.py
+++ b/numpy/core/einsumfunc.py
@@ -5,7 +5,7 @@
 from __future__ import division, absolute_import, print_function
 
 from numpy.core.multiarray import c_einsum
-from numpy.core.numeric import asarray, asanyarray, result_type
+from numpy.core.numeric import asarray, asanyarray, result_type, tensordot, dot
 
 __all__ = ['einsum', 'einsum_path']
 
@@ -166,8 +166,14 @@ def _optimal_path(input_sets, output_set, idx_dict, memory_limit):
                 new_pos = positions + [con]
                 iter_results.append((new_cost, new_pos, new_input_sets))
 
-        # Update list to iterate over
-        full_results = iter_results
+        # Update combinatorial list, if we did not find anything return best
+        # path + remaining contractions
+        if iter_results:
+            full_results = iter_results
+        else:
+            path = min(full_results, key=lambda x: x[0])[1]
+            path += [tuple(range(len(input_sets) - iteration))]
+            return path
 
     # If we have not found anything return single einsum contraction
     if len(full_results) == 0:
@@ -256,6 +262,114 @@ def _greedy_path(input_sets, output_set, idx_dict, memory_limit):
     return path
 
 
+def _can_dot(inputs, result, idx_removed):
+    """
+    Checks if we can use BLAS (np.tensordot) call and its beneficial to do so.
+
+    Parameters
+    ----------
+    inputs : list of str
+        Specifies the subscripts for summation.
+    result : str
+        Resulting summation.
+    idx_removed : set
+        Indices that are removed in the summation
+
+
+    Returns
+    -------
+    type : bool
+        Returns true if BLAS should and can be used, else False
+
+    Notes
+    -----
+    If the operations is BLAS level 1 or 2 and is not already aligned
+    we default back to einsum as the memory movement to copy is more
+    costly than the operation itself.
+
+
+    Examples
+    --------
+
+    # Standard GEMM operation
+    >>> _can_dot(['ij', 'jk'], 'ik', set('j'))
+    True
+
+    # Can use the standard BLAS, but requires odd data movement
+    >>> _can_dot(['ijj', 'jk'], 'ik', set('j'))
+    False
+
+    # DDOT where the memory is not aligned
+    >>> _can_dot(['ijk', 'ikj'], '', set('ijk'))
+    False
+
+    """
+
+    # All `dot` calls remove indices
+    if len(idx_removed) == 0:
+        return False
+
+    # BLAS can only handle two operands
+    if len(inputs) != 2:
+        return False
+
+    # Build a few temporaries
+    input_left, input_right = inputs
+    set_left = set(input_left)
+    set_right = set(input_right)
+    keep_left = set_left - idx_removed
+    keep_right = set_right - idx_removed
+    rs = len(idx_removed)
+
+    # Indices must overlap between the two operands
+    if not len(set_left & set_right):
+        return False
+
+    # We cannot have duplicate indices ("ijj, jk -> ik")
+    if (len(set_left) != len(input_left)) or (len(set_right) != len(input_right)):
+        return False
+
+    # Cannot handle partial inner ("ij, ji -> i")
+    if len(keep_left & keep_right):
+        return False
+
+    # At this point we are a DOT, GEMV, or GEMM operation
+
+    # Handle inner products
+
+    # DDOT with aligned data
+    if input_left == input_right:
+        return True
+
+    # DDOT without aligned data (better to use einsum)
+    if set_left == set_right:
+        return False
+
+    # Handle the 4 possible (aligned) GEMV or GEMM cases
+
+    # GEMM or GEMV no transpose
+    if input_left[-rs:] == input_right[:rs]:
+        return True
+
+    # GEMM or GEMV transpose both
+    if input_left[:rs] == input_right[-rs:]:
+        return True
+
+    # GEMM or GEMV transpose right
+    if input_left[-rs:] == input_right[-rs:]:
+        return True
+
+    # GEMM or GEMV transpose left
+    if input_left[:rs] == input_right[:rs]:
+        return True
+
+    # Einsum is faster than GEMV if we have to copy data
+    if not keep_left or not keep_right:
+        return False
+
+    # We are a matrix-matrix product, but we need to copy data
+    return True
+
 def _parse_einsum_input(operands):
     """
     A reproduction of einsum c side einsum parsing in python.
@@ -542,7 +656,7 @@ def einsum_path(*operands, **kwargs):
                         " %s" % unknown_kwargs)
 
     # Figure out what the path really is
-    path_type = kwargs.pop('optimize', False)
+    path_type = kwargs.pop('optimize', True)
     if path_type is True:
         path_type = 'greedy'
     if path_type is None:
@@ -653,6 +767,8 @@ def einsum_path(*operands, **kwargs):
         for x in contract_inds:
             tmp_inputs.append(input_list.pop(x))
 
+        do_blas = _can_dot(tmp_inputs, out_inds, idx_removed)
+
         # Last contraction
         if (cnum - len(path)) == -1:
             idx_result = output_subscript
@@ -663,7 +779,7 @@ def einsum_path(*operands, **kwargs):
         input_list.append(idx_result)
         einsum_str = ",".join(tmp_inputs) + "->" + idx_result
 
-        contraction = (contract_inds, idx_removed, einsum_str, input_list[:])
+        contraction = (contract_inds, idx_removed, einsum_str, input_list[:], do_blas)
         contraction_list.append(contraction)
 
     opt_cost = sum(cost_list) + 1
@@ -690,7 +806,7 @@ def einsum_path(*operands, **kwargs):
     path_print += "-" * 74
 
     for n, contraction in enumerate(contraction_list):
-        inds, idx_rm, einsum_str, remaining = contraction
+        inds, idx_rm, einsum_str, remaining, blas = contraction
         remaining_str = ",".join(remaining) + "->" + output_subscript
         path_run = (scale_list[n], einsum_str, remaining_str)
         path_print += "\n%4d    %24s %40s" % path_run
@@ -748,7 +864,7 @@ def einsum(*operands, **kwargs):
         Controls if intermediate optimization should occur. No optimization
         will occur if False and True will default to the 'greedy' algorithm.
         Also accepts an explicit contraction list from the ``np.einsum_path``
-        function. See ``np.einsum_path`` for more details. Default is False.
+        function. See ``np.einsum_path`` for more details. Default is True.
 
     Returns
     -------
@@ -969,19 +1085,54 @@ def einsum(*operands, **kwargs):
     # Build the contraction list and operand
     operands, contraction_list = einsum_path(*operands, optimize=optimize_arg,
                                              einsum_call=True)
+
+    handle_out = False
+
     # Start contraction loop
     for num, contraction in enumerate(contraction_list):
-        inds, idx_rm, einsum_str, remaining = contraction
+        inds, idx_rm, einsum_str, remaining, blas = contraction
         tmp_operands = []
         for x in inds:
             tmp_operands.append(operands.pop(x))
 
-        # If out was specified
+        # Do we need to deal with the output?
         if specified_out and ((num + 1) == len(contraction_list)):
-            einsum_kwargs["out"] = out_array
+            handle_out = True
+
+        # Call tensordot
+        if blas:
+
+            # Checks have already been handled
+            input_str, results_index = einsum_str.split('->')
+            input_left, input_right = input_str.split(',')
+
+            tensor_result = input_left + input_right
+            for s in idx_rm:
+                tensor_result = tensor_result.replace(s, "")
+
+            # Find indices to contract over
+            left_pos, right_pos = [], []
+            for s in idx_rm:
+                left_pos.append(input_left.find(s))
+                right_pos.append(input_right.find(s))
+
+            # Contract!
+            new_view = tensordot(*tmp_operands, axes=(tuple(left_pos), tuple(right_pos)))
+
+            # Build a new view if needed
+            if (tensor_result != results_index) or handle_out:
+                if handle_out:
+                    einsum_kwargs["out"] = out_array
+                new_view = c_einsum(tensor_result + '->' + results_index, new_view, **einsum_kwargs)
+
+        # Call einsum
+        else:
+            # If out was specified
+            if handle_out:
+                einsum_kwargs["out"] = out_array
 
-        # Do the contraction
-        new_view = c_einsum(einsum_str, *tmp_operands, **einsum_kwargs)
+            # Do the contraction
+            new_view = c_einsum(einsum_str, *tmp_operands, **einsum_kwargs)
 
         # Append new items and derefernce what we can
         operands.append(new_view)
diff --git a/numpy/core/numeric.py b/numpy/core/numeric.py
index 6b1c6e86e0a..cd71c5719f6 100644
--- a/numpy/core/numeric.py
+++ b/numpy/core/numeric.py
@@ -1323,7 +1323,7 @@ def tensordot(a, b, axes=2):
     N2 = 1
     for axis in axes_a:
         N2 *= as_[axis]
-    newshape_a = (-1, N2)
+    newshape_a = (int(multiply.reduce([as_[ax] for ax in notin])), N2)
     olda = [as_[axis] for axis in notin]
 
     notin = [k for k in range(ndb) if k not in axes_b]
@@ -1331,7 +1331,7 @@ def tensordot(a, b, axes=2):
     N2 = 1
     for axis in axes_b:
         N2 *= bs[axis]
-    newshape_b = (N2, -1)
+    newshape_b = (N2, int(multiply.reduce([bs[ax] for ax in notin])))
     oldb = [bs[axis] for axis in notin]
 
     at = a.transpose(newaxes_a).reshape(newshape_a)
diff --git a/numpy/core/tests/test_einsum.py b/numpy/core/tests/test_einsum.py
index 8466d924eda..0e4251775da 100644
--- a/numpy/core/tests/test_einsum.py
+++ b/numpy/core/tests/test_einsum.py
@@ -568,48 +568,37 @@ def test_einsum_broadcast(self):
 
         A = np.arange(2 * 3 * 4).reshape(2, 3, 4)
         B = np.arange(3)
-        ref = np.einsum('ijk,j->ijk', A, B)
-        assert_equal(np.einsum('ij...,j...->ij...', A, B), ref)
-        assert_equal(np.einsum('ij...,...j->ij...', A, B), ref)
-        assert_equal(np.einsum('ij...,j->ij...', A, B), ref)  # used to raise error
-
-        assert_equal(np.einsum('ij...,j...->ij...', A, B, optimize=True), ref)
-        assert_equal(np.einsum('ij...,...j->ij...', A, B, optimize=True), ref)
-        assert_equal(np.einsum('ij...,j->ij...', A, B, optimize=True), ref)  # used to raise error
+        ref = np.einsum('ijk,j->ijk', A, B, optimize=False)
+        for opt in [True, False]:
+            assert_equal(np.einsum('ij...,j...->ij...', A, B, optimize=opt), ref)
+            assert_equal(np.einsum('ij...,...j->ij...', A, B, optimize=opt), ref)
+            assert_equal(np.einsum('ij...,j->ij...', A, B, optimize=opt), ref)  # used to raise error
 
         A = np.arange(12).reshape((4, 3))
         B = np.arange(6).reshape((3, 2))
-        ref = np.einsum('ik,kj->ij', A, B)
-        assert_equal(np.einsum('ik...,k...->i...', A, B), ref)
-        assert_equal(np.einsum('ik...,...kj->i...j', A, B), ref)
-        assert_equal(np.einsum('...k,kj', A, B), ref)  # used to raise error
-        assert_equal(np.einsum('ik,k...->i...', A, B), ref)  # used to raise error
-
-        assert_equal(np.einsum('ik...,k...->i...', A, B, optimize=True), ref)
-        assert_equal(np.einsum('ik...,...kj->i...j', A, B, optimize=True), ref)
-        assert_equal(np.einsum('...k,kj', A, B, optimize=True), ref)  # used to raise error
-        assert_equal(np.einsum('ik,k...->i...', A, B, optimize=True), ref)  # used to raise error
+        ref = np.einsum('ik,kj->ij', A, B, optimize=False)
+        for opt in [True, False]:
+            assert_equal(np.einsum('ik...,k...->i...', A, B, optimize=opt), ref)
+            assert_equal(np.einsum('ik...,...kj->i...j', A, B, optimize=opt), ref)
+            assert_equal(np.einsum('...k,kj', A, B, optimize=opt), ref)  # used to raise error
+            assert_equal(np.einsum('ik,k...->i...', A, B, optimize=opt), ref)  # used to raise error
 
         dims = [2, 3, 4, 5]
         a = np.arange(np.prod(dims)).reshape(dims)
         v = np.arange(dims[2])
-        ref = np.einsum('ijkl,k->ijl', a, v)
-        assert_equal(np.einsum('ijkl,k', a, v), ref)
-        assert_equal(np.einsum('...kl,k', a, v), ref)  # used to raise error
-        assert_equal(np.einsum('...kl,k...', a, v), ref)
-        # no real diff from 1st
-
-        assert_equal(np.einsum('ijkl,k', a, v, optimize=True), ref)
-        assert_equal(np.einsum('...kl,k', a, v, optimize=True), ref)  # used to raise error
-        assert_equal(np.einsum('...kl,k...', a, v, optimize=True), ref)
+        ref = np.einsum('ijkl,k->ijl', a, v, optimize=False)
+        for opt in [True, False]:
+            assert_equal(np.einsum('ijkl,k', a, v, optimize=opt), ref)
+            assert_equal(np.einsum('...kl,k', a, v, optimize=opt), ref)  # used to raise error
+            assert_equal(np.einsum('...kl,k...', a, v, optimize=opt), ref)
 
         J, K, M = 160, 160, 120
         A = np.arange(J * K * M).reshape(1, 1, 1, J, K, M)
         B = np.arange(J * K * M * 3).reshape(J, K, M, 3)
-        ref = np.einsum('...lmn,...lmno->...o', A, B)
-        assert_equal(np.einsum('...lmn,lmno->...o', A, B), ref)  # used to raise error
-        assert_equal(np.einsum('...lmn,lmno->...o', A, B,
-                               optimize=True), ref)  # used to raise error
+        ref = np.einsum('...lmn,...lmno->...o', A, B, optimize=False)
+        for opt in [True, False]:
+            assert_equal(np.einsum('...lmn,lmno->...o', A, B,
+                                   optimize=opt), ref)  # used to raise error
 
     def test_einsum_fixedstridebug(self):
         # Issue #4485 obscure einsum bug
@@ -784,7 +773,7 @@ def build_operands(self, string):
         operands = [string]
         terms = string.split('->')[0].split(',')
         for term in terms:
-            dims = [global_size_dict[x] for x in term]
+            dims = [size_dict[x] for x in term]
             operands.append(np.random.rand(*dims))
 
         return operands
@@ -874,6 +863,16 @@ def test_edge_paths(self):
         path, path_str = np.einsum_path(*edge_test4, optimize='optimal')
         self.assert_path_equal(path, ['einsum_path', (1, 2), (0, 2), (0, 1)])
 
+        # Edge test5
+        edge_test4 = self.build_operands('a,ac,ab,ad,cd,bd,bc->',
+                                         size_dict={"a": 20, "b": 20, "c": 20, "d": 20})
+        path, path_str = np.einsum_path(*edge_test4, optimize='greedy')
+        self.assert_path_equal(path, ['einsum_path', (0, 1), (0, 1, 2, 3, 4, 5)])
+
+        path, path_str = np.einsum_path(*edge_test4, optimize='optimal')
+        self.assert_path_equal(path, ['einsum_path', (0, 1), (0, 1, 2, 3, 4, 5)])
+
+
     def test_path_type_input(self):
         # Test explicit path handeling
         path_test = self.build_operands('dcc,fce,ea,dbf->ab')
