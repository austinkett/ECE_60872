diff --git a/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java b/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
index a02020d277be..8bf8620c0703 100644
--- a/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
+++ b/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
@@ -20,12 +20,15 @@
 package org.elasticsearch.index.snapshots.blobstore;
 
 import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.RateLimiter;
+import org.apache.lucene.store.*;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.cluster.metadata.SnapshotId;
@@ -49,13 +52,11 @@
 import org.elasticsearch.indices.recovery.RecoveryState;
 import org.elasticsearch.repositories.RepositoryName;
 
+import java.io.ByteArrayOutputStream;
 import java.io.FilterInputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicInteger;
@@ -426,6 +427,7 @@ public void snapshot(SnapshotIndexCommit snapshotIndexCommit) {
                 long indexTotalFilesSize = 0;
                 ArrayList<FileInfo> filesToSnapshot = newArrayList();
                 final Store.MetadataSnapshot metadata;
+                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should
                 try {
                     metadata = store.getMetadata(snapshotIndexCommit);
                 } catch (IOException e) {
@@ -445,7 +447,15 @@ public void snapshot(SnapshotIndexCommit snapshotIndexCommit) {
     //            }
 
                     BlobStoreIndexShardSnapshot.FileInfo fileInfo = snapshots.findPhysicalIndexFile(fileName);
-
+                    try {
+                        // in 1.3.3 we added additional hashes for .si / segments_N files
+                        // to ensure we don't double the space in the repo since old snapshots
+                        // don't have this hash we try to read that hash from the blob store
+                        // in a bwc compatible way.
+                        maybeRecalculateMetadataHash(blobContainer, fileInfo, metadata);
+                    }  catch (Throwable e) {
+                        logger.warn("{} Can't calculate hash from blob for file [{}] [{}]", e, shardId, fileInfo.physicalName(), fileInfo.metadata());
+                    }
                     if (fileInfo == null || !fileInfo.isSame(md) || !snapshotFileExistsInBlobs(fileInfo, blobs)) {
                         // commit point file does not exists in any commit point, or has different length, or does not fully exists in the listed blobs
                         snapshotRequired = true;
@@ -635,6 +645,77 @@ private void checkAborted() {
         }
     }
 
+    /**
+     * This is a BWC layer to ensure we update the snapshots metdata with the corresponding hashes before we compare them.
+     * The new logic for StoreFileMetaData reads the entire <tt>.si</tt> and <tt>segments.n</tt> files to strengthen the
+     * comparison of the files on a per-segment / per-commit level.
+     */
+    private static final void maybeRecalculateMetadataHash(final ImmutableBlobContainer blobContainer, final FileInfo fileInfo, Store.MetadataSnapshot snapshot) throws Throwable {
+        final StoreFileMetaData metadata;
+        if (fileInfo != null && (metadata = snapshot.get(fileInfo.physicalName())) != null) {
+            if (metadata.hash().length > 0 && fileInfo.metadata().hash().length == 0) {
+                // we have a hash - check if our repo has a hash too otherwise we have
+                // to calculate it.
+                final ByteArrayOutputStream out = new ByteArrayOutputStream();
+                final CountDownLatch latch = new CountDownLatch(1);
+                final CopyOnWriteArrayList<Throwable> failures = new CopyOnWriteArrayList<>();
+                // we might have multiple parts even though the file is small... make sure we read all of it.
+                // TODO this API should really support a stream!
+                blobContainer.readBlob(fileInfo.partName(0), new BlobContainer.ReadBlobListener() {
+                    final AtomicInteger partIndex = new AtomicInteger();
+                    @Override
+                    public synchronized void onPartial(byte[] data, int offset, int size) throws IOException {
+                        out.write(data, offset, size);
+                    }
+
+                    @Override
+                    public synchronized void onCompleted() {
+                        boolean countDown = true;
+                        try {
+                            final int part = partIndex.incrementAndGet();
+                            if (part < fileInfo.numberOfParts()) {
+                                final String partName = fileInfo.partName(part);
+                                // continue with the new part
+                                blobContainer.readBlob(partName, this);
+                                countDown = false;
+                                return;
+                            }
+                        } finally {
+                            if (countDown) {
+                                latch.countDown();
+                            }
+                        }
+                    }
+
+                    @Override
+                    public void onFailure(Throwable t) {
+                        try {
+                            failures.add(t);
+                        } finally {
+                            latch.countDown();
+                        }
+                    }
+                });
+
+                try {
+                    latch.await();
+                } catch (InterruptedException e) {
+                    Thread.interrupted();
+                }
+
+                if (!failures.isEmpty()) {
+                    ExceptionsHelper.rethrowAndSuppress(failures);
+                }
+
+                final byte[] bytes = out.toByteArray();
+                assert bytes != null;
+                assert bytes.length == fileInfo.length() : bytes.length + " != " + fileInfo.length();
+                final BytesRef spare = new BytesRef(bytes);
+                Store.MetadataSnapshot.hashFile(fileInfo.metadata().hash(), spare);
+            }
+        }
+    }
+
     /**
      * Context for restore operations
      */
@@ -672,9 +753,9 @@ public void restore() {
                 long totalSize = 0;
                 int numberOfReusedFiles = 0;
                 long reusedTotalSize = 0;
-                Map<String, StoreFileMetaData> metadata = Collections.emptyMap();
+                Store.MetadataSnapshot recoveryTargetMetadata = Store.MetadataSnapshot.EMPTY;
                 try {
-                    metadata = store.getMetadata().asMap();
+                    recoveryTargetMetadata = store.getMetadata();
                 } catch (CorruptIndexException e) {
                     logger.warn("{} Can't read metadata from store", e, shardId);
                     throw new IndexShardRestoreFailedException(shardId, "Can't restore corrupted shard", e);
@@ -683,33 +764,51 @@ public void restore() {
                     logger.warn("{} Can't read metadata from store", e, shardId);
                 }
 
-                List<FileInfo> filesToRecover = Lists.newArrayList();
-                for (FileInfo fileInfo : snapshot.indexFiles()) {
-                    String fileName = fileInfo.physicalName();
-                    final StoreFileMetaData md = metadata.get(fileName);
+                final List<FileInfo> filesToRecover = Lists.newArrayList();
+                final Map<String, StoreFileMetaData> snapshotMetaData = new HashMap<>();
+                final Map<String, FileInfo> fileInfos = new HashMap<>();
+                for (final FileInfo fileInfo : snapshot.indexFiles()) {
+                    try {
+                        // in 1.3.3 we added additional hashes for .si / segments_N files
+                        // to ensure we don't double the space in the repo since old snapshots
+                        // don't have this hash we try to read that hash from the blob store
+                        // in a bwc compatible way.
+                        maybeRecalculateMetadataHash(blobContainer, fileInfo, recoveryTargetMetadata);
+                    }  catch (Throwable e) {
+                        // if the index is broken we might not be able to read it
+                        logger.warn("{} Can't calculate hash from blog for file [{}] [{}]", e, shardId, fileInfo.physicalName(), fileInfo.metadata());
+                    }
+                    snapshotMetaData.put(fileInfo.metadata().name(), fileInfo.metadata());
+                    fileInfos.put(fileInfo.metadata().name(), fileInfo);
+                }
+                final Store.MetadataSnapshot sourceMetaData = new Store.MetadataSnapshot(snapshotMetaData);
+                final Store.RecoveryDiff diff = sourceMetaData.recoveryDiff(recoveryTargetMetadata);
+                for (StoreFileMetaData md : diff.identical) {
+                    FileInfo fileInfo = fileInfos.get(md.name());
                     numberOfFiles++;
-                    if (md != null && fileInfo.isSame(md)) {
-                        totalSize += md.length();
-                        numberOfReusedFiles++;
-                        reusedTotalSize += md.length();
-                        recoveryState.getIndex().addReusedFileDetail(fileInfo.name(), fileInfo.length());
-                        if (logger.isTraceEnabled()) {
-                            logger.trace("not_recovering [{}], exists in local store and is same", fileInfo.physicalName());
-                        }
-                    } else {
-                        totalSize += fileInfo.length();
-                        filesToRecover.add(fileInfo);
-                        recoveryState.getIndex().addFileDetail(fileInfo.name(), fileInfo.length());
-                        if (logger.isTraceEnabled()) {
-                            if (md == null) {
-                                logger.trace("recovering [{}], does not exists in local store", fileInfo.physicalName());
-                            } else {
-                                logger.trace("recovering [{}], exists in local store but is different", fileInfo.physicalName());
-                            }
-                        }
+                    totalSize += md.length();
+                    numberOfReusedFiles++;
+                    reusedTotalSize += md.length();
+                    recoveryState.getIndex().addReusedFileDetail(fileInfo.name(), fileInfo.length());
+                    if (logger.isTraceEnabled()) {
+                        logger.trace("[{}] [{}] not_recovering [{}] from [{}], exists in local store and is same", shardId, snapshotId, fileInfo.physicalName(), fileInfo.name());
                     }
                 }
 
+                for (StoreFileMetaData md : Iterables.concat(diff.different, diff.missing)) {
+                    FileInfo fileInfo = fileInfos.get(md.name());
+                    numberOfFiles++;
+                    totalSize += fileInfo.length();
+                    filesToRecover.add(fileInfo);
+                    recoveryState.getIndex().addFileDetail(fileInfo.name(), fileInfo.length());
+                    if (logger.isTraceEnabled()) {
+                        if (md == null) {
+                            logger.trace("[{}] [{}] recovering [{}] from [{}], does not exists in local store", shardId, snapshotId, fileInfo.physicalName(), fileInfo.name());
+                        } else {
+                            logger.trace("[{}] [{}] recovering [{}] from [{}], exists in local store but is different", shardId, snapshotId, fileInfo.physicalName(), fileInfo.name());
+                        }
+                    }
+                }
                 final RecoveryState.Index index = recoveryState.getIndex();
                 index.totalFileCount(numberOfFiles);
                 index.totalByteCount(totalSize);
diff --git a/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java b/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
index e50a674ad865..1b78242aee8e 100644
--- a/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
+++ b/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
@@ -20,10 +20,12 @@
 package org.elasticsearch.index.snapshots.blobstore;
 
 import com.google.common.collect.ImmutableList;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.xcontent.ToXContent;
@@ -196,6 +198,7 @@ public boolean isSame(StoreFileMetaData md) {
             static final XContentBuilderString CHECKSUM = new XContentBuilderString("checksum");
             static final XContentBuilderString PART_SIZE = new XContentBuilderString("part_size");
             static final XContentBuilderString WRITTEN_BY = new XContentBuilderString("written_by");
+            static final XContentBuilderString META_HASH = new XContentBuilderString("meta_hash");
         }
 
         /**
@@ -221,6 +224,10 @@ public static void toXContent(FileInfo file, XContentBuilder builder, ToXContent
             if (file.metadata.writtenBy() != null) {
                 builder.field(Fields.WRITTEN_BY, file.metadata.writtenBy());
             }
+
+            if (file.metadata.hash() != null && file.metadata().hash().length > 0) {
+                builder.field(Fields.META_HASH, new BytesArray(file.metadata.hash()));
+            }
             builder.endObject();
         }
 
@@ -239,6 +246,7 @@ public static FileInfo fromXContent(XContentParser parser) throws IOException {
             String checksum = null;
             ByteSizeValue partSize = null;
             Version writtenBy = null;
+            BytesRef metaHash = new BytesRef();
             if (token == XContentParser.Token.START_OBJECT) {
                 while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                     if (token == XContentParser.Token.FIELD_NAME) {
@@ -257,6 +265,10 @@ public static FileInfo fromXContent(XContentParser parser) throws IOException {
                                 partSize = new ByteSizeValue(parser.longValue());
                             } else if ("written_by".equals(currentFieldName)) {
                                 writtenBy = Lucene.parseVersionLenient(parser.text(), null);
+                            } else if ("meta_hash".equals(currentFieldName)) {
+                                metaHash.bytes = parser.binaryValue();
+                                metaHash.offset = 0;
+                                metaHash.length = metaHash.bytes.length;
                             } else {
                                 throw new ElasticsearchParseException("unknown parameter [" + currentFieldName + "]");
                             }
@@ -269,7 +281,7 @@ public static FileInfo fromXContent(XContentParser parser) throws IOException {
                 }
             }
             // TODO: Verify???
-            return new FileInfo(name, new StoreFileMetaData(physicalName, length, checksum, writtenBy), partSize);
+            return new FileInfo(name, new StoreFileMetaData(physicalName, length, checksum, writtenBy, metaHash), partSize);
         }
 
     }
diff --git a/src/main/java/org/elasticsearch/index/store/Store.java b/src/main/java/org/elasticsearch/index/store/Store.java
index 59b8ff9b9715..598254563707 100644
--- a/src/main/java/org/elasticsearch/index/store/Store.java
+++ b/src/main/java/org/elasticsearch/index/store/Store.java
@@ -21,12 +21,12 @@
 
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Iterables;
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexCommit;
-import org.apache.lucene.index.SegmentCommitInfo;
-import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoFormat;
+import org.apache.lucene.index.*;
 import org.apache.lucene.store.*;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.ExceptionsHelper;
@@ -440,7 +440,17 @@ public String toString() {
      * @see StoreFileMetaData
      */
     public final static class MetadataSnapshot implements Iterable<StoreFileMetaData> {
-        private final ImmutableMap<String, StoreFileMetaData> metadata;
+        private final Map<String, StoreFileMetaData> metadata;
+
+        public static final MetadataSnapshot EMPTY = new MetadataSnapshot();
+
+        public MetadataSnapshot(Map<String, StoreFileMetaData> metadata) {
+            this.metadata = metadata;
+        }
+
+        MetadataSnapshot() {
+            this.metadata = Collections.emptyMap();
+        }
 
         MetadataSnapshot(IndexCommit commit, Directory directory, ESLogger logger) throws IOException {
             metadata = buildMetadata(commit, directory, logger);
@@ -467,7 +477,7 @@ public String toString() {
                     for (String file : info.files()) {
                         String legacyChecksum = checksumMap.get(file);
                         if (version.onOrAfter(Version.LUCENE_4_8) && legacyChecksum == null) {
-                            checksumFromLuceneFile(directory, file, builder, logger, version);
+                            checksumFromLuceneFile(directory, file, builder, logger, version, Lucene46SegmentInfoFormat.SI_EXTENSION.equals(IndexFileNames.getExtension(file)));
                         } else {
                             builder.put(file, new StoreFileMetaData(file, directory.fileLength(file), legacyChecksum, null));
                         }
@@ -476,7 +486,7 @@ public String toString() {
                 final String segmentsFile = segmentCommitInfos.getSegmentsFileName();
                 String legacyChecksum = checksumMap.get(segmentsFile);
                 if (maxVersion.onOrAfter(Version.LUCENE_4_8) && legacyChecksum == null) {
-                    checksumFromLuceneFile(directory, segmentsFile, builder, logger, maxVersion);
+                    checksumFromLuceneFile(directory, segmentsFile, builder, logger, maxVersion, true);
                 } else {
                     builder.put(segmentsFile, new StoreFileMetaData(segmentsFile, directory.fileLength(segmentsFile), legacyChecksum, null));
                 }
@@ -526,22 +536,49 @@ public String toString() {
             }
         }
 
-        private static void checksumFromLuceneFile(Directory directory, String file, ImmutableMap.Builder<String, StoreFileMetaData> builder,  ESLogger logger, Version version) throws IOException {
+        private static void checksumFromLuceneFile(Directory directory, String file, ImmutableMap.Builder<String, StoreFileMetaData> builder,  ESLogger logger, Version version, boolean readFileAsHash) throws IOException {
+            final String checksum;
+            final BytesRef fileHash = new BytesRef();
             try (IndexInput in = directory.openInput(file, IOContext.READONCE)) {
                 try {
                     if (in.length() < CodecUtil.footerLength()) {
                         // truncated files trigger IAE if we seek negative... these files are really corrupted though
                         throw new CorruptIndexException("Can't retrieve checksum from file: " + file + " file length must be >= " + CodecUtil.footerLength() + " but was: " + in.length());
                     }
-                    String checksum = digestToString(CodecUtil.retrieveChecksum(in));
-                    builder.put(file, new StoreFileMetaData(file, directory.fileLength(file), checksum, version));
+                    if (readFileAsHash) {
+                       hashFile(fileHash, in);
+                    }
+                    checksum = digestToString(CodecUtil.retrieveChecksum(in));
+
                 } catch (Throwable ex) {
                     logger.debug("Can retrieve checksum from file [{}]", ex, file);
                     throw ex;
                 }
+                builder.put(file, new StoreFileMetaData(file, directory.fileLength(file), checksum, version, fileHash));
             }
         }
 
+        /**
+         * Computes a strong hash value for small files. Note that this method should only be used for files < 1MB
+         */
+        public static void hashFile(BytesRef fileHash, IndexInput in) throws IOException {
+            final int len = (int)Math.min(1024 * 1024, in.length()); // for safety we limit this to 1MB
+            fileHash.offset = 0;
+            fileHash.grow(len);
+            fileHash.length = len;
+            in.readBytes(fileHash.bytes, 0, len);
+        }
+
+        /**
+         * Computes a strong hash value for small files. Note that this method should only be used for files < 1MB
+         */
+        public static void hashFile(BytesRef fileHash, BytesRef source) throws IOException {
+            final int len = Math.min(1024 * 1024, source.length); // for safety we limit this to 1MB
+            fileHash.offset = 0;
+            fileHash.grow(len);
+            fileHash.length = len;
+            System.arraycopy(source.bytes, source.offset, fileHash.bytes, 0, len);
+        }
 
         @Override
         public Iterator<StoreFileMetaData> iterator() {
@@ -555,6 +592,134 @@ public StoreFileMetaData get(String name) {
         public Map<String, StoreFileMetaData> asMap() {
             return metadata;
         }
+
+        private static final String DEL_FILE_EXTENSION = "del";  // TODO think about how we can detect if this changes?
+        private static final String FIELD_INFOS_FILE_EXTENSION = "fnm";
+
+        /**
+         * Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the
+         * recovery target and this snapshot as the source. The returned diff will hold a list of files that are:
+         *  <ul>
+         *      <li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>
+         *      <li>different: they exist in both snapshots but their they are not identical</li>
+         *      <li>missing: files that exist in the source but not in the target</li>
+         *  </ul>
+         * This method groups file into per-segment files and per-commit files. A file is treated as
+         * identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated
+         * as identical iff:
+         * <ul>
+         *     <li>all files in this segment have the same checksum</li>
+         *     <li>all files in this segment have the same length</li>
+         *     <li>the segments <tt>.si</tt> files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the <tt>.si</tt> file content as it's hash</li>
+         * </ul>
+         *
+         * The <tt>.si</tt> file contains a lot of diagnostics including a timestamp etc. in the future there might be
+         * unique segment identifiers in there hardening this method further.
+         *
+         * The per-commit files handles very similar. A commit is composed of the <tt>segments_N</tt> files as well as generational files like
+         * deletes (<tt>_x_y.del</tt>) or field-info (<tt>_x_y.fnm</tt>) files. On a per-commit level files for a commit are treated
+         * as identical iff:
+         * <ul>
+         *     <li>all files belonging to this commit have the same checksum</li>
+         *     <li>all files belonging to this commit have the same length</li>
+         *     <li>the segments file <tt>segments_N</tt> files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the <tt>segments_N</tt> file content as it's hash</li>
+         * </ul>
+         *
+         * NOTE: this diff will not contain the <tt>segments.gen</tt> file. This file is omitted on recovery.
+         */
+        public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {
+            final ImmutableList.Builder<StoreFileMetaData> identical =  ImmutableList.builder();
+            final ImmutableList.Builder<StoreFileMetaData> different =  ImmutableList.builder();
+            final ImmutableList.Builder<StoreFileMetaData> missing =  ImmutableList.builder();
+            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>();
+            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>();
+
+            for (StoreFileMetaData meta : this) {
+                if (IndexFileNames.SEGMENTS_GEN.equals(meta.name())) {
+                    continue; // we don't need that file at all
+                }
+                final String segmentId = IndexFileNames.parseSegmentName(meta.name());
+                final String extension = IndexFileNames.getExtension(meta.name());
+                assert FIELD_INFOS_FILE_EXTENSION.equals(extension) == false || IndexFileNames.stripExtension(IndexFileNames.stripSegmentName(meta.name())).isEmpty() : "FieldInfos are generational but updateable DV are not supported in elasticsearch";
+                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension)) {
+                        // only treat del files as per-commit files fnm files are generational but only for upgradable DV
+                    perCommitStoreFiles.add(meta);
+                } else {
+                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId);
+                    if (perSegStoreFiles == null) {
+                        perSegStoreFiles = new ArrayList<>();
+                        perSegment.put(segmentId, perSegStoreFiles);
+                    }
+                    perSegStoreFiles.add(meta);
+                }
+            }
+            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>();
+            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {
+                identicalFiles.clear();
+                boolean consistent = true;
+                for (StoreFileMetaData meta : segmentFiles) {
+                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name());
+                    if (storeFileMetaData == null) {
+                        consistent = false;
+                        missing.add(meta);
+                    } else if (storeFileMetaData.isSame(meta) == false) {
+                        consistent = false;
+                        different.add(meta);
+                    } else {
+                        identicalFiles.add(meta);
+                    }
+                }
+                if (consistent) {
+                    identical.addAll(identicalFiles);
+                } else {
+                    // make sure all files are added - this can happen if only the deletes are different
+                    different.addAll(identicalFiles);
+                }
+            }
+            RecoveryDiff recoveryDiff = new RecoveryDiff(identical.build(), different.build(), missing.build());
+            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.SEGMENTS_GEN) ? 1: 0)
+                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size()  + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.SEGMENTS_GEN) + "]"   ;
+            return recoveryDiff;
+        }
+
+        /**
+         * Returns the number of files in this snapshot
+         */
+        public int size() {
+            return metadata.size();
+        }
+    }
+
+    /**
+     * A class representing the diff between a recovery source and recovery target
+     * @see MetadataSnapshot#recoveryDiff(org.elasticsearch.index.store.Store.MetadataSnapshot)
+     */
+    public static final class RecoveryDiff {
+        /**
+         *  Files that exist in both snapshots and they can be considered the same ie. they don't need to be recovered
+         */
+        public final List<StoreFileMetaData> identical;
+        /**
+         * Files that exist in both snapshots but their they are not identical
+         */
+        public final List<StoreFileMetaData> different;
+        /**
+         * Files that exist in the source but not in the target
+         */
+        public final List<StoreFileMetaData> missing;
+
+        RecoveryDiff(List<StoreFileMetaData> identical, List<StoreFileMetaData> different, List<StoreFileMetaData> missing) {
+            this.identical = identical;
+            this.different = different;
+            this.missing = missing;
+        }
+
+        /**
+         * Returns the sum of the files in this diff.
+         */
+        public int size() {
+            return identical.size() + different.size() + missing.size();
+        }
     }
 
     public final static class LegacyChecksums {
diff --git a/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java b/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java
index 3875c20be2a5..559a74be4efb 100644
--- a/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java
+++ b/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.store;
 
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -42,22 +43,35 @@
 
     private Version writtenBy;
 
+    private BytesRef hash;
+
     private StoreFileMetaData() {
     }
 
     public StoreFileMetaData(String name, long length) {
-        this(name, length, null, null);
+        this(name, length, null);
+    }
 
+    public StoreFileMetaData(String name, long length, String checksum) {
+        this(name, length, checksum, null, null);
     }
 
     public StoreFileMetaData(String name, long length, String checksum, Version writtenBy) {
+        this(name, length, checksum, writtenBy, null);
+    }
+
+    public StoreFileMetaData(String name, long length, String checksum, Version writtenBy, BytesRef hash) {
         this.name = name;
         this.length = length;
         this.checksum = checksum;
         this.writtenBy = writtenBy;
+        this.hash = hash == null ? new BytesRef() : hash;
     }
 
 
+    /**
+     * Returns the name of this file
+     */
     public String name() {
         return name;
     }
@@ -69,6 +83,12 @@ public long length() {
         return length;
     }
 
+    /**
+     * Returns a string representation of the files checksum. Since Lucene 4.8 this is a CRC32 checksum written
+     * by lucene. Previously we use Adler32 on top of Lucene as the checksum algorithm, if {@link #hasLegacyChecksum()} returns
+     * <code>true</code> this is a Adler32 checksum.
+     * @return
+     */
     @Nullable
     public String checksum() {
         return this.checksum;
@@ -81,7 +101,7 @@ public boolean isSame(StoreFileMetaData other) {
         if (checksum == null || other.checksum == null) {
             return false;
         }
-        return length == other.length && checksum.equals(other.checksum);
+        return length == other.length && checksum.equals(other.checksum) && hash.equals(other.hash);
     }
 
     public static StoreFileMetaData readStoreFileMetaData(StreamInput in) throws IOException {
@@ -104,6 +124,11 @@ public void readFrom(StreamInput in) throws IOException {
             String versionString = in.readOptionalString();
             writtenBy = Lucene.parseVersionLenient(versionString, null);
         }
+        if (in.getVersion().onOrAfter(org.elasticsearch.Version.V_1_3_3)) {
+            hash = in.readBytesRef();
+        } else {
+            hash = new BytesRef();
+        }
     }
 
     @Override
@@ -114,6 +139,9 @@ public void writeTo(StreamOutput out) throws IOException {
         if (out.getVersion().onOrAfter(org.elasticsearch.Version.V_1_3_0)) {
             out.writeOptionalString(writtenBy == null ? null : writtenBy.name());
         }
+        if (out.getVersion().onOrAfter(org.elasticsearch.Version.V_1_3_3)) {
+            out.writeBytesRef(hash);
+        }
     }
 
     /**
@@ -130,4 +158,12 @@ public Version writtenBy() {
     public boolean hasLegacyChecksum() {
         return checksum != null && ((writtenBy != null  && writtenBy.onOrAfter(Version.LUCENE_4_8)) == false);
     }
+
+    /**
+     * Returns a variable length hash of the file represented by this metadata object. This can be the file
+     * itself if the file is small enough. If the length of the hash is <tt>0</tt> no hash value is available
+     */
+    public BytesRef hash() {
+        return hash;
+    }
 }
diff --git a/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java b/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
index d31b707fed36..0b351a7729f6 100644
--- a/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
+++ b/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.indices.recovery;
 
+import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
 import org.apache.lucene.index.CorruptIndexException;
@@ -141,35 +142,32 @@ public void phase1(final SnapshotIndexCommit snapshot) throws ElasticsearchExcep
                 store.incRef();
                 try {
                     StopWatch stopWatch = new StopWatch().start();
-                    final Store.MetadataSnapshot metadata;
-                    metadata = store.getMetadata(snapshot);
+                    final Store.MetadataSnapshot recoverySourceMetadata = store.getMetadata(snapshot);
                     for (String name : snapshot.getFiles()) {
-                        final StoreFileMetaData md = metadata.get(name);
+                        final StoreFileMetaData md = recoverySourceMetadata.get(name);
                         if (md == null) {
-                            logger.info("Snapshot differs from actual index for file: {} meta: {}", name, metadata.asMap());
-                            throw new CorruptIndexException("Snapshot differs from actual index - maybe index was removed metadata has " + metadata.asMap().size() + " files");
+                            logger.info("Snapshot differs from actual index for file: {} meta: {}", name, recoverySourceMetadata.asMap());
+                            throw new CorruptIndexException("Snapshot differs from actual index - maybe index was removed metadata has " + recoverySourceMetadata.asMap().size() + " files");
                         }
-                        boolean useExisting = false;
-                        if (request.existingFiles().containsKey(name)) {
-                            if (md.isSame(request.existingFiles().get(name))) {
-                                response.phase1ExistingFileNames.add(name);
-                                response.phase1ExistingFileSizes.add(md.length());
-                                existingTotalSize += md.length();
-                                useExisting = true;
-                                if (logger.isTraceEnabled()) {
-                                    logger.trace("[{}][{}] recovery [phase1] to {}: not recovering [{}], exists in local store and has checksum [{}], size [{}]", request.shardId().index().name(), request.shardId().id(), request.targetNode(), name, md.checksum(), md.length());
-                                }
-                            }
+                    }
+                    final Store.RecoveryDiff diff = recoverySourceMetadata.recoveryDiff(new Store.MetadataSnapshot(request.existingFiles()));
+                    for (StoreFileMetaData md : diff.identical) {
+                        response.phase1ExistingFileNames.add(md.name());
+                        response.phase1ExistingFileSizes.add(md.length());
+                        existingTotalSize += md.length();
+                        if (logger.isTraceEnabled()) {
+                            logger.trace("[{}][{}] recovery [phase1] to {}: not recovering [{}], exists in local store and has checksum [{}], size [{}]", request.shardId().index().name(), request.shardId().id(), request.targetNode(), md.name(), md.checksum(), md.length());
                         }
-                        if (!useExisting) {
-                            if (request.existingFiles().containsKey(name)) {
-                                logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], exists in local store, but is different: remote [{}], local [{}]", request.shardId().index().name(), request.shardId().id(), request.targetNode(), name, request.existingFiles().get(name), md);
-                            } else {
-                                logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], does not exists in remote", request.shardId().index().name(), request.shardId().id(), request.targetNode(), name);
-                            }
-                            response.phase1FileNames.add(name);
-                            response.phase1FileSizes.add(md.length());
+                        totalSize += md.length();
+                    }
+                    for (StoreFileMetaData md : Iterables.concat(diff.different, diff.missing)) {
+                        if (request.existingFiles().containsKey(md.name())) {
+                            logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], exists in local store, but is different: remote [{}], local [{}]", request.shardId().index().name(), request.shardId().id(), request.targetNode(), md.name(), request.existingFiles().get(md.name()), md);
+                        } else {
+                            logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], does not exists in remote", request.shardId().index().name(), request.shardId().id(), request.targetNode(), md.name());
                         }
+                        response.phase1FileNames.add(md.name());
+                        response.phase1FileSizes.add(md.length());
                         totalSize += md.length();
                     }
                     response.phase1TotalSize = totalSize;
@@ -199,7 +197,7 @@ public void phase1(final SnapshotIndexCommit snapshot) throws ElasticsearchExcep
                             public void run() {
                                 IndexInput indexInput = null;
                                 store.incRef();
-                                final StoreFileMetaData md = metadata.get(name);
+                                final StoreFileMetaData md = recoverySourceMetadata.get(name);
                                 try {
                                     final int BUFFER_SIZE = (int) recoverySettings.fileChunkSize().bytes();
                                     byte[] buf = new byte[BUFFER_SIZE];
diff --git a/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityTest.java b/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityTest.java
index 37f554203946..313fe3ad84e9 100644
--- a/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityTest.java
+++ b/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityTest.java
@@ -411,94 +411,4 @@ public Version getMasterVersion() {
         return client().admin().cluster().prepareState().get().getState().nodes().masterNode().getVersion();
     }
 
-    @Test
-    public void testSnapshotAndRestore() throws ExecutionException, InterruptedException, IOException {
-        logger.info("-->  creating repository");
-        assertAcked(client().admin().cluster().preparePutRepository("test-repo")
-                .setType("fs").setSettings(ImmutableSettings.settingsBuilder()
-                        .put("location", newTempDir(LifecycleScope.SUITE).getAbsolutePath())
-                        .put("compress", randomBoolean())
-                        .put("chunk_size", randomIntBetween(100, 1000))));
-        String[] indices = new String[randomIntBetween(1,5)];
-        for (int i = 0; i < indices.length; i++) {
-            indices[i] = "index_" + i;
-            createIndex(indices[i]);
-        }
-        ensureYellow();
-        logger.info("--> indexing some data");
-        IndexRequestBuilder[] builders = new IndexRequestBuilder[randomIntBetween(10, 200)];
-        for (int i = 0; i < builders.length; i++) {
-            builders[i] = client().prepareIndex(RandomPicks.randomFrom(getRandom(), indices), "foo", Integer.toString(i)).setSource("{ \"foo\" : \"bar\" } ");
-        }
-        indexRandom(true, builders);
-        assertThat(client().prepareCount(indices).get().getCount(), equalTo((long)builders.length));
-        long[] counts = new long[indices.length];
-        for (int i = 0; i < indices.length; i++) {
-            counts[i] = client().prepareCount(indices[i]).get().getCount();
-        }
-
-        logger.info("--> snapshot");
-        CreateSnapshotResponse createSnapshotResponse = client().admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("index_*").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
-
-        assertThat(client().admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("--> delete some data");
-        int howMany = randomIntBetween(1, builders.length);
-        
-        for (int i = 0; i < howMany; i++) {
-            IndexRequestBuilder indexRequestBuilder = RandomPicks.randomFrom(getRandom(), builders);
-            IndexRequest request = indexRequestBuilder.request();
-            client().prepareDelete(request.index(), request.type(), request.id()).get();
-        }
-        refresh();
-        final long numDocs = client().prepareCount(indices).get().getCount();
-        assertThat(client().prepareCount(indices).get().getCount(), lessThan((long)builders.length));
-
-
-        client().admin().indices().prepareUpdateSettings(indices).setSettings(ImmutableSettings.builder().put(EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE, "none")).get();
-        backwardsCluster().allowOnAllNodes(indices);
-        logClusterState();
-        boolean upgraded;
-        do {
-            logClusterState();
-            CountResponse countResponse = client().prepareCount().get();
-            assertHitCount(countResponse, numDocs);
-            upgraded = backwardsCluster().upgradeOneNode();
-            ensureYellow();
-            countResponse = client().prepareCount().get();
-            assertHitCount(countResponse, numDocs);
-        } while (upgraded);
-        client().admin().indices().prepareUpdateSettings(indices).setSettings(ImmutableSettings.builder().put(EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE, "all")).get();
-
-        logger.info("--> close indices");
-
-        client().admin().indices().prepareClose(indices).get();
-
-        logger.info("--> restore all indices from the snapshot");
-        RestoreSnapshotResponse restoreSnapshotResponse = client().admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-
-        ensureYellow();
-        assertThat(client().prepareCount(indices).get().getCount(), equalTo((long)builders.length));
-        for (int i = 0; i < indices.length; i++) {
-            assertThat(counts[i], equalTo(client().prepareCount(indices[i]).get().getCount()));
-        }
-
-        // Test restore after index deletion
-        logger.info("--> delete indices");
-        String index = RandomPicks.randomFrom(getRandom(), indices);
-        cluster().wipeIndices(index);
-        logger.info("--> restore one index after deletion");
-        restoreSnapshotResponse = client().admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices(index).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureYellow();
-        assertThat(client().prepareCount(indices).get().getCount(), equalTo((long)builders.length));
-        for (int i = 0; i < indices.length; i++) {
-            assertThat(counts[i], equalTo(client().prepareCount(indices[i]).get().getCount()));
-        }
-    }
-
-
 }
diff --git a/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTest.java b/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTest.java
new file mode 100644
index 000000000000..d16a9db3fda4
--- /dev/null
+++ b/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTest.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.snapshots.blobstore;
+
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.common.xcontent.*;
+import org.elasticsearch.index.store.StoreFileMetaData;
+import org.elasticsearch.test.ElasticsearchTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+
+/**
+ */
+public class FileInfoTest extends ElasticsearchTestCase {
+
+    @Test
+    public void testToFromXContent() throws IOException {
+        final int iters = scaledRandomIntBetween(1, 10);
+        for (int iter = 0; iter < iters; iter++) {
+            final BytesRef hash = new BytesRef(scaledRandomIntBetween(0, 1024 * 1024));
+            hash.length = hash.bytes.length;
+            for (int i = 0; i < hash.length; i++) {
+                hash.bytes[i] = randomByte();
+            }
+            StoreFileMetaData meta = new StoreFileMetaData("foobar", randomInt(), randomAsciiOfLengthBetween(1, 10), TEST_VERSION_CURRENT, hash);
+            ByteSizeValue size = new ByteSizeValue(Math.max(0,Math.abs(randomLong())));
+            BlobStoreIndexShardSnapshot.FileInfo info = new BlobStoreIndexShardSnapshot.FileInfo("_foobar", meta, size);
+            XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON).prettyPrint();
+            BlobStoreIndexShardSnapshot.FileInfo.toXContent(info, builder, ToXContent.EMPTY_PARAMS);
+            byte[] xcontent = builder.bytes().toBytes();
+
+            final BlobStoreIndexShardSnapshot.FileInfo parsedInfo;
+            try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(xcontent)) {
+                parser.nextToken();
+                parsedInfo = BlobStoreIndexShardSnapshot.FileInfo.fromXContent(parser);
+            }
+            assertThat(info.name(), equalTo(parsedInfo.name()));
+            assertThat(info.physicalName(), equalTo(parsedInfo.physicalName()));
+            assertThat(info.length(), equalTo(parsedInfo.length()));
+            assertThat(info.checksum(), equalTo(parsedInfo.checksum()));
+            assertThat(info.partBytes(), equalTo(parsedInfo.partBytes()));
+            assertThat(parsedInfo.metadata().hash().length, equalTo(hash.length));
+            assertThat(parsedInfo.metadata().hash(), equalTo(hash));
+            assertThat(parsedInfo.metadata().writtenBy(), equalTo(TEST_VERSION_CURRENT));
+            assertThat(parsedInfo.isSame(info.metadata()), is(true));
+        }
+    }
+}
diff --git a/src/test/java/org/elasticsearch/index/store/StoreTest.java b/src/test/java/org/elasticsearch/index/store/StoreTest.java
index 95f1d847d676..1bf6ffeefb5a 100644
--- a/src/test/java/org/elasticsearch/index/store/StoreTest.java
+++ b/src/test/java/org/elasticsearch/index/store/StoreTest.java
@@ -20,10 +20,7 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.document.TextField;
+import org.apache.lucene.document.*;
 import org.apache.lucene.index.*;
 import org.apache.lucene.store.*;
 import org.apache.lucene.util.BytesRef;
@@ -41,9 +38,7 @@
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.nio.file.NoSuchFileException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Map;
+import java.util.*;
 
 import static org.hamcrest.Matchers.*;
 
@@ -108,7 +103,7 @@ public void testVerifyingIndexOutputWithBogusInput() throws IOException {
     @Test
     public void testWriteLegacyChecksums() throws IOException {
         final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService();
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
         Store store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(directoryService));
         // set default codec - all segments need checksums
         IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(actualDefaultCodec()));
@@ -171,7 +166,7 @@ public void testWriteLegacyChecksums() throws IOException {
     @Test
     public void testNewChecksums() throws IOException {
         final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService();
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
         Store store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(directoryService));
         // set default codec - all segments need checksums
         IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(actualDefaultCodec()));
@@ -211,6 +206,9 @@ public void testNewChecksums() throws IOException {
                 assertThat("File: " + meta.name() + " has a different checksum", meta.checksum(), equalTo(checksum));
                 assertThat(meta.hasLegacyChecksum(), equalTo(false));
                 assertThat(meta.writtenBy(), equalTo(TEST_VERSION_CURRENT));
+                if (meta.name().endsWith(".si") || meta.name().startsWith("segments_")) {
+                    assertThat(meta.hash().length, greaterThan(0));
+                }
             }
         }
         assertConsistent(store, metadata);
@@ -223,7 +221,7 @@ public void testNewChecksums() throws IOException {
     @Test
     public void testMixedChecksums() throws IOException {
         final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService();
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
         Store store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(directoryService));
         // this time random codec....
         IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(actualDefaultCodec()));
@@ -309,7 +307,7 @@ public void testMixedChecksums() throws IOException {
     @Test
     public void testRenameFile() throws IOException {
         final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(false);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random(), false);
         Store store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(directoryService));
         {
             IndexOutput output = store.directory().createOutput("foo.bar", IOContext.DEFAULT);
@@ -368,20 +366,22 @@ public void assertDeleteContent(Store store,DirectoryService service) throws IOE
         }
     }
 
-    private  final class LuceneManagedDirectoryService implements DirectoryService {
+    private static final class LuceneManagedDirectoryService implements DirectoryService {
         private final Directory[] dirs;
+        private final Random random;
 
-        public LuceneManagedDirectoryService() {
-            this(true);
+        public LuceneManagedDirectoryService(Random random) {
+            this(random, true);
         }
-        public LuceneManagedDirectoryService(boolean preventDoubleWrite) {
-            this.dirs = new Directory[1 + random().nextInt(5)];
+        public LuceneManagedDirectoryService(Random random, boolean preventDoubleWrite) {
+            this.dirs = new Directory[1 + random.nextInt(5)];
             for (int i = 0; i < dirs.length; i++) {
-                dirs[i]  = newDirectory();
+                dirs[i]  = newDirectory(random);
                 if (dirs[i] instanceof MockDirectoryWrapper) {
                     ((MockDirectoryWrapper)dirs[i]).setPreventDoubleWrite(preventDoubleWrite);
                 }
             }
+            this.random = random;
         }
         @Override
         public Directory[] build() throws IOException {
@@ -390,7 +390,7 @@ public LuceneManagedDirectoryService(boolean preventDoubleWrite) {
 
         @Override
         public long throttleTimeInNanos() {
-            return random().nextInt(1000);
+            return random.nextInt(1000);
         }
 
         @Override
@@ -416,9 +416,159 @@ public static void assertConsistent(Store store, Store.MetadataSnapshot metadata
             }
         }
     }
-
     private Distributor randomDistributor(DirectoryService service) throws IOException {
-        return random().nextBoolean() ? new LeastUsedDistributor(service) : new RandomWeightedDistributor(service);
+        return randomDistributor(random(), service);
+    }
+
+    private Distributor randomDistributor(Random random, DirectoryService service) throws IOException {
+        return random.nextBoolean() ? new LeastUsedDistributor(service) : new RandomWeightedDistributor(service);
+    }
+
+
+    @Test
+    public void testRecoveryDiff() throws IOException, InterruptedException {
+        int numDocs = 2 + random().nextInt(100);
+        List<Document> docs = new ArrayList<>();
+        for (int i = 0; i < numDocs; i++) {
+            Document doc = new Document();
+            doc.add(new StringField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
+            docs.add(doc);
+        }
+        long seed = random().nextLong();
+        Store.MetadataSnapshot first;
+        {
+            Random random = new Random(seed);
+            IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(actualDefaultCodec());
+            iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+            iwc.setUseCompoundFile(random.nextBoolean());
+            iwc.setMaxThreadStates(1);
+            final ShardId shardId = new ShardId(new Index("index"), 1);
+            DirectoryService directoryService = new LuceneManagedDirectoryService(random);
+            Store store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(random, directoryService));
+            IndexWriter writer = new IndexWriter(store.directory(), iwc);
+            final boolean lotsOfSegments = rarely(random);
+            for (Document d : docs) {
+                writer.addDocument(d);
+                if (lotsOfSegments && random.nextBoolean()) {
+                    writer.commit();
+                } else if (rarely(random)) {
+                    writer.commit();
+                }
+            }
+            writer.close();
+            first = store.getMetadata();
+            assertDeleteContent(store, directoryService);
+            store.close();
+        }
+        long time = new Date().getTime();
+        while(time == new Date().getTime()) {
+            Thread.sleep(10); // bump the time
+        }
+        Store.MetadataSnapshot second;
+        Store store;
+        {
+            Random random = new Random(seed);
+            IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(actualDefaultCodec());
+            iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+            iwc.setUseCompoundFile(random.nextBoolean());
+            iwc.setMaxThreadStates(1);
+            final ShardId shardId = new ShardId(new Index("index"), 1);
+            DirectoryService directoryService = new LuceneManagedDirectoryService(random);
+            store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(random, directoryService));
+            IndexWriter writer = new IndexWriter(store.directory(), iwc);
+            final boolean lotsOfSegments = rarely(random);
+            for (Document d : docs) {
+                writer.addDocument(d);
+                if (lotsOfSegments && random.nextBoolean()) {
+                    writer.commit();
+                } else if (rarely(random)) {
+                    writer.commit();
+                }
+            }
+            writer.close();
+            second = store.getMetadata();
+        }
+        Store.RecoveryDiff diff = first.recoveryDiff(second);
+        assertThat(first.size(), equalTo(second.size()));
+        for (StoreFileMetaData md : first) {
+            assertThat(second.get(md.name()), notNullValue());
+            // si files are different - containing timestamps etc
+            assertThat(second.get(md.name()).isSame(md), equalTo(md.name().endsWith(".si") == false));
+        }
+        assertThat(diff.different.size(), equalTo(first.size()-1));
+        assertThat(diff.identical.size(), equalTo(1)); // commit point is identical
+        assertThat(diff.missing, empty());
+
+        // check the self diff
+        Store.RecoveryDiff selfDiff = first.recoveryDiff(first);
+        assertThat(selfDiff.identical.size(), equalTo(first.size()));
+        assertThat(selfDiff.different, empty());
+        assertThat(selfDiff.missing, empty());
+
+
+        // lets add some deletes
+        Random random = new Random(seed);
+        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(actualDefaultCodec());
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        iwc.setUseCompoundFile(random.nextBoolean());
+        iwc.setMaxThreadStates(1);
+        iwc.setOpenMode(IndexWriterConfig.OpenMode.APPEND);
+        IndexWriter writer = new IndexWriter(store.directory(), iwc);
+        writer.deleteDocuments(new Term("id", Integer.toString(random().nextInt(numDocs))));
+        writer.close();
+        Store.MetadataSnapshot metadata = store.getMetadata();
+        StoreFileMetaData delFile = null;
+        for (StoreFileMetaData md : metadata) {
+            if (md.name().endsWith(".del")) {
+                delFile = md;
+                break;
+            }
+        }
+        Store.RecoveryDiff afterDeleteDiff = metadata.recoveryDiff(second);
+        if (delFile != null) {
+            assertThat(afterDeleteDiff.identical.size(), equalTo(metadata.size()-2)); // segments_N + del file
+            assertThat(afterDeleteDiff.different.size(), equalTo(0));
+            assertThat(afterDeleteDiff.missing.size(), equalTo(2));
+        } else {
+            // an entire segment must be missing (single doc segment got dropped)
+            assertThat(afterDeleteDiff.identical.size(), greaterThan(0));
+            assertThat(afterDeleteDiff.different.size(), equalTo(0));
+            assertThat(afterDeleteDiff.missing.size(), equalTo(1)); // the commit file is different
+        }
+
+        // check the self diff
+        selfDiff = metadata.recoveryDiff(metadata);
+        assertThat(selfDiff.identical.size(), equalTo(metadata.size()));
+        assertThat(selfDiff.different, empty());
+        assertThat(selfDiff.missing, empty());
+
+        // add a new commit
+        iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(actualDefaultCodec());
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        iwc.setUseCompoundFile(true); // force CFS - easier to test here since we know it will add 3 files
+        iwc.setMaxThreadStates(1);
+        iwc.setOpenMode(IndexWriterConfig.OpenMode.APPEND);
+        writer = new IndexWriter(store.directory(), iwc);
+        writer.addDocument(docs.get(0));
+        writer.close();
+
+        Store.MetadataSnapshot newCommitMetaData = store.getMetadata();
+        Store.RecoveryDiff newCommitDiff = newCommitMetaData.recoveryDiff(metadata);
+        if (delFile != null) {
+            assertThat(newCommitDiff.identical.size(), equalTo(newCommitMetaData.size()-5)); // segments_N, del file, cfs, cfe, si for the new segment
+            assertThat(newCommitDiff.different.size(), equalTo(1)); // the del file must be different
+            assertThat(newCommitDiff.different.get(0).name(), endsWith(".del"));
+            assertThat(newCommitDiff.missing.size(), equalTo(4)); // segments_N,cfs, cfe, si for the new segment
+        } else {
+            assertThat(newCommitDiff.identical.size(), equalTo(newCommitMetaData.size() - 4)); // segments_N, cfs, cfe, si for the new segment
+            assertThat(newCommitDiff.different.size(), equalTo(0));
+            assertThat(newCommitDiff.missing.size(), equalTo(4)); // an entire segment must be missing (single doc segment got dropped)  plus the commit is different
+        }
+
+        store.deleteContent();
+        IOUtils.close(store);
     }
 
 
diff --git a/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java b/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java
index 1085d2d57da1..042f69549170 100644
--- a/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java
+++ b/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java
@@ -22,7 +22,6 @@
 import com.carrotsearch.randomizedtesting.LifecycleScope;
 import com.google.common.base.Predicate;
 import com.google.common.collect.ImmutableList;
-import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ListenableActionFuture;
@@ -36,8 +35,10 @@
 import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;
 import org.elasticsearch.action.admin.indices.template.get.GetIndexTemplatesResponse;
 import org.elasticsearch.action.count.CountResponse;
+import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
 import org.elasticsearch.cluster.metadata.SnapshotMetaData;
 import org.elasticsearch.cluster.routing.allocation.decider.FilterAllocationDecider;
@@ -52,6 +53,8 @@
 import org.junit.Test;
 
 import java.io.File;
+import java.util.List;
+import java.util.concurrent.ExecutionException;
 import java.util.concurrent.TimeUnit;
 
 import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
@@ -1232,6 +1235,68 @@ public void snapshotRelocatingPrimary() throws Exception {
         logger.info("--> done");
     }
 
+    public void testSnapshotMoreThanOnce() throws ExecutionException, InterruptedException {
+        Client client = client();
+
+        logger.info("-->  creating repository");
+        assertAcked(client.admin().cluster().preparePutRepository("test-repo")
+                .setType("fs").setSettings(ImmutableSettings.settingsBuilder()
+                        .put("location", newTempDir(LifecycleScope.SUITE))
+                        .put("compress", randomBoolean())
+                        .put("chunk_size", randomIntBetween(100, 1000))));
+
+        // only one shard
+        assertAcked(prepareCreate("test").setSettings(ImmutableSettings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)));
+        ensureGreen();
+        logger.info("-->  indexing");
+
+        final int numdocs = randomIntBetween(10, 100);
+        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];
+        for (int i = 0; i < builders.length; i++) {
+            builders[i] = client().prepareIndex("test", "doc", Integer.toString(i)).setSource("foo", "bar" + i);
+        }
+        indexRandom(true, builders);
+        flushAndRefresh();
+        assertNoFailures(client().admin().indices().prepareOptimize("test").setForce(true).setFlush(true).setWaitForMerge(true).setMaxNumSegments(1).get());
+
+        CreateSnapshotResponse createSnapshotResponseFirst = client.admin().cluster().prepareCreateSnapshot("test-repo", "test").setWaitForCompletion(true).setIndices("test").get();
+        assertThat(createSnapshotResponseFirst.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponseFirst.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponseFirst.getSnapshotInfo().totalShards()));
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+        {
+            SnapshotStatus snapshotStatus = client.admin().cluster().prepareSnapshotStatus("test-repo").setSnapshots("test").get().getSnapshots().get(0);
+            List<SnapshotIndexShardStatus> shards = snapshotStatus.getShards();
+            for (SnapshotIndexShardStatus status : shards) {
+                assertThat(status.getStats().getProcessedFiles(), greaterThan(1));
+            }
+        }
+
+        CreateSnapshotResponse createSnapshotResponseSecond = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-1").setWaitForCompletion(true).setIndices("test").get();
+        assertThat(createSnapshotResponseSecond.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponseSecond.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponseSecond.getSnapshotInfo().totalShards()));
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-1").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+        {
+            SnapshotStatus snapshotStatus = client.admin().cluster().prepareSnapshotStatus("test-repo").setSnapshots("test-1").get().getSnapshots().get(0);
+            List<SnapshotIndexShardStatus> shards = snapshotStatus.getShards();
+            for (SnapshotIndexShardStatus status : shards) {
+                assertThat(status.getStats().getProcessedFiles(), equalTo(1)); // we flush before the snapshot such that we have to process the segments_N files
+            }
+        }
+
+        client().prepareDelete("test", "doc", "1").get();
+        CreateSnapshotResponse createSnapshotResponseThird = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-2").setWaitForCompletion(true).setIndices("test").get();
+        assertThat(createSnapshotResponseThird.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponseThird.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponseThird.getSnapshotInfo().totalShards()));
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-2").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+        {
+            SnapshotStatus snapshotStatus = client.admin().cluster().prepareSnapshotStatus("test-repo").setSnapshots("test-2").get().getSnapshots().get(0);
+            List<SnapshotIndexShardStatus> shards = snapshotStatus.getShards();
+            for (SnapshotIndexShardStatus status : shards) {
+                assertThat(status.getStats().getProcessedFiles(), equalTo(2)); // we flush before the snapshot such that we have to process the segments_N files plus the .del file
+            }
+        }
+    }
+
     private boolean waitForIndex(final String index, TimeValue timeout) throws InterruptedException {
         return awaitBusy(new Predicate<Object>() {
             @Override
diff --git a/src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityTest.java b/src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityTest.java
new file mode 100644
index 000000000000..f0e09e3579be
--- /dev/null
+++ b/src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityTest.java
@@ -0,0 +1,246 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.snapshots;
+
+import com.carrotsearch.randomizedtesting.LifecycleScope;
+import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexShardStatus;
+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus;
+import org.elasticsearch.action.count.CountResponse;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
+import org.elasticsearch.common.settings.ImmutableSettings;
+import org.elasticsearch.test.ElasticsearchBackwardsCompatIntegrationTest;
+import org.junit.Ignore;
+import org.junit.Test;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.List;
+import java.util.concurrent.ExecutionException;
+
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.lessThan;
+
+public class SnapshotBackwardsCompatibilityTest extends ElasticsearchBackwardsCompatIntegrationTest {
+
+    @Test
+    public void testSnapshotAndRestore() throws ExecutionException, InterruptedException, IOException {
+        logger.info("-->  creating repository");
+        assertAcked(client().admin().cluster().preparePutRepository("test-repo")
+                .setType("fs").setSettings(ImmutableSettings.settingsBuilder()
+                        .put("location", newTempDir(LifecycleScope.SUITE).getAbsolutePath())
+                        .put("compress", randomBoolean())
+                        .put("chunk_size", randomIntBetween(100, 1000))));
+        String[] indicesBefore = new String[randomIntBetween(2,5)];
+        String[] indicesAfter = new String[randomIntBetween(2,5)];
+        for (int i = 0; i < indicesBefore.length; i++) {
+            indicesBefore[i] = "index_before_" + i;
+            createIndex(indicesBefore[i]);
+        }
+        for (int i = 0; i < indicesAfter.length; i++) {
+            indicesAfter[i] = "index_after_" + i;
+            createIndex(indicesAfter[i]);
+        }
+        String[] indices = new String[indicesBefore.length + indicesAfter.length];
+        System.arraycopy(indicesBefore, 0, indices, 0, indicesBefore.length);
+        System.arraycopy(indicesAfter, 0, indices, indicesBefore.length, indicesAfter.length);
+        ensureYellow();
+        logger.info("--> indexing some data");
+        IndexRequestBuilder[] buildersBefore = new IndexRequestBuilder[randomIntBetween(10, 200)];
+        for (int i = 0; i < buildersBefore.length; i++) {
+            buildersBefore[i] = client().prepareIndex(RandomPicks.randomFrom(getRandom(), indicesBefore), "foo", Integer.toString(i)).setSource("{ \"foo\" : \"bar\" } ");
+        }
+        IndexRequestBuilder[] buildersAfter = new IndexRequestBuilder[randomIntBetween(10, 200)];
+        for (int i = 0; i < buildersAfter.length; i++) {
+            buildersAfter[i] = client().prepareIndex(RandomPicks.randomFrom(getRandom(), indicesBefore), "bar", Integer.toString(i)).setSource("{ \"foo\" : \"bar\" } ");
+        }
+        indexRandom(true, buildersBefore);
+        indexRandom(true, buildersAfter);
+        assertThat(client().prepareCount(indices).get().getCount(), equalTo((long) (buildersBefore.length + buildersAfter.length)));
+        long[] counts = new long[indices.length];
+        for (int i = 0; i < indices.length; i++) {
+            counts[i] = client().prepareCount(indices[i]).get().getCount();
+        }
+
+        logger.info("--> snapshot subset of indices before upgrage");
+        CreateSnapshotResponse createSnapshotResponse = client().admin().cluster().prepareCreateSnapshot("test-repo", "test-snap-1").setWaitForCompletion(true).setIndices("index_before_*").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
+
+        assertThat(client().admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap-1").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("--> delete some data from indices that were already snapshotted");
+        int howMany = randomIntBetween(1, buildersBefore.length);
+
+        for (int i = 0; i < howMany; i++) {
+            IndexRequestBuilder indexRequestBuilder = RandomPicks.randomFrom(getRandom(), buildersBefore);
+            IndexRequest request = indexRequestBuilder.request();
+            client().prepareDelete(request.index(), request.type(), request.id()).get();
+        }
+        refresh();
+        final long numDocs = client().prepareCount(indices).get().getCount();
+        assertThat(client().prepareCount(indices).get().getCount(), lessThan((long) (buildersBefore.length + buildersAfter.length)));
+
+
+        client().admin().indices().prepareUpdateSettings(indices).setSettings(ImmutableSettings.builder().put(EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE, "none")).get();
+        backwardsCluster().allowOnAllNodes(indices);
+        logClusterState();
+        boolean upgraded;
+        do {
+            logClusterState();
+            CountResponse countResponse = client().prepareCount().get();
+            assertHitCount(countResponse, numDocs);
+            upgraded = backwardsCluster().upgradeOneNode();
+            ensureYellow();
+            countResponse = client().prepareCount().get();
+            assertHitCount(countResponse, numDocs);
+        } while (upgraded);
+        client().admin().indices().prepareUpdateSettings(indices).setSettings(ImmutableSettings.builder().put(EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE, "all")).get();
+
+        logger.info("--> close indices");
+
+        client().admin().indices().prepareClose("index_before_*").get();
+
+        logger.info("--> restore all indices from the snapshot");
+        RestoreSnapshotResponse restoreSnapshotResponse = client().admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap-1").setWaitForCompletion(true).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+
+        ensureYellow();
+        assertThat(client().prepareCount(indices).get().getCount(), equalTo((long) (buildersBefore.length + buildersAfter.length)));
+        for (int i = 0; i < indices.length; i++) {
+            assertThat(counts[i], equalTo(client().prepareCount(indices[i]).get().getCount()));
+        }
+
+        logger.info("--> snapshot subset of indices after upgrade");
+        createSnapshotResponse = client().admin().cluster().prepareCreateSnapshot("test-repo", "test-snap-2").setWaitForCompletion(true).setIndices("index_*").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
+
+        // Test restore after index deletion
+        logger.info("--> delete indices");
+        String index = RandomPicks.randomFrom(getRandom(), indices);
+        cluster().wipeIndices(index);
+        logger.info("--> restore one index after deletion");
+        restoreSnapshotResponse = client().admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap-2").setWaitForCompletion(true).setIndices(index).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureYellow();
+        assertThat(client().prepareCount(indices).get().getCount(), equalTo((long) (buildersBefore.length + buildersAfter.length)));
+        for (int i = 0; i < indices.length; i++) {
+            assertThat(counts[i], equalTo(client().prepareCount(indices[i]).get().getCount()));
+        }
+    }
+
+    public void testSnapshotMoreThanOnce() throws ExecutionException, InterruptedException, IOException {
+        Client client = client();
+        final File tempDir = newTempDir(LifecycleScope.SUITE).getAbsoluteFile();
+        logger.info("-->  creating repository");
+        assertAcked(client.admin().cluster().preparePutRepository("test-repo")
+                .setType("fs").setSettings(ImmutableSettings.settingsBuilder()
+                        .put("location", tempDir)
+                        .put("compress", randomBoolean())
+                        .put("chunk_size", randomIntBetween(100, 1000))));
+
+        // only one shard
+        assertAcked(prepareCreate("test").setSettings(ImmutableSettings.builder()
+                .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
+                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
+        ));
+        ensureYellow();
+        logger.info("-->  indexing");
+
+        final int numDocs = randomIntBetween(10, 100);
+        IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs];
+        for (int i = 0; i < builders.length; i++) {
+            builders[i] = client().prepareIndex("test", "doc", Integer.toString(i)).setSource("foo", "bar" + i);
+        }
+        indexRandom(true, builders);
+        flushAndRefresh();
+        assertNoFailures(client().admin().indices().prepareOptimize("test").setForce(true).setFlush(true).setWaitForMerge(true).setMaxNumSegments(1).get());
+
+        CreateSnapshotResponse createSnapshotResponseFirst = client.admin().cluster().prepareCreateSnapshot("test-repo", "test").setWaitForCompletion(true).setIndices("test").get();
+        assertThat(createSnapshotResponseFirst.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponseFirst.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponseFirst.getSnapshotInfo().totalShards()));
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+        {
+            SnapshotStatus snapshotStatus = client.admin().cluster().prepareSnapshotStatus("test-repo").setSnapshots("test").get().getSnapshots().get(0);
+            List<SnapshotIndexShardStatus> shards = snapshotStatus.getShards();
+            for (SnapshotIndexShardStatus status : shards) {
+                assertThat(status.getStats().getProcessedFiles(), greaterThan(1));
+            }
+        }
+        if (frequently()) {
+            logger.info("-->  upgrade");
+            client().admin().indices().prepareUpdateSettings("test").setSettings(ImmutableSettings.builder().put(EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE, "none")).get();
+            backwardsCluster().allowOnAllNodes("test");
+            logClusterState();
+            boolean upgraded;
+            do {
+                logClusterState();
+                CountResponse countResponse = client().prepareCount().get();
+                assertHitCount(countResponse, numDocs);
+                upgraded = backwardsCluster().upgradeOneNode();
+                ensureYellow();
+                countResponse = client().prepareCount().get();
+                assertHitCount(countResponse, numDocs);
+            } while (upgraded);
+            client().admin().indices().prepareUpdateSettings("test").setSettings(ImmutableSettings.builder().put(EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE, "all")).get();
+        }
+        if (cluster().numDataNodes() > 1 && randomBoolean()) { // only bump the replicas if we have enough nodes
+            logger.info("--> move from 0 to 1 replica");
+            client().admin().indices().prepareUpdateSettings("test").setSettings(ImmutableSettings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)).get();
+        }
+        logger.debug("---> repo exists: " + new File(tempDir, "indices/test/0").exists() + " files: " + Arrays.toString(new File(tempDir, "indices/test/0").list())); // it's only one shard!
+        CreateSnapshotResponse createSnapshotResponseSecond = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-1").setWaitForCompletion(true).setIndices("test").get();
+        assertThat(createSnapshotResponseSecond.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponseSecond.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponseSecond.getSnapshotInfo().totalShards()));
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-1").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+        {
+            SnapshotStatus snapshotStatus = client.admin().cluster().prepareSnapshotStatus("test-repo").setSnapshots("test-1").get().getSnapshots().get(0);
+            List<SnapshotIndexShardStatus> shards = snapshotStatus.getShards();
+            for (SnapshotIndexShardStatus status : shards) {
+
+                assertThat(status.getStats().getProcessedFiles(), equalTo(1)); // we flush before the snapshot such that we have to process the segments_N files
+            }
+        }
+
+        client().prepareDelete("test", "doc", "1").get();
+        CreateSnapshotResponse createSnapshotResponseThird = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-2").setWaitForCompletion(true).setIndices("test").get();
+        assertThat(createSnapshotResponseThird.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponseThird.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponseThird.getSnapshotInfo().totalShards()));
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-2").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+        {
+            SnapshotStatus snapshotStatus = client.admin().cluster().prepareSnapshotStatus("test-repo").setSnapshots("test-2").get().getSnapshots().get(0);
+            List<SnapshotIndexShardStatus> shards = snapshotStatus.getShards();
+            for (SnapshotIndexShardStatus status : shards) {
+                assertThat(status.getStats().getProcessedFiles(), equalTo(2)); // we flush before the snapshot such that we have to process the segments_N files plus the .del file
+            }
+        }
+    }
+}
