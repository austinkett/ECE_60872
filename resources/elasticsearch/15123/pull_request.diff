diff --git a/core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java b/core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java
index d0eb29d6b220..f8507e5b689e 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java
@@ -25,7 +25,6 @@
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaDataMappingService;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -57,7 +56,7 @@ public NodeMappingRefreshAction(Settings settings, TransportService transportSer
     public void nodeMappingRefresh(final ClusterState state, final NodeMappingRefreshRequest request) {
         final DiscoveryNodes nodes = state.nodes();
         if (nodes.masterNode() == null) {
-            logger.warn("can't send mapping refresh for [{}][{}], no master known.", request.index(), Strings.arrayToCommaDelimitedString(request.types()));
+            logger.warn("can't send mapping refresh for [{}], no master known.", request.index());
             return;
         }
         transportService.sendRequest(nodes.masterNode(), ACTION_NAME, request, EmptyTransportResponseHandler.INSTANCE_SAME);
@@ -67,7 +66,7 @@ public void nodeMappingRefresh(final ClusterState state, final NodeMappingRefres
 
         @Override
         public void messageReceived(NodeMappingRefreshRequest request, TransportChannel channel) throws Exception {
-            metaDataMappingService.refreshMapping(request.index(), request.indexUUID(), request.types());
+            metaDataMappingService.refreshMapping(request.index(), request.indexUUID());
             channel.sendResponse(TransportResponse.Empty.INSTANCE);
         }
     }
@@ -76,16 +75,14 @@ public void messageReceived(NodeMappingRefreshRequest request, TransportChannel
 
         private String index;
         private String indexUUID = IndexMetaData.INDEX_UUID_NA_VALUE;
-        private String[] types;
         private String nodeId;
 
         public NodeMappingRefreshRequest() {
         }
 
-        public NodeMappingRefreshRequest(String index, String indexUUID, String[] types, String nodeId) {
+        public NodeMappingRefreshRequest(String index, String indexUUID, String nodeId) {
             this.index = index;
             this.indexUUID = indexUUID;
-            this.types = types;
             this.nodeId = nodeId;
         }
 
@@ -107,11 +104,6 @@ public String indexUUID() {
             return indexUUID;
         }
 
-
-        public String[] types() {
-            return types;
-        }
-
         public String nodeId() {
             return nodeId;
         }
@@ -120,7 +112,6 @@ public String nodeId() {
         public void writeTo(StreamOutput out) throws IOException {
             super.writeTo(out);
             out.writeString(index);
-            out.writeStringArray(types);
             out.writeString(nodeId);
             out.writeString(indexUUID);
         }
@@ -129,7 +120,6 @@ public void writeTo(StreamOutput out) throws IOException {
         public void readFrom(StreamInput in) throws IOException {
             super.readFrom(in);
             index = in.readString();
-            types = in.readStringArray();
             nodeId = in.readString();
             indexUUID = in.readString();
         }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
index d3ba811a6e56..bbb323749eef 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
@@ -304,26 +304,18 @@ public ClusterState execute(ClusterState currentState) throws Exception {
                     // now add the mappings
                     IndexService indexService = indicesService.indexServiceSafe(request.index());
                     MapperService mapperService = indexService.mapperService();
-                    // first, add the default mapping
-                    if (mappings.containsKey(MapperService.DEFAULT_MAPPING)) {
-                        try {
-                            mapperService.merge(MapperService.DEFAULT_MAPPING, new CompressedXContent(XContentFactory.jsonBuilder().map(mappings.get(MapperService.DEFAULT_MAPPING)).string()), false, request.updateAllTypes());
-                        } catch (Exception e) {
-                            removalReason = "failed on parsing default mapping on index creation";
-                            throw new MapperParsingException("Failed to parse mapping [{}]: {}", e, MapperService.DEFAULT_MAPPING, e.getMessage());
-                        }
-                    }
+                    final Map<String, CompressedXContent> mappingSources = new HashMap<>();
                     for (Map.Entry<String, Map<String, Object>> entry : mappings.entrySet()) {
-                        if (entry.getKey().equals(MapperService.DEFAULT_MAPPING)) {
-                            continue;
-                        }
-                        try {
-                            // apply the default here, its the first time we parse it
-                            mapperService.merge(entry.getKey(), new CompressedXContent(XContentFactory.jsonBuilder().map(entry.getValue()).string()), true, request.updateAllTypes());
-                        } catch (Exception e) {
-                            removalReason = "failed on parsing mappings on index creation";
-                            throw new MapperParsingException("Failed to parse mapping [{}]: {}", e, entry.getKey(), e.getMessage());
-                        }
+                        final String type = entry.getKey();
+                        final Map<String, Object> mapping = entry.getValue();
+                        final String mappingSource = XContentFactory.jsonBuilder().map(mapping).string();
+                        mappingSources.put(type, new CompressedXContent(mappingSource));
+                    }
+                    try {
+                        mapperService.merge(mappingSources, true, request.updateAllTypes());
+                    } catch (Exception e) {
+                        removalReason = "failed on parsing mappings on index creation";
+                        throw new MapperParsingException("Failed to parse mappings [{}]: {}", e, mappingSources, e.getMessage());
                     }
 
                     QueryShardContext queryShardContext = indexService.getQueryShardContext();
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java
index b13f9711bef7..b680015e819d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java
@@ -19,7 +19,8 @@
 
 package org.elasticsearch.cluster.metadata;
 
-import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.admin.indices.alias.IndicesAliasesClusterStateUpdateRequest;
 import org.elasticsearch.cluster.AckedClusterStateUpdateTask;
@@ -29,12 +30,12 @@
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.NodeServicesProvider;
-import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.indices.IndicesService;
 
 import java.util.*;
@@ -100,13 +101,13 @@ public ClusterState execute(final ClusterState currentState) {
                                         // temporarily create the index and add mappings so we can parse the filter
                                         try {
                                             indexService = indicesService.createIndex(nodeServicesProvider, indexMetaData, Collections.EMPTY_LIST);
-                                            if (indexMetaData.getMappings().containsKey(MapperService.DEFAULT_MAPPING)) {
-                                                indexService.mapperService().merge(MapperService.DEFAULT_MAPPING, indexMetaData.getMappings().get(MapperService.DEFAULT_MAPPING).source(), false, false);
-                                            }
-                                            for (ObjectCursor<MappingMetaData> cursor : indexMetaData.getMappings().values()) {
-                                                MappingMetaData mappingMetaData = cursor.value;
-                                                indexService.mapperService().merge(mappingMetaData.type(), mappingMetaData.source(), false, false);
+                                            final Map<String, CompressedXContent> mappingSources = new HashMap<>();
+                                            for (ObjectObjectCursor<String, MappingMetaData> entry : indexMetaData.getMappings()) {
+                                                final String type = entry.key;
+                                                final MappingMetaData mapping = entry.value;
+                                                mappingSources.put(type, mapping.source());
                                             }
+                                            indexService.mapperService().merge(mappingSources, false, false);
                                         } catch (Exception e) {
                                             logger.warn("[{}] failed to temporary create in order to apply alias action", e, indexMetaData.getIndex());
                                             continue;
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
index 2d89857f60d9..7e35d26ef8fc 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
@@ -19,11 +19,13 @@
 package org.elasticsearch.cluster.metadata;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.routing.UnassignedInfo;
 import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexSettings;
@@ -34,6 +36,8 @@
 import org.elasticsearch.indices.mapper.MapperRegistry;
 
 import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
 import java.util.Set;
 
 import static java.util.Collections.unmodifiableSet;
@@ -223,10 +227,11 @@ private void checkMappingsCompatibility(IndexMetaData indexMetaData) {
 
             try (AnalysisService analysisService = new FakeAnalysisService(indexSettings)) {
                 try (MapperService mapperService = new MapperService(indexSettings, analysisService, similarityService, mapperRegistry)) {
-                    for (ObjectCursor<MappingMetaData> cursor : indexMetaData.getMappings().values()) {
-                        MappingMetaData mappingMetaData = cursor.value;
-                        mapperService.merge(mappingMetaData.type(), mappingMetaData.source(), false, false);
+                    final Map<String, CompressedXContent> mappingSources = new HashMap<>();
+                    for (ObjectObjectCursor<String, MappingMetaData> entry : indexMetaData.getMappings()) {
+                        mappingSources.put(entry.key, entry.value.source());
                     }
+                    mapperService.merge(mappingSources, false, false);
                 }
             }
         } catch (Exception ex) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java
index fb6ed1f0753d..4b0ffa2745ed 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java
@@ -19,7 +19,8 @@
 
 package org.elasticsearch.cluster.metadata;
 
-import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.admin.indices.mapping.put.PutMappingClusterStateUpdateRequest;
 import org.elasticsearch.cluster.*;
@@ -36,15 +37,11 @@
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.NodeServicesProvider;
 import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.MergeMappingException;
-import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.indices.IndicesService;
-import org.elasticsearch.indices.InvalidTypeNameException;
-import org.elasticsearch.percolator.PercolatorService;
 
 import java.io.IOException;
 import java.util.*;
+
 /**
  * Service responsible for submitting mapping changes
  */
@@ -69,12 +66,10 @@ public MetaDataMappingService(Settings settings, ClusterService clusterService,
     static class RefreshTask {
         final String index;
         final String indexUUID;
-        final String[] types;
 
-        RefreshTask(String index, final String indexUUID, String[] types) {
+        RefreshTask(String index, final String indexUUID) {
             this.index = index;
             this.indexUUID = indexUUID;
-            this.types = types;
         }
     }
 
@@ -120,13 +115,16 @@ ClusterState executeRefresh(final ClusterState currentState, final List<RefreshT
             // the tasks lists to iterate over, filled with the list of mapping tasks, trying to keep
             // the latest (based on order) update mapping one per node
             List<RefreshTask> allIndexTasks = entry.getValue();
-            List<RefreshTask> tasks = new ArrayList<>();
+            boolean hasTaskWithRightUUID = false;
             for (RefreshTask task : allIndexTasks) {
-                if (!indexMetaData.isSameUUID(task.indexUUID)) {
+                if (indexMetaData.isSameUUID(task.indexUUID)) {
+                    hasTaskWithRightUUID = true;
+                } else {
                     logger.debug("[{}] ignoring task [{}] - index meta data doesn't match task uuid", index, task);
-                    continue;
                 }
-                tasks.add(task);
+            }
+            if (hasTaskWithRightUUID == false) {
+                continue;
             }
 
             // construct the actual index if needed, and make sure the relevant mappings are there
@@ -134,24 +132,19 @@ ClusterState executeRefresh(final ClusterState currentState, final List<RefreshT
             IndexService indexService = indicesService.indexService(index);
             if (indexService == null) {
                 // we need to create the index here, and add the current mapping to it, so we can merge
-                indexService = indicesService.createIndex(nodeServicesProvider, indexMetaData, Collections.EMPTY_LIST);
+                indexService = indicesService.createIndex(nodeServicesProvider, indexMetaData, Collections.emptyList());
                 removeIndex = true;
-                Set<String> typesToIntroduce = new HashSet<>();
-                for (RefreshTask task : tasks) {
-                    Collections.addAll(typesToIntroduce, task.types);
-                }
-                for (String type : typesToIntroduce) {
-                    // only add the current relevant mapping (if exists)
-                    if (indexMetaData.getMappings().containsKey(type)) {
-                        // don't apply the default mapping, it has been applied when the mapping was created
-                        indexService.mapperService().merge(type, indexMetaData.getMappings().get(type).source(), false, true);
-                    }
+                final Map<String, CompressedXContent> mappingSources = new HashMap<>();
+                for (ObjectObjectCursor<String, MappingMetaData> e : indexMetaData.getMappings()) {
+                    mappingSources.put(e.key, e.value.source());
                 }
+                // don't apply the default mapping, it has been applied when the mapping was created
+                indexService.mapperService().merge(mappingSources, false, true);
             }
 
             IndexMetaData.Builder builder = IndexMetaData.builder(indexMetaData);
             try {
-                boolean indexDirty = processIndexMappingTasks(tasks, indexService, builder);
+                boolean indexDirty = processIndexMapping(indexService, builder);
                 if (indexDirty) {
                     mdBuilder.put(builder);
                     dirty = true;
@@ -169,38 +162,26 @@ ClusterState executeRefresh(final ClusterState currentState, final List<RefreshT
         return ClusterState.builder(currentState).metaData(mdBuilder).build();
     }
 
-    private boolean processIndexMappingTasks(List<RefreshTask> tasks, IndexService indexService, IndexMetaData.Builder builder) {
+    private boolean processIndexMapping(IndexService indexService, IndexMetaData.Builder builder) {
         boolean dirty = false;
         String index = indexService.index().name();
-        // keep track of what we already refreshed, no need to refresh it again...
-        Set<String> processedRefreshes = new HashSet<>();
-        for (RefreshTask refreshTask : tasks) {
-            try {
-                List<String> updatedTypes = new ArrayList<>();
-                for (String type : refreshTask.types) {
-                    if (processedRefreshes.contains(type)) {
-                        continue;
-                    }
-                    DocumentMapper mapper = indexService.mapperService().documentMapper(type);
-                    if (mapper == null) {
-                        continue;
-                    }
-                    if (!mapper.mappingSource().equals(builder.mapping(type).source())) {
-                        updatedTypes.add(type);
-                        builder.putMapping(new MappingMetaData(mapper));
-                    }
-                    processedRefreshes.add(type);
-                }
-
-                if (updatedTypes.isEmpty()) {
-                    continue;
+        try {
+            List<String> updatedTypes = new ArrayList<>();
+            for (DocumentMapper mapper : indexService.mapperService().docMappers(true)) {
+                final String type = mapper.type();
+                if (builder.mapping(type) == null
+                        || !mapper.mappingSource().equals(builder.mapping(type).source())) {
+                    updatedTypes.add(type);
+                    builder.putMapping(new MappingMetaData(mapper));
                 }
+            }
 
+            if (updatedTypes.isEmpty() == false) {
                 logger.warn("[{}] re-syncing mappings with cluster state for types [{}]", index, updatedTypes);
                 dirty = true;
-            } catch (Throwable t) {
-                logger.warn("[{}] failed to refresh-mapping in cluster state, types [{}]", index, refreshTask.types);
             }
+        } catch (Throwable t) {
+            logger.warn("[{}] failed to refresh-mapping in cluster state", index);
         }
         return dirty;
     }
@@ -208,9 +189,9 @@ private boolean processIndexMappingTasks(List<RefreshTask> tasks, IndexService i
     /**
      * Refreshes mappings if they are not the same between original and parsed version
      */
-    public void refreshMapping(final String index, final String indexUUID, final String... types) {
-        final RefreshTask refreshTask = new RefreshTask(index, indexUUID, types);
-        clusterService.submitStateUpdateTask("refresh-mapping [" + index + "][" + Arrays.toString(types) + "]",
+    public void refreshMapping(final String index, final String indexUUID) {
+        final RefreshTask refreshTask = new RefreshTask(index, indexUUID);
+        clusterService.submitStateUpdateTask("refresh-mapping [" + index + "]",
                 refreshTask,
                 ClusterStateTaskConfig.build(Priority.HIGH),
                 refreshExectuor,
@@ -223,7 +204,6 @@ public void refreshMapping(final String index, final String indexUUID, final Str
         public BatchResult<PutMappingClusterStateUpdateRequest> execute(ClusterState currentState, List<PutMappingClusterStateUpdateRequest> tasks) throws Exception {
             List<String> indicesToClose = new ArrayList<>();
             BatchResult.Builder<PutMappingClusterStateUpdateRequest> builder = BatchResult.builder();
-            Map<PutMappingClusterStateUpdateRequest, TaskResult> executionResults = new HashMap<>();
             try {
                 // precreate incoming indices;
                 for (PutMappingClusterStateUpdateRequest request : tasks) {
@@ -235,14 +215,12 @@ public void refreshMapping(final String index, final String indexUUID, final Str
                                 final IndexMetaData indexMetaData = currentState.metaData().index(index);
                                 IndexService indexService = indicesService.createIndex(nodeServicesProvider, indexMetaData, Collections.EMPTY_LIST);
                                 indicesToClose.add(indexMetaData.getIndex());
-                                // make sure to add custom default mapping if exists
-                                if (indexMetaData.getMappings().containsKey(MapperService.DEFAULT_MAPPING)) {
-                                    indexService.mapperService().merge(MapperService.DEFAULT_MAPPING, indexMetaData.getMappings().get(MapperService.DEFAULT_MAPPING).source(), false, request.updateAllTypes());
-                                }
-                                // only add the current relevant mapping (if exists)
-                                if (indexMetaData.getMappings().containsKey(request.type())) {
-                                    indexService.mapperService().merge(request.type(), indexMetaData.getMappings().get(request.type()).source(), false, request.updateAllTypes());
+                                // we need to add all types since we perform cross-type validation
+                                final Map<String, CompressedXContent> mappingSources = new HashMap<>();
+                                for (ObjectObjectCursor<String, MappingMetaData> entry : indexMetaData.getMappings()) {
+                                    mappingSources.put(entry.key, entry.value.source());
                                 }
+                                indexService.mapperService().merge(mappingSources, false, request.updateAllTypes());
                             }
                         }
                     }
@@ -265,94 +243,27 @@ public void refreshMapping(final String index, final String indexUUID, final Str
         }
 
         private ClusterState applyRequest(ClusterState currentState, PutMappingClusterStateUpdateRequest request) throws IOException {
-            Map<String, DocumentMapper> newMappers = new HashMap<>();
-            Map<String, DocumentMapper> existingMappers = new HashMap<>();
-            for (String index : request.indices()) {
-                IndexService indexService = indicesService.indexServiceSafe(index);
-                // try and parse it (no need to add it here) so we can bail early in case of parsing exception
-                DocumentMapper newMapper;
-                DocumentMapper existingMapper = indexService.mapperService().documentMapper(request.type());
-                if (MapperService.DEFAULT_MAPPING.equals(request.type())) {
-                    // _default_ types do not go through merging, but we do test the new settings. Also don't apply the old default
-                    newMapper = indexService.mapperService().parse(request.type(), new CompressedXContent(request.source()), false);
-                } else {
-                    newMapper = indexService.mapperService().parse(request.type(), new CompressedXContent(request.source()), existingMapper == null);
-                    if (existingMapper != null) {
-                        // first, simulate
-                        MergeResult mergeResult = existingMapper.merge(newMapper.mapping(), true, request.updateAllTypes());
-                        // if we have conflicts, throw an exception
-                        if (mergeResult.hasConflicts()) {
-                            throw new MergeMappingException(mergeResult.buildConflicts());
-                        }
-                    } else {
-                        // TODO: can we find a better place for this validation?
-                        // The reason this validation is here is that the mapper service doesn't learn about
-                        // new types all at once , which can create a false error.
-
-                        // For example in MapperService we can't distinguish between a create index api call
-                        // and a put mapping api call, so we don't which type did exist before.
-                        // Also the order of the mappings may be backwards.
-                        if (newMapper.parentFieldMapper().active()) {
-                            IndexMetaData indexMetaData = currentState.metaData().index(index);
-                            for (ObjectCursor<MappingMetaData> mapping : indexMetaData.getMappings().values()) {
-                                if (newMapper.parentFieldMapper().type().equals(mapping.value.type())) {
-                                    throw new IllegalArgumentException("can't add a _parent field that points to an already existing type");
-                                }
-                            }
-                        }
-                    }
-                }
-                newMappers.put(index, newMapper);
-                if (existingMapper != null) {
-                    existingMappers.put(index, existingMapper);
-                }
-            }
+            final String mappingType = request.type();
 
-            String mappingType = request.type();
-            if (mappingType == null) {
-                mappingType = newMappers.values().iterator().next().type();
-            } else if (!mappingType.equals(newMappers.values().iterator().next().type())) {
-                throw new InvalidTypeNameException("Type name provided does not match type name within mapping definition");
-            }
-            if (!MapperService.DEFAULT_MAPPING.equals(mappingType) && !PercolatorService.TYPE_NAME.equals(mappingType) && mappingType.charAt(0) == '_') {
-                throw new InvalidTypeNameException("Document mapping type name can't start with '_'");
-            }
             final Map<String, MappingMetaData> mappings = new HashMap<>();
-            for (Map.Entry<String, DocumentMapper> entry : newMappers.entrySet()) {
-                String index = entry.getKey();
-                // do the actual merge here on the master, and update the mapping source
-                DocumentMapper newMapper = entry.getValue();
-                IndexService indexService = indicesService.indexService(index);
-                if (indexService == null) {
-                    continue;
-                }
-
+            for (String index : request.indices()) {
+                IndexService indexService = indicesService.indexServiceSafe(index);
                 CompressedXContent existingSource = null;
-                if (existingMappers.containsKey(entry.getKey())) {
-                    existingSource = existingMappers.get(entry.getKey()).mappingSource();
+                DocumentMapper oldMapper = indexService.mapperService().documentMapper(mappingType);
+                if (oldMapper != null) {
+                    existingSource = oldMapper.mappingSource();
                 }
-                DocumentMapper mergedMapper = indexService.mapperService().merge(newMapper.type(), newMapper.mappingSource(), false, request.updateAllTypes());
+                DocumentMapper mergedMapper = indexService.mapperService().merge(Collections.singletonMap(mappingType, new CompressedXContent(request.source())), true, request.updateAllTypes()).get(mappingType);
                 CompressedXContent updatedSource = mergedMapper.mappingSource();
 
-                if (existingSource != null) {
-                    if (existingSource.equals(updatedSource)) {
-                        // same source, no changes, ignore it
-                    } else {
-                        // use the merged mapping source
-                        mappings.put(index, new MappingMetaData(mergedMapper));
-                        if (logger.isDebugEnabled()) {
-                            logger.debug("[{}] update_mapping [{}] with source [{}]", index, mergedMapper.type(), updatedSource);
-                        } else if (logger.isInfoEnabled()) {
-                            logger.info("[{}] update_mapping [{}]", index, mergedMapper.type());
-                        }
-
-                    }
-                } else {
+                if (existingSource == null || existingSource.equals(updatedSource) == false) {
+                    // the mapping was modified
+                    final String op = existingSource == null ? "create" : "update";
                     mappings.put(index, new MappingMetaData(mergedMapper));
                     if (logger.isDebugEnabled()) {
-                        logger.debug("[{}] create_mapping [{}] with source [{}]", index, newMapper.type(), updatedSource);
+                        logger.debug("[{}] {}_mapping [{}] with source [{}]", index, op, mergedMapper.type(), updatedSource);
                     } else if (logger.isInfoEnabled()) {
-                        logger.info("[{}] create_mapping [{}]", index, newMapper.type());
+                        logger.info("[{}] {}_mapping [{}]", index, op, mergedMapper.type());
                     }
                 }
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
index 5f266cbd48fa..7ef36e8c9c22 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
@@ -336,8 +336,6 @@ public boolean isParent(String type) {
 
     private void addMappers(Collection<ObjectMapper> objectMappers, Collection<FieldMapper> fieldMappers, boolean updateAllTypes) {
         assert mappingLock.isWriteLockedByCurrentThread();
-        // first ensure we don't have any incompatible new fields
-        mapperService.checkNewMappersCompatibility(objectMappers, fieldMappers, updateAllTypes);
 
         // update mappers for this document type
         Map<String, ObjectMapper> builder = new HashMap<>(this.objectMappers);
@@ -354,15 +352,25 @@ private void addMappers(Collection<ObjectMapper> objectMappers, Collection<Field
         mapperService.addMappers(objectMappers, fieldMappers);
     }
 
-    public MergeResult merge(Mapping mapping, boolean simulate, boolean updateAllTypes) {
+    public void merge(Mapping mapping, boolean updateAllTypes) {
         try (ReleasableLock lock = mappingWriteLock.acquire()) {
-            final MergeResult mergeResult = new MergeResult(simulate, updateAllTypes);
+            // first simulate to only check for conflicts
+            MergeResult mergeResult = new MergeResult(true, updateAllTypes);
             this.mapping.merge(mapping, mergeResult);
-            if (simulate == false) {
-                addMappers(mergeResult.getNewObjectMappers(), mergeResult.getNewFieldMappers(), updateAllTypes);
-                refreshSource();
+            if (mergeResult.hasConflicts()) {
+                throw new MergeMappingException(mergeResult.buildConflicts());
             }
-            return mergeResult;
+
+            // then check conflicts with other types
+            // if there is a single type then we are implicitly updating all types
+            mapperService.checkMappersCompatibility(mapping, updateAllTypes || mapperService.types().size() == 1);
+
+            // finally apply the mapping update for real
+            mergeResult = new MergeResult(false, updateAllTypes);
+            assert mergeResult.hasConflicts() == false; // we already simulated
+            this.mapping.merge(mapping, mergeResult);
+            addMappers(mergeResult.getNewObjectMappers(), mergeResult.getNewFieldMappers(), updateAllTypes);
+            refreshSource();
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
index 45bef68ee000..2fbfcf7ee4ae 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
@@ -387,7 +387,8 @@ public void merge(Mapper mergeWith, MergeResult mergeResult) throws MergeMapping
         }
         multiFields.merge(mergeWith, mergeResult);
 
-        if (mergeResult.simulate() == false && mergeResult.hasConflicts() == false) {
+        if (mergeResult.simulate() == false) {
+            assert mergeResult.hasConflicts() == false;
             // apply changeable values
             MappedFieldType fieldType = fieldMergeWith.fieldType().clone();
             fieldType.freeze();
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/FieldTypeLookup.java b/core/src/main/java/org/elasticsearch/index/mapper/FieldTypeLookup.java
index 1b0e827ac35d..ae5e0ec6e148 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/FieldTypeLookup.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/FieldTypeLookup.java
@@ -95,8 +95,8 @@ public FieldTypeLookup copyAndAddAll(Collection<FieldMapper> newFieldMappers) {
      * If any are not compatible, an IllegalArgumentException is thrown.
      * If updateAllTypes is true, only basic compatibility is checked.
      */
-    public void checkCompatibility(Collection<FieldMapper> newFieldMappers, boolean updateAllTypes) {
-        for (FieldMapper fieldMapper : newFieldMappers) {
+    void checkCompatibility(Collection<FieldMapper> fieldMappers, boolean updateAllTypes) {
+        for (FieldMapper fieldMapper : fieldMappers) {
             MappedFieldTypeReference ref = fullNameToFieldType.get(fieldMapper.fieldType().names().fullName());
             if (ref != null) {
                 List<String> conflicts = new ArrayList<>();
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java b/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
index f617dd5c6f03..c3d9c4111f3b 100755
--- a/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
@@ -80,7 +80,7 @@
     private final boolean dynamic;
 
     private volatile String defaultMappingSource;
-    private volatile String defaultPercolatorMappingSource;
+    private final String defaultPercolatorMappingSource;
 
     private volatile Map<String, DocumentMapper> mappers = emptyMap();
 
@@ -195,92 +195,128 @@ public void removeTypeListener(DocumentTypeListener listener) {
         typeListeners.remove(listener);
     }
 
-    public DocumentMapper merge(String type, CompressedXContent mappingSource, boolean applyDefault, boolean updateAllTypes) {
-        if (DEFAULT_MAPPING.equals(type)) {
-            // verify we can parse it
-            DocumentMapper mapper = documentParser.parseCompressed(type, mappingSource);
-            // still add it as a document mapper so we have it registered and, for example, persisted back into
-            // the cluster meta data if needed, or checked for existence
-            try (ReleasableLock lock = mappingWriteLock.acquire()) {
-                mappers = newMapBuilder(mappers).put(type, mapper).map();
-            }
+    /**
+     * Merge the provided mapping sources and return the new map of document
+     * mappers, once all updates have been applied.
+     */
+    public Map<String, DocumentMapper> merge(Map<String, CompressedXContent> mappingSources, boolean applyDefault, boolean updateAllTypes) {
+        try (ReleasableLock lock = mappingWriteLock.acquire()) {
+            return doMerge(mappingSources, applyDefault, updateAllTypes);
+        }
+    }
+
+    private Map<String, DocumentMapper> doMerge(Map<String, CompressedXContent> mappingSources, boolean applyDefault, boolean updateAllTypes) {
+        final Set<String> preExistingTypes = Collections.unmodifiableSet(new HashSet<>(mappers.keySet()));
+        String defaultMappingSource = this.defaultMappingSource;
+        DocumentMapper defaultMapper = null;
+
+        // merge the default mapping first, so that it applies to other mappings
+        if (mappingSources.containsKey(DEFAULT_MAPPING)) {
+            final CompressedXContent mappingSource = mappingSources.get(DEFAULT_MAPPING);
+            defaultMapper = documentParser.parseCompressed(DEFAULT_MAPPING, mappingSource);
             try {
                 defaultMappingSource = mappingSource.string();
             } catch (IOException e) {
                 throw new ElasticsearchGenerationException("failed to un-compress", e);
             }
-            return mapper;
-        } else {
-            return merge(parse(type, mappingSource, applyDefault), updateAllTypes);
         }
-    }
 
-    // never expose this to the outside world, we need to reparse the doc mapper so we get fresh
-    // instances of field mappers to properly remove existing doc mapper
-    private DocumentMapper merge(DocumentMapper mapper, boolean updateAllTypes) {
-        try (ReleasableLock lock = mappingWriteLock.acquire()) {
-            if (mapper.type().length() == 0) {
-                throw new InvalidTypeNameException("mapping type name is empty");
-            }
-            if (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_2_0_0_beta1) && mapper.type().length() > 255) {
-                throw new InvalidTypeNameException("mapping type name [" + mapper.type() + "] is too long; limit is length 255 but was [" + mapper.type().length() + "]");
-            }
-            if (mapper.type().charAt(0) == '_') {
-                throw new InvalidTypeNameException("mapping type name [" + mapper.type() + "] can't start with '_'");
-            }
-            if (mapper.type().contains("#")) {
-                throw new InvalidTypeNameException("mapping type name [" + mapper.type() + "] should not include '#' in it");
+        for (Map.Entry<String, CompressedXContent> entry : mappingSources.entrySet()) {
+            final String type = entry.getKey();
+            if (type.equals(DEFAULT_MAPPING)) {
+                continue;
             }
-            if (mapper.type().contains(",")) {
-                throw new InvalidTypeNameException("mapping type name [" + mapper.type() + "] should not include ',' in it");
+
+            final CompressedXContent mappingSource = entry.getValue();
+            final String defaultSource;
+            if (preExistingTypes.contains(type) || applyDefault == false) {
+                defaultSource = null;
+            } else if (PercolatorService.TYPE_NAME.equals(type)) {
+                defaultSource = defaultPercolatorMappingSource;
+            } else {
+                defaultSource = defaultMappingSource;
             }
-            if (mapper.type().equals(mapper.parentFieldMapper().type())) {
-                throw new IllegalArgumentException("The [_parent.type] option can't point to the same type");
+
+            final DocumentMapper mapper = documentParser.parseCompressed(type, mappingSource, defaultSource);
+
+            if (mapper.type().equals(type) == false) {
+                throw new InvalidTypeNameException("Type name provided does not match type name within mapping definition");
             }
-            if (typeNameStartsWithIllegalDot(mapper)) {
-                if (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
-                    throw new IllegalArgumentException("mapping type name [" + mapper.type() + "] must not start with a '.'");
-                } else {
-                    logger.warn("Type [{}] starts with a '.', it is recommended not to start a type name with a '.'", mapper.type());
-                }
+
+            if (mapper.parentFieldMapper().active()
+                    && preExistingTypes.contains(type) == false
+                    && preExistingTypes.contains(mapper.parentFieldMapper().type())) {
+                throw new IllegalArgumentException("can't add a _parent field that points to an already existing type");
             }
-            // we can add new field/object mappers while the old ones are there
-            // since we get new instances of those, and when we remove, we remove
-            // by instance equality
-            DocumentMapper oldMapper = mappers.get(mapper.type());
 
-            if (oldMapper != null) {
-                MergeResult result = oldMapper.merge(mapper.mapping(), false, updateAllTypes);
-                if (result.hasConflicts()) {
-                    // TODO: What should we do???
-                    if (logger.isDebugEnabled()) {
-                        logger.debug("merging mapping for type [{}] resulted in conflicts: [{}]", mapper.type(), Arrays.toString(result.buildConflicts()));
-                    }
-                }
-                return oldMapper;
+            merge(mapper, updateAllTypes);
+        }
+
+        // Serialize defaults
+        if (defaultMapper != null) {
+            // update the default mapping source
+            this.defaultMappingSource = defaultMappingSource;
+            // still add it as a document mapper so we have it registered and, for example, persisted back into
+            // the cluster meta data if needed, or checked for existence
+            mappers = newMapBuilder(mappers).put(DEFAULT_MAPPING, defaultMapper).immutableMap();
+        }
+
+        return mappers;
+    }
+
+    private DocumentMapper merge(DocumentMapper mapper, boolean updateAllTypes) {
+        if (mapper.type().length() == 0) {
+            throw new InvalidTypeNameException("mapping type name is empty");
+        }
+        if (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_2_0_0_beta1) && mapper.type().length() > 255) {
+            throw new InvalidTypeNameException("mapping type name [" + mapper.type() + "] is too long; limit is length 255 but was [" + mapper.type().length() + "]");
+        }
+        if (mapper.type().charAt(0) == '_') {
+            throw new InvalidTypeNameException("mapping type name [" + mapper.type() + "] can't start with '_'");
+        }
+        if (mapper.type().contains("#")) {
+            throw new InvalidTypeNameException("mapping type name [" + mapper.type() + "] should not include '#' in it");
+        }
+        if (mapper.type().contains(",")) {
+            throw new InvalidTypeNameException("mapping type name [" + mapper.type() + "] should not include ',' in it");
+        }
+        if (mapper.type().equals(mapper.parentFieldMapper().type())) {
+            throw new IllegalArgumentException("The [_parent.type] option can't point to the same type");
+        }
+        if (typeNameStartsWithIllegalDot(mapper)) {
+            if (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
+                throw new IllegalArgumentException("mapping type name [" + mapper.type() + "] must not start with a '.'");
             } else {
-                List<ObjectMapper> newObjectMappers = new ArrayList<>();
-                List<FieldMapper> newFieldMappers = new ArrayList<>();
-                for (MetadataFieldMapper metadataMapper : mapper.mapping().metadataMappers) {
-                    newFieldMappers.add(metadataMapper);
-                }
-                MapperUtils.collect(mapper.mapping().root, newObjectMappers, newFieldMappers);
-                checkNewMappersCompatibility(newObjectMappers, newFieldMappers, updateAllTypes);
-                addMappers(newObjectMappers, newFieldMappers);
+                logger.warn("Type [{}] starts with a '.', it is recommended not to start a type name with a '.'", mapper.type());
+            }
+        }
+        DocumentMapper oldMapper = mappers.get(mapper.type());
 
-                for (DocumentTypeListener typeListener : typeListeners) {
-                    typeListener.beforeCreate(mapper);
-                }
-                mappers = newMapBuilder(mappers).put(mapper.type(), mapper).map();
-                if (mapper.parentFieldMapper().active()) {
-                    Set<String> newParentTypes = new HashSet<>(parentTypes.size() + 1);
-                    newParentTypes.addAll(parentTypes);
-                    newParentTypes.add(mapper.parentFieldMapper().type());
-                    parentTypes = unmodifiableSet(newParentTypes);
-                }
-                assert assertSerialization(mapper);
-                return mapper;
+        if (oldMapper != null) {
+            oldMapper.merge(mapper.mapping(), updateAllTypes);
+            return oldMapper;
+        } else {
+            List<ObjectMapper> objectMappers = new ArrayList<>();
+            List<FieldMapper> fieldMappers = new ArrayList<>();
+            for (MetadataFieldMapper metadataMapper : mapper.mapping().metadataMappers) {
+                fieldMappers.add(metadataMapper);
             }
+            MapperUtils.collect(mapper.mapping().root, objectMappers, fieldMappers);
+            checkMappersCompatibility(objectMappers, fieldMappers, updateAllTypes);
+            addMappers(objectMappers, fieldMappers);
+
+            for (DocumentTypeListener typeListener : typeListeners) {
+                typeListener.beforeCreate(mapper);
+            }
+            mappers = newMapBuilder(mappers).put(mapper.type(), mapper).immutableMap();
+            if (mapper.parentFieldMapper().active()) {
+                Set<String> newParentTypes = new HashSet<>(parentTypes.size() + 1);
+                newParentTypes.addAll(parentTypes);
+                newParentTypes.add(mapper.parentFieldMapper().type());
+                parentTypes = unmodifiableSet(newParentTypes);
+            }
+            assert assertSerialization(mapper);
+            return mapper;
         }
     }
 
@@ -291,7 +327,7 @@ private boolean typeNameStartsWithIllegalDot(DocumentMapper mapper) {
     private boolean assertSerialization(DocumentMapper mapper) {
         // capture the source now, it may change due to concurrent parsing
         final CompressedXContent mappingSource = mapper.mappingSource();
-        DocumentMapper newMapper = parse(mapper.type(), mappingSource, false);
+        DocumentMapper newMapper = documentParser.parseCompressed(mapper.type(), mappingSource);
 
         if (newMapper.mappingSource().equals(mappingSource) == false) {
             throw new IllegalStateException("DocumentMapper serialization result is different from source. \n--> Source ["
@@ -301,9 +337,37 @@ private boolean assertSerialization(DocumentMapper mapper) {
         return true;
     }
 
-    protected void checkNewMappersCompatibility(Collection<ObjectMapper> newObjectMappers, Collection<FieldMapper> newFieldMappers, boolean updateAllTypes) {
+    /**
+     * Check that fields are defined only once. It is possible to define fields
+     * several times eg. if a field is defined once via 'fields' in the mapping
+     * and once from a field mapper.
+     */
+    private void checkUniqueness(Collection<ObjectMapper> objectMappers, Collection<FieldMapper> fieldMappers) {
+        Map<String, Mapper> alreadySeen = new HashMap<>();
+        for (ObjectMapper mapper : objectMappers) {
+            final Mapper removed = alreadySeen.put(mapper.fullPath(), mapper);
+            if (removed != null) {
+                throw new IllegalArgumentException("Two fields are defined for path [" + mapper.fullPath() + "]: " + Arrays.asList(mapper, removed));
+            }
+        }
+        for (FieldMapper mapper : fieldMappers) {
+            final Mapper removed = alreadySeen.put(mapper.name(), mapper);
+            if (removed != null && (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_2_0_0) || mapper != removed)) {
+                // we need the 'removed != mapper' condition because some metadata mappers used to be registered both as a metadata mapper and
+                // as a sub mapper of the root object mapper
+                throw new IllegalArgumentException("Two fields are defined for path [" + mapper.name() + "]: " + Arrays.asList(mapper, removed));
+            }
+        }
+    }
+
+    protected void checkMappersCompatibility(Collection<ObjectMapper> objectMappers, Collection<FieldMapper> fieldMappers, boolean updateAllTypes) {
         assert mappingLock.isWriteLockedByCurrentThread();
-        for (ObjectMapper newObjectMapper : newObjectMappers) {
+
+        // check compatibility within the mapping update
+        checkUniqueness(objectMappers, fieldMappers);
+
+        // check compatibility with existing types
+        for (ObjectMapper newObjectMapper : objectMappers) {
             ObjectMapper existingObjectMapper = fullPathObjectMappers.get(newObjectMapper.fullPath());
             if (existingObjectMapper != null) {
                 MergeResult result = new MergeResult(true, updateAllTypes);
@@ -314,7 +378,18 @@ protected void checkNewMappersCompatibility(Collection<ObjectMapper> newObjectMa
                 }
             }
         }
-        fieldTypes.checkCompatibility(newFieldMappers, updateAllTypes);
+        fieldTypes.checkCompatibility(fieldMappers, updateAllTypes);
+    }
+
+    protected void checkMappersCompatibility(Mapping mapping, boolean updateAllTypes) {
+        // First check compatibility of the new mapping with other types
+        List<ObjectMapper> objectMappers = new ArrayList<>();
+        List<FieldMapper> fieldMappers = new ArrayList<>();
+        for (MetadataFieldMapper metadataMapper : mapping.metadataMappers) {
+            fieldMappers.add(metadataMapper);
+        }
+        MapperUtils.collect(mapping.root(), objectMappers, fieldMappers);
+        checkMappersCompatibility(objectMappers, fieldMappers, updateAllTypes);
     }
 
     protected void addMappers(Collection<ObjectMapper> objectMappers, Collection<FieldMapper> fieldMappers) {
@@ -330,22 +405,19 @@ protected void addMappers(Collection<ObjectMapper> objectMappers, Collection<Fie
         this.fieldTypes = this.fieldTypes.copyAndAddAll(fieldMappers);
     }
 
-    public DocumentMapper parse(String mappingType, CompressedXContent mappingSource, boolean applyDefault) throws MapperParsingException {
-        String defaultMappingSource;
-        if (PercolatorService.TYPE_NAME.equals(mappingType)) {
-            defaultMappingSource = this.defaultPercolatorMappingSource;
-        }  else {
-            defaultMappingSource = this.defaultMappingSource;
-        }
-        return documentParser.parseCompressed(mappingType, mappingSource, applyDefault ? defaultMappingSource : null);
-    }
-
     public boolean hasMapping(String mappingType) {
         return mappers.containsKey(mappingType);
     }
 
+    /**
+     * Return the list of the active types in this index.
+     * NOTE: even if a default mapping has been specified, it will not be
+     * included in the results.
+     */
     public Collection<String> types() {
-        return mappers.keySet();
+        final Set<String> types = new HashSet<>(mappers.keySet());
+        types.remove(DEFAULT_MAPPING);
+        return Collections.unmodifiableSet(types);
     }
 
     public DocumentMapper documentMapper(String type) {
@@ -364,7 +436,10 @@ public DocumentMapperForType documentMapperWithAutoCreate(String type) {
         if (!dynamic) {
             throw new TypeMissingException(index(), type, "trying to auto create mapping, but dynamic mapping is disabled");
         }
-        mapper = parse(type, null, true);
+        final String defaultMappingSource = PercolatorService.TYPE_NAME.equals(type)
+                ? this.defaultPercolatorMappingSource
+                : this.defaultMappingSource;
+        mapper = documentParser.parse(type, null, defaultMappingSource);
         return new DocumentMapperForType(mapper, mapper.mapping());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java b/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java
index bac421625528..d04078f9231a 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java
@@ -89,7 +89,7 @@ public Mapping mappingUpdate(Mapper rootObjectMapper) {
         return (T) metadataMappersMap.get(clazz);
     }
 
-    /** @see DocumentMapper#merge(Mapping, boolean, boolean) */
+    /** @see DocumentMapper#merge(Mapping, boolean) */
     public void merge(Mapping mergeWith, MergeResult mergeResult) {
         assert metadataMappers.length == mergeWith.metadataMappers.length;
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java
index 3fba511fb52d..26b0496400f2 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java
@@ -257,7 +257,8 @@ public void merge(Mapper mergeWith, MergeResult mergeResult) throws MergeMapping
             }
         }
 
-        if (mergeResult.simulate() == false && mergeResult.hasConflicts() == false) {
+        if (mergeResult.simulate() == false) {
+            assert mergeResult.hasConflicts() == false;
             this.includeInAll = nfmMergeWith.includeInAll;
             if (nfmMergeWith.ignoreMalformed.explicit()) {
                 this.ignoreMalformed = nfmMergeWith.ignoreMalformed;
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/geo/BaseGeoPointFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/geo/BaseGeoPointFieldMapper.java
index 1c33b66fbc88..94d41a210e4e 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/geo/BaseGeoPointFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/geo/BaseGeoPointFieldMapper.java
@@ -396,7 +396,8 @@ public void merge(Mapper mergeWith, MergeResult mergeResult) throws MergeMapping
         }
 
         BaseGeoPointFieldMapper gpfmMergeWith = (BaseGeoPointFieldMapper) mergeWith;
-        if (mergeResult.simulate() == false && mergeResult.hasConflicts() == false) {
+        if (mergeResult.simulate() == false) {
+            assert mergeResult.hasConflicts() == false;
             if (gpfmMergeWith.ignoreMalformed.explicit()) {
                 this.ignoreMalformed = gpfmMergeWith.ignoreMalformed;
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperLegacy.java b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperLegacy.java
index 350c8f5d668c..1db5ff8283c1 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperLegacy.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperLegacy.java
@@ -311,7 +311,8 @@ public void merge(Mapper mergeWith, MergeResult mergeResult) throws MergeMapping
             }
         }
 
-        if (mergeResult.simulate() == false && mergeResult.hasConflicts() == false) {
+        if (mergeResult.simulate() == false) {
+            assert mergeResult.hasConflicts() == false;
             if (gpfmMergeWith.coerce.explicit()) {
                 this.coerce = gpfmMergeWith.coerce;
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java
index 6d385875b187..c78028a2c194 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java
@@ -479,7 +479,8 @@ public void merge(Mapper mergeWith, MergeResult mergeResult) throws MergeMapping
         }
 
         GeoShapeFieldMapper gsfm = (GeoShapeFieldMapper)mergeWith;
-        if (mergeResult.simulate() == false && mergeResult.hasConflicts() == false) {
+        if (mergeResult.simulate() == false) {
+            assert mergeResult.hasConflicts() == false;
             if (gsfm.coerce.explicit()) {
                 this.coerce = gsfm.coerce;
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
index d6221ae1db3a..fd962012bd30 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
@@ -390,7 +390,8 @@ public void merge(Mapper mergeWith, MergeResult mergeResult) throws MergeMapping
             mergeResult.addConflict(conflict);
         }
 
-        if (active() && mergeResult.simulate() == false && mergeResult.hasConflicts() == false) {
+        if (active() && mergeResult.simulate() == false) {
+            assert mergeResult.hasConflicts() == false;
             childJoinFieldType = fieldMergeWith.childJoinFieldType.clone();
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
index cd60b87765a9..f46173503c51 100644
--- a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
+++ b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
@@ -20,7 +20,8 @@
 package org.elasticsearch.indices.cluster;
 
 import com.carrotsearch.hppc.IntHashSet;
-import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.cluster.ClusterChangedEvent;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
@@ -35,7 +36,6 @@
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.inject.Inject;
@@ -83,10 +83,6 @@
 
     private static final ShardStateAction.Listener SHARD_STATE_ACTION_LISTENER = new NoOpShardStateActionListener();
 
-    // a map of mappings type we have seen per index due to cluster state
-    // we need this so we won't remove types automatically created as part of the indexing process
-    private final ConcurrentMap<Tuple<String, String>, Boolean> seenMappings = ConcurrentCollections.newConcurrentMap();
-
     // a list of shards that failed during recovery
     // we keep track of these shards in order to prevent repeated recovery of these shards on each cluster state update
     private final ConcurrentMap<ShardId, FailedShard> failedShards = ConcurrentCollections.newConcurrentMap();
@@ -245,7 +241,7 @@ private void applyDeletedIndices(final ClusterChangedEvent event) {
             } else {
                 final IndexMetaData metaData = previousState.metaData().index(index);
                 assert metaData != null;
-                indexSettings = new IndexSettings(metaData, settings, Collections.EMPTY_LIST);
+                indexSettings = new IndexSettings(metaData, settings, Collections.emptyList());
                 indicesService.deleteClosedIndex("closed index no longer part of the metadata", metaData, event.state());
             }
             try {
@@ -341,7 +337,6 @@ private void applySettings(ClusterChangedEvent event) {
         }
     }
 
-
     private void applyMappings(ClusterChangedEvent event) {
         // go over and update mappings
         for (IndexMetaData indexMetaData : event.state().metaData()) {
@@ -349,7 +344,6 @@ private void applyMappings(ClusterChangedEvent event) {
                 // we only create / update here
                 continue;
             }
-            List<String> typesToRefresh = new ArrayList<>();
             String index = indexMetaData.getIndex();
             IndexService indexService = indicesService.indexService(index);
             if (indexService == null) {
@@ -357,32 +351,13 @@ private void applyMappings(ClusterChangedEvent event) {
                 return;
             }
             try {
-                MapperService mapperService = indexService.mapperService();
-                // first, go over and update the _default_ mapping (if exists)
-                if (indexMetaData.getMappings().containsKey(MapperService.DEFAULT_MAPPING)) {
-                    boolean requireRefresh = processMapping(index, mapperService, MapperService.DEFAULT_MAPPING, indexMetaData.mapping(MapperService.DEFAULT_MAPPING).source());
-                    if (requireRefresh) {
-                        typesToRefresh.add(MapperService.DEFAULT_MAPPING);
-                    }
-                }
-
-                // go over and add the relevant mappings (or update them)
-                for (ObjectCursor<MappingMetaData> cursor : indexMetaData.getMappings().values()) {
-                    MappingMetaData mappingMd = cursor.value;
-                    String mappingType = mappingMd.type();
-                    CompressedXContent mappingSource = mappingMd.source();
-                    if (mappingType.equals(MapperService.DEFAULT_MAPPING)) { // we processed _default_ first
-                        continue;
-                    }
-                    boolean requireRefresh = processMapping(index, mapperService, mappingType, mappingSource);
-                    if (requireRefresh) {
-                        typesToRefresh.add(mappingType);
-                    }
+                Map<String, CompressedXContent> mappingSources = new HashMap<>();
+                for (ObjectObjectCursor<String, MappingMetaData> entry : indexMetaData.getMappings()) {
+                    mappingSources.put(entry.key, entry.value.source());
                 }
-                if (!typesToRefresh.isEmpty() && sendRefreshMapping) {
+                if (processMappings(index, indexService.mapperService(), mappingSources) && sendRefreshMapping) {
                     nodeMappingRefreshAction.nodeMappingRefresh(event.state(),
-                            new NodeMappingRefreshAction.NodeMappingRefreshRequest(index, indexMetaData.getIndexUUID(),
-                                    typesToRefresh.toArray(new String[typesToRefresh.size()]), event.state().nodes().localNodeId())
+                            new NodeMappingRefreshAction.NodeMappingRefreshRequest(index, indexMetaData.getIndexUUID(), event.state().nodes().localNodeId())
                     );
                 }
             } catch (Throwable t) {
@@ -397,61 +372,43 @@ private void applyMappings(ClusterChangedEvent event) {
         }
     }
 
-    private boolean processMapping(String index, MapperService mapperService, String mappingType, CompressedXContent mappingSource) throws Throwable {
-        if (!seenMappings.containsKey(new Tuple<>(index, mappingType))) {
-            seenMappings.put(new Tuple<>(index, mappingType), true);
-        }
-
-        // refresh mapping can happen for 2 reasons. The first is less urgent, and happens when the mapping on this
-        // node is ahead of what there is in the cluster state (yet an update-mapping has been sent to it already,
-        // it just hasn't been processed yet and published). Eventually, the mappings will converge, and the refresh
-        // mapping sent is more of a safe keeping (assuming the update mapping failed to reach the master, ...)
-        // the second case is where the parsing/merging of the mapping from the metadata doesn't result in the same
+    private boolean processMappings(String index, MapperService mapperService, Map<String, CompressedXContent> mappingSources) throws Throwable {
+        // refresh mapping can happen when the parsing/merging of the mapping from the metadata doesn't result in the same
         // mapping, in this case, we send to the master to refresh its own version of the mappings (to conform with the
         // merge version of it, which it does when refreshing the mappings), and warn log it.
-        boolean requiresRefresh = false;
+
         try {
-            if (!mapperService.hasMapping(mappingType)) {
+            // we don't apply default, since it has been applied when the mappings were parsed initially
+            final Collection<String> preExistingTypes = mapperService.types();
+            final Map<String, DocumentMapper> newMappers = mapperService.merge(mappingSources, false, true);
+
+            boolean requiresRefresh = false;
+            for (Map.Entry<String, CompressedXContent> entry : mappingSources.entrySet()) {
+                final String type = entry.getKey();
+                final CompressedXContent mappingSource = entry.getValue();
+
+                final String operation = preExistingTypes.contains(type) ? "updating" : "creating";
                 if (logger.isDebugEnabled() && mappingSource.compressed().length < 512) {
-                    logger.debug("[{}] adding mapping [{}], source [{}]", index, mappingType, mappingSource.string());
+                    logger.debug("[{}] {} mapping [{}], source [{}]", index, operation, type, mappingSource.string());
                 } else if (logger.isTraceEnabled()) {
-                    logger.trace("[{}] adding mapping [{}], source [{}]", index, mappingType, mappingSource.string());
+                    logger.trace("[{}] {} mapping [{}], source [{}]", index, operation, type, mappingSource.string());
                 } else {
-                    logger.debug("[{}] adding mapping [{}] (source suppressed due to length, use TRACE level if needed)", index, mappingType);
+                    logger.debug("[{}] {} mapping [{}] (source suppressed due to length, use TRACE level if needed)", index, operation, type);
                 }
-                // we don't apply default, since it has been applied when the mappings were parsed initially
-                mapperService.merge(mappingType, mappingSource, false, true);
-                if (!mapperService.documentMapper(mappingType).mappingSource().equals(mappingSource)) {
-                    logger.debug("[{}] parsed mapping [{}], and got different sources\noriginal:\n{}\nparsed:\n{}", index, mappingType, mappingSource, mapperService.documentMapper(mappingType).mappingSource());
+
+                final DocumentMapper mergedMapper = newMappers.get(type);
+                if (mergedMapper.mappingSource().equals(mappingSource) == false) {
                     requiresRefresh = true;
-                }
-            } else {
-                DocumentMapper existingMapper = mapperService.documentMapper(mappingType);
-                if (!mappingSource.equals(existingMapper.mappingSource())) {
-                    // mapping changed, update it
-                    if (logger.isDebugEnabled() && mappingSource.compressed().length < 512) {
-                        logger.debug("[{}] updating mapping [{}], source [{}]", index, mappingType, mappingSource.string());
-                    } else if (logger.isTraceEnabled()) {
-                        logger.trace("[{}] updating mapping [{}], source [{}]", index, mappingType, mappingSource.string());
-                    } else {
-                        logger.debug("[{}] updating mapping [{}] (source suppressed due to length, use TRACE level if needed)", index, mappingType);
-                    }
-                    // we don't apply default, since it has been applied when the mappings were parsed initially
-                    mapperService.merge(mappingType, mappingSource, false, true);
-                    if (!mapperService.documentMapper(mappingType).mappingSource().equals(mappingSource)) {
-                        requiresRefresh = true;
-                        logger.debug("[{}] parsed mapping [{}], and got different sources\noriginal:\n{}\nparsed:\n{}", index, mappingType, mappingSource, mapperService.documentMapper(mappingType).mappingSource());
-                    }
+                    logger.debug("[{}] parsed mapping [{}], and got different sources\noriginal:\n{}\nparsed:\n{}", index, type, mappingSource, mergedMapper.mappingSource());
                 }
             }
+            return requiresRefresh;
         } catch (Throwable e) {
-            logger.warn("[{}] failed to add mapping [{}], source [{}]", e, index, mappingType, mappingSource);
+            logger.warn("[{}] failed to process mappings: {}", e, index, mappingSources);
             throw e;
         }
-        return requiresRefresh;
     }
 
-
     private void applyNewOrUpdatedShards(final ClusterChangedEvent event) {
         if (!indicesService.changesAllowed()) {
             return;
@@ -781,17 +738,7 @@ private void removeIndex(String index, String reason) {
         } catch (Throwable e) {
             logger.warn("failed to clean index ({})", e, reason);
         }
-        clearSeenMappings(index);
-
-    }
 
-    private void clearSeenMappings(String index) {
-        // clear seen mappings as well
-        for (Tuple<String, String> tuple : seenMappings.keySet()) {
-            if (tuple.v1().equals(index)) {
-                seenMappings.remove(tuple);
-            }
-        }
     }
 
     private void deleteIndex(String index, String reason) {
@@ -800,8 +747,6 @@ private void deleteIndex(String index, String reason) {
         } catch (Throwable e) {
             logger.warn("failed to delete index ({})", e, reason);
         }
-        // clear seen mappings as well
-        clearSeenMappings(index);
 
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java
index eefe8c891833..fc1fbd205bfb 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java
@@ -59,12 +59,10 @@
 
     @Before
     public void before() throws Exception {
-        mapperService.merge(
-                childType, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(childType, "_parent", "type=" + parentType).string()), true, false
-        );
-        mapperService.merge(
-                grandChildType, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(grandChildType, "_parent", "type=" + childType).string()), true, false
-        );
+        Map<String, CompressedXContent> mappingSources = new HashMap<>();
+        mappingSources.put(childType, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(childType, "_parent", "type=" + parentType).string()));
+        mappingSources.put(grandChildType, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(grandChildType, "_parent", "type=" + childType).string()));
+        mapperService.merge(mappingSources, true, false);
 
         Document d = new Document();
         d.add(new StringField(UidFieldMapper.NAME, Uid.createUid(parentType, "1"), Field.Store.NO));
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java
index 301d6b13e3bc..b2806b1b6251 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java
@@ -222,11 +222,7 @@ public void testCopyToFieldMerge() throws Exception {
 
         DocumentMapper docMapperAfter = parser.parse(mappingAfter);
 
-        MergeResult mergeResult = docMapperBefore.merge(docMapperAfter.mapping(), true, false);
-
-        assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(false));
-
-        docMapperBefore.merge(docMapperAfter.mapping(), false, false);
+        docMapperBefore.merge(docMapperAfter.mapping(), false);
 
         fields = docMapperBefore.mappers().getMapper("copy_test").copyTo().copyToFields();
 
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java
index b1224d5c6c71..f2c1ec511758 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java
@@ -27,6 +27,7 @@
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
+import org.elasticsearch.index.mapper.Mapper;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 
@@ -64,12 +65,17 @@ public void testMerge() throws IOException {
                 .endObject().endObject().string();
         DocumentMapper stage2 = parser.parse(stage2Mapping);
 
-        MergeResult mergeResult = stage1.merge(stage2.mapping(), true, false);
+        Mapper tokenCountMapper1 = stage1.mapping().root().getMapper("tc");
+        Mapper tokenCountMapper2 = stage2.mapping().root().getMapper("tc");
+        MergeResult mergeResult = new MergeResult(true, false);
+        tokenCountMapper1.merge(tokenCountMapper2, mergeResult);
+
         assertThat(mergeResult.hasConflicts(), equalTo(false));
         // Just simulated so merge hasn't happened yet
         assertThat(((TokenCountFieldMapper) stage1.mappers().smartNameFieldMapper("tc")).analyzer(), equalTo("keyword"));
 
-        mergeResult = stage1.merge(stage2.mapping(), false, false);
+        mergeResult = new MergeResult(false, false);
+        tokenCountMapper1.merge(tokenCountMapper2, mergeResult);
         assertThat(mergeResult.hasConflicts(), equalTo(false));
         // Just simulated so merge hasn't happened yet
         assertThat(((TokenCountFieldMapper) stage1.mappers().smartNameFieldMapper("tc")).analyzer(), equalTo("standard"));
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/date/DateBackwardsCompatibilityTests.java b/core/src/test/java/org/elasticsearch/index/mapper/date/DateBackwardsCompatibilityTests.java
index 8ddfc3a2ae73..243792d85047 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/date/DateBackwardsCompatibilityTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/date/DateBackwardsCompatibilityTests.java
@@ -175,7 +175,7 @@ public void testDynamicDateDetectionIn2xDoesNotSupportEpochs() throws Exception
             createIndex(Version.CURRENT, mapping);
             fail("Expected a MapperParsingException, but did not happen");
         } catch (MapperParsingException e) {
-            assertThat(e.getMessage(), containsString("Failed to parse mapping [" + type + "]"));
+            assertThat(e.getMessage(), containsString("Failed to parse mappings [{testtype"));
             assertThat(e.getMessage(), containsString("Epoch [epoch_seconds] is not supported as dynamic date format"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java
index fb67401e334f..f54b76c3c0ef 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java
@@ -371,9 +371,8 @@ public void testThatMergingWorks() throws Exception {
         Map<String, String> config = getConfigurationViaXContent(initialDateFieldMapper);
         assertThat(config.get("format"), is("EEE MMM dd HH:mm:ss.S Z yyyy||EEE MMM dd HH:mm:ss.SSS Z yyyy"));
 
-        MergeResult mergeResult = defaultMapper.merge(mergeMapper.mapping(), false, false);
+        defaultMapper.merge(mergeMapper.mapping(), false);
 
-        assertThat("Merging resulting in conflicts: " + Arrays.asList(mergeResult.buildConflicts()), mergeResult.hasConflicts(), is(false));
         assertThat(defaultMapper.mappers().getMapper("field"), is(instanceOf(DateFieldMapper.class)));
 
         DateFieldMapper mergedFieldMapper = (DateFieldMapper) defaultMapper.mappers().getMapper("field");
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java b/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java
index 4cf7b405217e..7e519c3b7221 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java
@@ -87,7 +87,7 @@ public void testExternalValuesWithMultifield() throws Exception {
                 .startObject("f")
                     .field("type", ExternalMapperPlugin.EXTERNAL_UPPER)
                     .startObject("fields")
-                        .startObject("f")
+                        .startObject("g")
                             .field("type", "string")
                             .field("store", "yes")
                             .startObject("fields")
@@ -107,7 +107,7 @@ public void testExternalValuesWithMultifield() throws Exception {
         refresh();
 
         SearchResponse response = client().prepareSearch("test-idx")
-                .setQuery(QueryBuilders.termQuery("f.f.raw", "FOO BAR"))
+                .setQuery(QueryBuilders.termQuery("f.g.raw", "FOO BAR"))
                 .execute().actionGet();
 
         assertThat(response.getHits().totalHits(), equalTo((long) 1));
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java
index 9741b82aad6e..91392ec42d67 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java
@@ -30,6 +30,7 @@
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
+import org.elasticsearch.index.mapper.Mapper;
 import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.ParsedDocument;
@@ -722,7 +723,11 @@ public void testGeoPointMapperMerge() throws Exception {
                 .field("geohash", false).endObject().endObject().endObject().endObject().string();
         DocumentMapper stage2 = parser.parse(stage2Mapping);
 
-        MergeResult mergeResult = stage1.merge(stage2.mapping(), false, false);
+        Mapper mapper1 = stage1.mapping().root().getMapper("point");
+        Mapper mapper2 = stage2.mapping().root().getMapper("point");
+
+        MergeResult mergeResult = new MergeResult(true, false);
+        mapper1.merge(mapper2, mergeResult);
         assertThat(mergeResult.hasConflicts(), equalTo(true));
         assertThat(mergeResult.buildConflicts().length, equalTo(3));
         // todo better way of checking conflict?
@@ -735,7 +740,9 @@ public void testGeoPointMapperMerge() throws Exception {
                 .startObject("properties").startObject("point").field("type", "geo_point").field("lat_lon", true)
                 .field("geohash", true).endObject().endObject().endObject().endObject().string();
         stage2 = parser.parse(stage2Mapping);
-        mergeResult = stage1.merge(stage2.mapping(), false, false);
+        mapper2 = stage2.mapping().root().getMapper("point");
+        mergeResult = new MergeResult(false, false);
+        mapper1.merge(mapper2, mergeResult);
         assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(false));
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java
index c00bd3101ae9..0f87adb0cdfb 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java
@@ -28,6 +28,7 @@
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.FieldMapper;
+import org.elasticsearch.index.mapper.Mapper;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 
@@ -384,7 +385,10 @@ public void testGeoShapeMapperMerge() throws Exception {
                 .field("orientation", "cw").endObject().endObject().endObject().endObject().string();
         DocumentMapper stage2 = parser.parse(stage2Mapping);
 
-        MergeResult mergeResult = stage1.merge(stage2.mapping(), false, false);
+        Mapper mapper1 = stage1.root().getMapper("shape");
+        Mapper mapper2 = stage2.root().getMapper("shape");
+        MergeResult mergeResult = new MergeResult(true, false);
+        mapper1.merge(mapper2, mergeResult);
         // check correct conflicts
         assertThat(mergeResult.hasConflicts(), equalTo(true));
         assertThat(mergeResult.buildConflicts().length, equalTo(4));
@@ -412,7 +416,9 @@ public void testGeoShapeMapperMerge() throws Exception {
                 .startObject("properties").startObject("shape").field("type", "geo_shape").field("precision", "1m")
                 .field("tree_levels", 8).field("distance_error_pct", 0.001).field("orientation", "cw").endObject().endObject().endObject().endObject().string();
         stage2 = parser.parse(stage2Mapping);
-        mergeResult = stage1.merge(stage2.mapping(), false, false);
+        mapper2 = stage2.root().getMapper("shape");
+        mergeResult = new MergeResult(false, false);
+        mapper1.merge(mapper2, mergeResult);
 
         // verify mapping changes, and ensure no failures
         assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(false));
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java
index 40cf05c4d6a8..da6ccb00ad08 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java
@@ -100,7 +100,7 @@ public void testThatMergingFieldMappingAllowsDisablingBackcompat() throws Except
                 .endObject().endObject().string();
         DocumentMapper mapperDisabled = parser.parse(mappingWithIndexDisabled);
 
-        mapperEnabled.merge(mapperDisabled.mapping(), false, false);
+        mapperEnabled.merge(mapperDisabled.mapping(), false);
         assertThat(mapperEnabled.IndexFieldMapper().enabled(), is(false));
     }
     
@@ -116,7 +116,7 @@ public void testThatDisablingWorksWhenMergingBackcompat() throws Exception {
                 .endObject().endObject().string();
         DocumentMapper disabledMapper = parser.parse(disabledMapping);
 
-        enabledMapper.merge(disabledMapper.mapping(), false, false);
+        enabledMapper.merge(disabledMapper.mapping(), false);
         assertThat(enabledMapper.indexMapper().enabled(), is(false));
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java
index 3f3c5702e8c7..000e48f21f9a 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java
@@ -190,11 +190,11 @@ public void testMergingMappings() throws Exception {
 
         DocumentMapper mapperEnabled = parser.parse(enabledMapping);
         DocumentMapper mapperDisabled = parser.parse(disabledMapping);
-        mapperEnabled.merge(mapperDisabled.mapping(), false, false);
+        mapperEnabled.merge(mapperDisabled.mapping(), false);
         assertFalse(mapperEnabled.metadataMapper(FieldNamesFieldMapper.class).fieldType().isEnabled());
 
         mapperEnabled = parser.parse(enabledMapping);
-        mapperDisabled.merge(mapperEnabled.mapping(), false, false);
+        mapperDisabled.merge(mapperEnabled.mapping(), false);
         assertTrue(mapperEnabled.metadataMapper(FieldNamesFieldMapper.class).fieldType().isEnabled());
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java
index 1a66879c4484..6f519636b0bc 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java
@@ -29,12 +29,13 @@
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.Mapping;
-import org.elasticsearch.index.mapper.MergeResult;
+import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 
+import java.util.Collections;
 import java.util.concurrent.CyclicBarrier;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicReference;
@@ -53,22 +54,31 @@ public void test1Merge() throws Exception {
         DocumentMapperParser parser = createIndex("test").mapperService().documentMapperParser();
         DocumentMapper stage1 = parser.parse(stage1Mapping);
         String stage2Mapping = XContentFactory.jsonBuilder().startObject().startObject("person").startObject("properties")
-                .startObject("name").field("type", "string").endObject()
+                .startObject("name").field("type", "long").endObject()
                 .startObject("age").field("type", "integer").endObject()
                 .startObject("obj1").startObject("properties").startObject("prop1").field("type", "integer").endObject().endObject().endObject()
                 .endObject().endObject().endObject().string();
         DocumentMapper stage2 = parser.parse(stage2Mapping);
 
-        MergeResult mergeResult = stage1.merge(stage2.mapping(), true, false);
-        assertThat(mergeResult.hasConflicts(), equalTo(false));
-        // since we are simulating, we should not have the age mapping
+        try {
+            stage1.merge(stage2.mapping(), false);
+            fail();
+        } catch (MergeMappingException e) {
+            // expected
+        }
+        // since we failed, we should not have the age mapping
+        assertThat(stage1.mappers().smartNameFieldMapper("name"), notNullValue());
         assertThat(stage1.mappers().smartNameFieldMapper("age"), nullValue());
         assertThat(stage1.mappers().smartNameFieldMapper("obj1.prop1"), nullValue());
-        // now merge, don't simulate
-        mergeResult = stage1.merge(stage2.mapping(), false, false);
-        // there is still merge failures
-        assertThat(mergeResult.hasConflicts(), equalTo(false));
-        // but we have the age in
+
+        stage2Mapping = XContentFactory.jsonBuilder().startObject().startObject("person").startObject("properties")
+                .startObject("name").field("type", "string").endObject()
+                .startObject("age").field("type", "integer").endObject()
+                .startObject("obj1").startObject("properties").startObject("prop1").field("type", "integer").endObject().endObject().endObject()
+                .endObject().endObject().endObject().string();
+        stage2 = parser.parse(stage2Mapping);
+        stage1.merge(stage2.mapping(), false);
+        assertThat(stage1.mappers().smartNameFieldMapper("name"), notNullValue());
         assertThat(stage1.mappers().smartNameFieldMapper("age"), notNullValue());
         assertThat(stage1.mappers().smartNameFieldMapper("obj1.prop1"), notNullValue());
     }
@@ -83,8 +93,7 @@ public void testMergeObjectDynamic() throws Exception {
         DocumentMapper withDynamicMapper = parser.parse(withDynamicMapping);
         assertThat(withDynamicMapper.root().dynamic(), equalTo(ObjectMapper.Dynamic.FALSE));
 
-        MergeResult mergeResult = mapper.merge(withDynamicMapper.mapping(), false, false);
-        assertThat(mergeResult.hasConflicts(), equalTo(false));
+        mapper.merge(withDynamicMapper.mapping(), false);
         assertThat(mapper.root().dynamic(), equalTo(ObjectMapper.Dynamic.FALSE));
     }
 
@@ -99,14 +108,13 @@ public void testMergeObjectAndNested() throws Exception {
                 .endObject().endObject().endObject().string();
         DocumentMapper nestedMapper = parser.parse(nestedMapping);
 
-        MergeResult mergeResult = objectMapper.merge(nestedMapper.mapping(), true, false);
-        assertThat(mergeResult.hasConflicts(), equalTo(true));
-        assertThat(mergeResult.buildConflicts().length, equalTo(1));
-        assertThat(mergeResult.buildConflicts()[0], equalTo("object mapping [obj] can't be changed from non-nested to nested"));
-
-        mergeResult = nestedMapper.merge(objectMapper.mapping(), true, false);
-        assertThat(mergeResult.buildConflicts().length, equalTo(1));
-        assertThat(mergeResult.buildConflicts()[0], equalTo("object mapping [obj] can't be changed from nested to non-nested"));
+        try {
+            objectMapper.merge(nestedMapper.mapping(), false);
+            fail();
+        } catch (MergeMappingException e) {
+            // expected
+            assertThat(e.getMessage(), equalTo("Merge failed with failures {[object mapping [obj] can't be changed from non-nested to nested]}"));
+        }
     }
 
     public void testMergeSearchAnalyzer() throws Exception {
@@ -122,9 +130,8 @@ public void testMergeSearchAnalyzer() throws Exception {
         DocumentMapper changed = parser.parse(mapping2);
 
         assertThat(((NamedAnalyzer) existing.mappers().getMapper("field").fieldType().searchAnalyzer()).name(), equalTo("whitespace"));
-        MergeResult mergeResult = existing.merge(changed.mapping(), false, false);
+        existing.merge(changed.mapping(), false);
 
-        assertThat(mergeResult.hasConflicts(), equalTo(false));
         assertThat(((NamedAnalyzer) existing.mappers().getMapper("field").fieldType().searchAnalyzer()).name(), equalTo("keyword"));
     }
 
@@ -141,16 +148,15 @@ public void testChangeSearchAnalyzerToDefault() throws Exception {
         DocumentMapper changed = parser.parse(mapping2);
 
         assertThat(((NamedAnalyzer) existing.mappers().getMapper("field").fieldType().searchAnalyzer()).name(), equalTo("whitespace"));
-        MergeResult mergeResult = existing.merge(changed.mapping(), false, false);
+        existing.merge(changed.mapping(), false);
 
-        assertThat(mergeResult.hasConflicts(), equalTo(false));
         assertThat(((NamedAnalyzer) existing.mappers().getMapper("field").fieldType().searchAnalyzer()).name(), equalTo("standard"));
         assertThat(((StringFieldMapper) (existing.mappers().getMapper("field"))).getIgnoreAbove(), equalTo(14));
     }
 
     public void testConcurrentMergeTest() throws Throwable {
         final MapperService mapperService = createIndex("test").mapperService();
-        mapperService.merge("test", new CompressedXContent("{\"test\":{}}"), true, false);
+        mapperService.merge(Collections.singletonMap("test", new CompressedXContent("{\"test\":{}}")), true, false);
         final DocumentMapper documentMapper = mapperService.documentMapper("test");
 
         DocumentFieldMappers dfm = documentMapper.mappers();
@@ -176,7 +182,7 @@ public void run() {
                         Mapping update = doc.dynamicMappingsUpdate();
                         assert update != null;
                         lastIntroducedFieldName.set(fieldName);
-                        mapperService.merge("test", new CompressedXContent(update.toString()), false, false);
+                        mapperService.merge(Collections.singletonMap("test", new CompressedXContent(update.toString())), false, false);
                     }
                 } catch (Throwable t) {
                     error.set(t);
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java b/core/src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java
index 07671a2d4b01..303adf0b06fb 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java
@@ -25,6 +25,7 @@
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
+import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.ParseContext.Document;
 import org.elasticsearch.test.ESSingleNodeTestCase;
@@ -59,10 +60,7 @@ public void testMergeMultiField() throws Exception {
         mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/multifield/merge/test-mapping2.json");
         DocumentMapper docMapper2 = parser.parse(mapping);
 
-        MergeResult mergeResult = docMapper.merge(docMapper2.mapping(), true, false);
-        assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(false));
-
-        docMapper.merge(docMapper2.mapping(), false, false);
+        docMapper.merge(docMapper2.mapping(), false);
 
         assertNotSame(IndexOptions.NONE, docMapper.mappers().getMapper("name").fieldType().indexOptions());
 
@@ -81,10 +79,7 @@ public void testMergeMultiField() throws Exception {
         mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/multifield/merge/test-mapping3.json");
         DocumentMapper docMapper3 = parser.parse(mapping);
 
-        mergeResult = docMapper.merge(docMapper3.mapping(), true, false);
-        assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(false));
-
-        docMapper.merge(docMapper3.mapping(), false, false);
+        docMapper.merge(docMapper3.mapping(), false);
 
         assertNotSame(IndexOptions.NONE, docMapper.mappers().getMapper("name").fieldType().indexOptions());
 
@@ -97,10 +92,7 @@ public void testMergeMultiField() throws Exception {
         mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/multifield/merge/test-mapping4.json");
         DocumentMapper docMapper4 = parser.parse(mapping);
 
-        mergeResult = docMapper.merge(docMapper4.mapping(), true, false);
-        assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(false));
-
-        docMapper.merge(docMapper4.mapping(), false, false);
+        docMapper.merge(docMapper4.mapping(), false);
 
         assertNotSame(IndexOptions.NONE, docMapper.mappers().getMapper("name").fieldType().indexOptions());
 
@@ -131,10 +123,7 @@ public void testUpgradeFromMultiFieldTypeToMultiFields() throws Exception {
         mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/multifield/merge/upgrade1.json");
         DocumentMapper docMapper2 = parser.parse(mapping);
 
-        MergeResult mergeResult = docMapper.merge(docMapper2.mapping(), true, false);
-        assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(false));
-
-        docMapper.merge(docMapper2.mapping(), false, false);
+        docMapper.merge(docMapper2.mapping(), false);
 
         assertNotSame(IndexOptions.NONE, docMapper.mappers().getMapper("name").fieldType().indexOptions());
 
@@ -153,10 +142,7 @@ public void testUpgradeFromMultiFieldTypeToMultiFields() throws Exception {
         mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/multifield/merge/upgrade2.json");
         DocumentMapper docMapper3 = parser.parse(mapping);
 
-        mergeResult = docMapper.merge(docMapper3.mapping(), true, false);
-        assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(false));
-
-        docMapper.merge(docMapper3.mapping(), false, false);
+        docMapper.merge(docMapper3.mapping(), false);
 
         assertNotSame(IndexOptions.NONE, docMapper.mappers().getMapper("name").fieldType().indexOptions());
 
@@ -169,23 +155,18 @@ public void testUpgradeFromMultiFieldTypeToMultiFields() throws Exception {
 
         mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/multifield/merge/upgrade3.json");
         DocumentMapper docMapper4 = parser.parse(mapping);
-        mergeResult = docMapper.merge(docMapper4.mapping(), true, false);
-        assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(true));
-        assertThat(mergeResult.buildConflicts()[0], equalTo("mapper [name] has different [index] values"));
-        assertThat(mergeResult.buildConflicts()[1], equalTo("mapper [name] has different [store] values"));
-
-        mergeResult = docMapper.merge(docMapper4.mapping(), false, false);
-        assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(true));
-
-        assertNotSame(IndexOptions.NONE, docMapper.mappers().getMapper("name").fieldType().indexOptions());
-        assertThat(mergeResult.buildConflicts()[0], equalTo("mapper [name] has different [index] values"));
-        assertThat(mergeResult.buildConflicts()[1], equalTo("mapper [name] has different [store] values"));
-
-        // There are conflicts, but the `name.not_indexed3` has been added, b/c that field has no conflicts
+        try {
+            docMapper.merge(docMapper4.mapping(), false);
+            fail();
+        } catch (MergeMappingException e) {
+            assertEquals("Merge failed with failures {[mapper [name] has different [index] values, mapper [name] has different [store] values]}", e.getMessage());
+        }
+
+        // There are conflicts, so the `name.not_indexed3` has not been added,even though that field has no conflicts
         assertNotSame(IndexOptions.NONE, docMapper.mappers().getMapper("name").fieldType().indexOptions());
         assertThat(docMapper.mappers().getMapper("name.indexed"), notNullValue());
         assertThat(docMapper.mappers().getMapper("name.not_indexed"), notNullValue());
         assertThat(docMapper.mappers().getMapper("name.not_indexed2"), notNullValue());
-        assertThat(docMapper.mappers().getMapper("name.not_indexed3"), notNullValue());
+        assertThat(docMapper.mappers().getMapper("name.not_indexed3"), nullValue());
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java
index 4ec0ff5211ee..29d3aed3292c 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java
@@ -33,9 +33,9 @@
 import org.elasticsearch.test.ESSingleNodeTestCase;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.List;
+import java.util.Collections;
+import java.util.HashMap;
 import java.util.Map;
 
 import static org.hamcrest.Matchers.equalTo;
@@ -200,7 +200,7 @@ public void testDefaultMappingAndNoMappingWithMapperService() throws Exception {
                 .endObject().endObject().string();
 
         MapperService mapperService = createIndex("test").mapperService();
-        mapperService.merge(MapperService.DEFAULT_MAPPING, new CompressedXContent(defaultMapping), true, false);
+        mapperService.merge(Collections.singletonMap(MapperService.DEFAULT_MAPPING, new CompressedXContent(defaultMapping)), true, false);
 
         DocumentMapper mapper = mapperService.documentMapperWithAutoCreate("my_type").getDocumentMapper();
         assertThat(mapper.type(), equalTo("my_type"));
@@ -213,14 +213,14 @@ public void testDefaultMappingAndWithMappingOverrideWithMapperService() throws E
                 .endObject().endObject().string();
 
         MapperService mapperService = createIndex("test").mapperService();
-        mapperService.merge(MapperService.DEFAULT_MAPPING, new CompressedXContent(defaultMapping), true, false);
-
+        Map<String, CompressedXContent> mappingSources = new HashMap<>();
+        mappingSources.put(MapperService.DEFAULT_MAPPING, new CompressedXContent(defaultMapping));
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("my_type")
                 .startObject("_source").field("enabled", true).endObject()
                 .endObject().endObject().string();
-        mapperService.merge("my_type", new CompressedXContent(mapping), true, false);
+        mappingSources.put("my_type", new CompressedXContent(mapping));
 
-        DocumentMapper mapper = mapperService.documentMapper("my_type");
+        DocumentMapper mapper = mapperService.merge(mappingSources, true, false).get("my_type");
         assertThat(mapper.type(), equalTo("my_type"));
         assertThat(mapper.sourceMapper().enabled(), equalTo(true));
     }
@@ -228,13 +228,14 @@ public void testDefaultMappingAndWithMappingOverrideWithMapperService() throws E
     void assertConflicts(String mapping1, String mapping2, DocumentMapperParser parser, String... conflicts) throws IOException {
         DocumentMapper docMapper = parser.parse(mapping1);
         docMapper = parser.parse(docMapper.mappingSource().string());
-        MergeResult mergeResult = docMapper.merge(parser.parse(mapping2).mapping(), true, false);
-
-        List<String> expectedConflicts = new ArrayList<>(Arrays.asList(conflicts));
-        for (String conflict : mergeResult.buildConflicts()) {
-            assertTrue("found unexpected conflict [" + conflict + "]", expectedConflicts.remove(conflict));
+        try {
+            docMapper.merge(parser.parse(mapping2).mapping(), false);
+            if (conflicts.length > 0) {
+                fail();
+            }
+        } catch (MergeMappingException e) {
+            assertEquals("Merge failed with failures {" + Arrays.toString(conflicts) + "}", e.getMessage());
         }
-        assertTrue("missing conflicts: " + Arrays.toString(expectedConflicts.toArray()), expectedConflicts.isEmpty());
     }
 
     public void testEnabledNotUpdateable() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
index 121f100ffa6a..d33fb0440491 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
@@ -37,6 +37,7 @@
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.FieldMapper;
+import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.Mapper.BuilderContext;
 import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.MergeResult;
@@ -492,8 +493,7 @@ public void testDisableNorms() throws Exception {
         String updatedMapping = XContentFactory.jsonBuilder().startObject().startObject("type")
                 .startObject("properties").startObject("field").field("type", "string").startObject("norms").field("enabled", false).endObject()
                 .endObject().endObject().endObject().endObject().string();
-        MergeResult mergeResult = defaultMapper.merge(parser.parse(updatedMapping).mapping(), false, false);
-        assertFalse(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts());
+        defaultMapper.merge(parser.parse(updatedMapping).mapping(), false);
 
         doc = defaultMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
                 .startObject()
@@ -507,10 +507,12 @@ public void testDisableNorms() throws Exception {
         updatedMapping = XContentFactory.jsonBuilder().startObject().startObject("type")
                 .startObject("properties").startObject("field").field("type", "string").startObject("norms").field("enabled", true).endObject()
                 .endObject().endObject().endObject().endObject().string();
-        mergeResult = defaultMapper.merge(parser.parse(updatedMapping).mapping(), true, false);
-        assertTrue(mergeResult.hasConflicts());
-        assertEquals(1, mergeResult.buildConflicts().length);
-        assertTrue(mergeResult.buildConflicts()[0].contains("different [omit_norms]"));
+        try {
+            defaultMapper.merge(parser.parse(updatedMapping).mapping(), false);
+            fail();
+        } catch (MergeMappingException e) {
+            assertTrue(e.getMessage().contains("different [omit_norms]"));
+        }
     }
 
     /**
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java
index df9cc10d8c23..a93135eadf1a 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java
@@ -41,6 +41,7 @@
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperParsingException;
+import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.SourceToParse;
@@ -152,7 +153,7 @@ public void testThatDisablingDuringMergeIsWorking() throws Exception {
                 .endObject().endObject().string();
         DocumentMapper disabledMapper = parser.parse(disabledMapping);
 
-        enabledMapper.merge(disabledMapper.mapping(), false, false);
+        enabledMapper.merge(disabledMapper.mapping(), false);
 
         assertThat(enabledMapper.timestampFieldMapper().enabled(), is(false));
     }
@@ -514,8 +515,7 @@ public void testMergingFielddataLoadingWorks() throws Exception {
                 .startObject("_timestamp").field("enabled", randomBoolean()).startObject("fielddata").field("loading", "eager").field("format", "array").endObject().field("store", "yes").endObject()
                 .endObject().endObject().string();
 
-        MergeResult mergeResult = docMapper.merge(parser.parse(mapping).mapping(), false, false);
-        assertThat(mergeResult.buildConflicts().length, equalTo(0));
+        docMapper.merge(parser.parse(mapping).mapping(), false);
         assertThat(docMapper.timestampFieldMapper().fieldType().fieldDataType().getLoading(), equalTo(MappedFieldType.Loading.EAGER));
         assertThat(docMapper.timestampFieldMapper().fieldType().fieldDataType().getFormat(indexSettings), equalTo("array"));
     }
@@ -579,17 +579,17 @@ public void testMergingConflicts() throws Exception {
                 .endObject()
                 .endObject().endObject().string();
 
-        MergeResult mergeResult = docMapper.merge(parser.parse(mapping).mapping(), true, false);
-        List<String> expectedConflicts = new ArrayList<>(Arrays.asList(
-            "mapper [_timestamp] has different [index] values",
-            "mapper [_timestamp] has different [store] values",
-            "Cannot update default in _timestamp value. Value is 1970-01-01 now encountering 1970-01-02",
-            "Cannot update path in _timestamp value. Value is foo path in merged mapping is bar"));
-
-        for (String conflict : mergeResult.buildConflicts()) {
-            assertTrue("found unexpected conflict [" + conflict + "]", expectedConflicts.remove(conflict));
+        try {
+            docMapper.merge(parser.parse(mapping).mapping(), false);
+            fail();
+        } catch (MergeMappingException e) {
+            final String[] expectedConflicts = new String[] {
+                "mapper [_timestamp] has different [index] values",
+                "mapper [_timestamp] has different [store] values",
+                "Cannot update default in _timestamp value. Value is 1970-01-01 now encountering 1970-01-02",
+                "Cannot update path in _timestamp value. Value is foo path in merged mapping is bar"};
+            assertEquals("Merge failed with failures {" + Arrays.toString(expectedConflicts) + "}", e.getMessage());
         }
-        assertTrue("missing conflicts: " + Arrays.toString(expectedConflicts.toArray()), expectedConflicts.isEmpty());
         assertThat(docMapper.timestampFieldMapper().fieldType().fieldDataType().getLoading(), equalTo(MappedFieldType.Loading.LAZY));
         assertTrue(docMapper.timestampFieldMapper().enabled());
         assertThat(docMapper.timestampFieldMapper().fieldType().fieldDataType().getFormat(indexSettings), equalTo("doc_values"));
@@ -616,18 +616,11 @@ public void testBackcompatMergingConflictsForIndexValues() throws Exception {
                 .endObject()
                 .endObject().endObject().string();
 
-        MergeResult mergeResult = docMapper.merge(parser.parse(mapping).mapping(), true, false);
-        List<String> expectedConflicts = new ArrayList<>();
-        expectedConflicts.add("mapper [_timestamp] has different [index] values");
-        expectedConflicts.add("mapper [_timestamp] has different [tokenize] values");
-        if (indexValues.get(0).equals("not_analyzed") == false) {
-            // if the only index value left is not_analyzed, then the doc values setting will be the same, but in the
-            // other two cases, it will change
-            expectedConflicts.add("mapper [_timestamp] has different [doc_values] values");
-        }
-
-        for (String conflict : mergeResult.buildConflicts()) {
-            assertThat(conflict, isIn(expectedConflicts));
+        try {
+            docMapper.merge(parser.parse(mapping).mapping(), false);
+            fail();
+        } catch (MergeMappingException e) {
+            assertTrue(e.getMessage().contains("mapper [_timestamp] has different [index] values"));
         }
     }
 
@@ -674,10 +667,12 @@ public void testBackcompatMergePaths() throws Exception {
     void assertConflict(String mapping1, String mapping2, DocumentMapperParser parser, String conflict) throws IOException {
         DocumentMapper docMapper = parser.parse(mapping1);
         docMapper = parser.parse(docMapper.mappingSource().string());
-        MergeResult mergeResult = docMapper.merge(parser.parse(mapping2).mapping(), true, false);
-        assertThat(mergeResult.buildConflicts().length, equalTo(conflict == null ? 0 : 1));
-        if (conflict != null) {
-            assertThat(mergeResult.buildConflicts()[0], containsString(conflict));
+        try {
+            docMapper.merge(parser.parse(mapping2).mapping(), false);
+            assertNull(conflict);
+        } catch (MergeMappingException e) {
+            assertNotNull(conflict);
+            assertTrue(e.getMessage().contains(conflict));
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java
index 7802e8668191..62f30b03ae59 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java
@@ -37,12 +37,14 @@
 import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
+import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.index.mapper.internal.TTLFieldMapper;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 
 import java.io.IOException;
+import java.util.Collections;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.hamcrest.Matchers.containsString;
@@ -117,9 +119,7 @@ public void testThatEnablingTTLFieldOnMergeWorks() throws Exception {
         DocumentMapper mapperWithoutTtl = parser.parse(mappingWithoutTtl);
         DocumentMapper mapperWithTtl = parser.parse(mappingWithTtl);
 
-        MergeResult mergeResult = mapperWithoutTtl.merge(mapperWithTtl.mapping(), false, false);
-
-        assertThat(mergeResult.hasConflicts(), equalTo(false));
+        mapperWithoutTtl.merge(mapperWithTtl.mapping(), false);
         assertThat(mapperWithoutTtl.TTLFieldMapper().enabled(), equalTo(true));
     }
 
@@ -142,7 +142,10 @@ public void testThatChangingTTLKeepsMapperEnabled() throws Exception {
         DocumentMapper initialMapper = parser.parse(mappingWithTtl);
         DocumentMapper updatedMapper = parser.parse(updatedMapping);
 
-        MergeResult mergeResult = initialMapper.merge(updatedMapper.mapping(), true, false);
+        MergeResult mergeResult = new MergeResult(true, false);
+        TTLFieldMapper mapper1 = initialMapper.mapping().metadataMapper(TTLFieldMapper.class);
+        TTLFieldMapper mapper2 = updatedMapper.mapping().metadataMapper(TTLFieldMapper.class);
+        mapper1.merge(mapper2, mergeResult);
 
         assertThat(mergeResult.hasConflicts(), equalTo(false));
         assertThat(initialMapper.TTLFieldMapper().enabled(), equalTo(true));
@@ -155,7 +158,10 @@ public void testThatDisablingTTLReportsConflict() throws Exception {
         DocumentMapper initialMapper = parser.parse(mappingWithTtl);
         DocumentMapper updatedMapper = parser.parse(mappingWithTtlDisabled);
 
-        MergeResult mergeResult = initialMapper.merge(updatedMapper.mapping(), true, false);
+        MergeResult mergeResult = new MergeResult(true, false);
+        TTLFieldMapper mapper1 = initialMapper.mapping().metadataMapper(TTLFieldMapper.class);
+        TTLFieldMapper mapper2 = updatedMapper.mapping().metadataMapper(TTLFieldMapper.class);
+        mapper1.merge(mapper2, mergeResult);
 
         assertThat(mergeResult.hasConflicts(), equalTo(true));
         assertThat(initialMapper.TTLFieldMapper().enabled(), equalTo(true));
@@ -190,23 +196,20 @@ public void testThatEnablingTTLAfterFirstDisablingWorks() throws Exception {
     public void testNoConflictIfNothingSetAndDisabledLater() throws Exception {
         IndexService indexService = createIndex("testindex", Settings.settingsBuilder().build(), "type");
         XContentBuilder mappingWithTtlDisabled = getMappingWithTtlDisabled("7d");
-        MergeResult mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingWithTtlDisabled.string()), true).mapping(), randomBoolean(), false);
-        assertFalse(mergeResult.hasConflicts());
+        indexService.mapperService().merge(Collections.singletonMap("type", new CompressedXContent(mappingWithTtlDisabled.string())), true, false);
     }
 
     public void testNoConflictIfNothingSetAndEnabledLater() throws Exception {
         IndexService indexService = createIndex("testindex", Settings.settingsBuilder().build(), "type");
         XContentBuilder mappingWithTtlEnabled = getMappingWithTtlEnabled("7d");
-        MergeResult mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingWithTtlEnabled.string()), true).mapping(), randomBoolean(), false);
-        assertFalse(mergeResult.hasConflicts());
+        indexService.mapperService().merge(Collections.singletonMap("type", new CompressedXContent(mappingWithTtlEnabled.string())), true, false);
     }
 
     public void testMergeWithOnlyDefaultSet() throws Exception {
         XContentBuilder mappingWithTtlEnabled = getMappingWithTtlEnabled("7d");
         IndexService indexService = createIndex("testindex", Settings.settingsBuilder().build(), "type", mappingWithTtlEnabled);
         XContentBuilder mappingWithOnlyDefaultSet = getMappingWithOnlyTtlDefaultSet("6m");
-        MergeResult mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingWithOnlyDefaultSet.string()), true).mapping(), false, false);
-        assertFalse(mergeResult.hasConflicts());
+        indexService.mapperService().merge(Collections.singletonMap("type", new CompressedXContent(mappingWithOnlyDefaultSet.string())), true, false);
         CompressedXContent mappingAfterMerge = indexService.mapperService().documentMapper("type").mappingSource();
         assertThat(mappingAfterMerge, equalTo(new CompressedXContent("{\"type\":{\"_ttl\":{\"enabled\":true,\"default\":360000},\"properties\":{\"field\":{\"type\":\"string\"}}}}")));
     }
@@ -217,8 +220,7 @@ public void testMergeWithOnlyDefaultSetTtlDisabled() throws Exception {
         CompressedXContent mappingAfterCreation = indexService.mapperService().documentMapper("type").mappingSource();
         assertThat(mappingAfterCreation, equalTo(new CompressedXContent("{\"type\":{\"_ttl\":{\"enabled\":false},\"properties\":{\"field\":{\"type\":\"string\"}}}}")));
         XContentBuilder mappingWithOnlyDefaultSet = getMappingWithOnlyTtlDefaultSet("6m");
-        MergeResult mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingWithOnlyDefaultSet.string()), true).mapping(), false, false);
-        assertFalse(mergeResult.hasConflicts());
+        indexService.mapperService().merge(Collections.singletonMap("type", new CompressedXContent(mappingWithOnlyDefaultSet.string())), true, false);
         CompressedXContent mappingAfterMerge = indexService.mapperService().documentMapper("type").mappingSource();
         assertThat(mappingAfterMerge, equalTo(new CompressedXContent("{\"type\":{\"_ttl\":{\"enabled\":false},\"properties\":{\"field\":{\"type\":\"string\"}}}}")));
     }
@@ -229,7 +231,11 @@ public void testThatSimulatedMergingLeavesStateUntouched() throws Exception {
         IndexService indexService = createIndex("testindex", Settings.settingsBuilder().build(), "type", mappingWithTtl);
         CompressedXContent mappingBeforeMerge = indexService.mapperService().documentMapper("type").mappingSource();
         XContentBuilder mappingWithTtlDifferentDefault = getMappingWithTtlEnabled("7d");
-        MergeResult mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingWithTtlDifferentDefault.string()), true).mapping(), true, false);
+
+        MergeResult mergeResult = new MergeResult(true, false);
+        TTLFieldMapper mapper1 = indexService.mapperService().documentMapper("type").mapping().metadataMapper(TTLFieldMapper.class);
+        TTLFieldMapper mapper2 = indexService.mapperService().documentMapperParser().parse("type", mappingWithTtlDifferentDefault.string()).mapping().metadataMapper(TTLFieldMapper.class);
+        mapper1.merge(mapper2, mergeResult);
         assertFalse(mergeResult.hasConflicts());
         // make sure simulate flag actually worked - no mappings applied
         CompressedXContent mappingAfterMerge = indexService.mapperService().documentMapper("type").mappingSource();
@@ -241,8 +247,10 @@ public void testThatSimulatedMergingLeavesStateUntouched() throws Exception {
         indexService = createIndex("testindex", Settings.settingsBuilder().build(), "type", mappingWithoutTtl);
         mappingBeforeMerge = indexService.mapperService().documentMapper("type").mappingSource();
         XContentBuilder mappingWithTtlEnabled = getMappingWithTtlEnabled();
-        mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingWithTtlEnabled.string()), true).mapping(), true, false);
-        assertFalse(mergeResult.hasConflicts());
+        mergeResult = new MergeResult(true, false);
+        mapper1 = indexService.mapperService().documentMapper("type").mapping().metadataMapper(TTLFieldMapper.class);
+        mapper2 = indexService.mapperService().documentMapperParser().parse("type", mappingWithTtlEnabled.string()).mapping().metadataMapper(TTLFieldMapper.class);
+        mapper1.merge(mapper2, mergeResult);
         // make sure simulate flag actually worked - no mappings applied
         mappingAfterMerge = indexService.mapperService().documentMapper("type").mappingSource();
         assertThat(mappingAfterMerge, equalTo(mappingBeforeMerge));
@@ -253,7 +261,10 @@ public void testThatSimulatedMergingLeavesStateUntouched() throws Exception {
         indexService = createIndex("testindex", Settings.settingsBuilder().build(), "type", mappingWithoutTtl);
         mappingBeforeMerge = indexService.mapperService().documentMapper("type").mappingSource();
         mappingWithTtlEnabled = getMappingWithTtlEnabled("7d");
-        mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingWithTtlEnabled.string()), true).mapping(), true, false);
+        mergeResult = new MergeResult(true, false);
+        mapper1 = indexService.mapperService().documentMapper("type").mapping().metadataMapper(TTLFieldMapper.class);
+        mapper2 = indexService.mapperService().documentMapperParser().parse("type", mappingWithTtlEnabled.string()).mapping().metadataMapper(TTLFieldMapper.class);
+        mapper1.merge(mapper2, mergeResult);
         assertFalse(mergeResult.hasConflicts());
         // make sure simulate flag actually worked - no mappings applied
         mappingAfterMerge = indexService.mapperService().documentMapper("type").mappingSource();
@@ -264,8 +275,7 @@ public void testThatSimulatedMergingLeavesStateUntouched() throws Exception {
         mappingWithoutTtl = getMappingWithTtlDisabled("6d");
         indexService = createIndex("testindex", Settings.settingsBuilder().build(), "type", mappingWithoutTtl);
         mappingWithTtlEnabled = getMappingWithTtlEnabled("7d");
-        mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingWithTtlEnabled.string()), true).mapping(), false, false);
-        assertFalse(mergeResult.hasConflicts());
+        indexService.mapperService().documentMapper("type").merge(indexService.mapperService().documentMapperParser().parse("type", mappingWithTtlEnabled.string()).mapping(), false);
         // make sure simulate flag actually worked - mappings applied
         mappingAfterMerge = indexService.mapperService().documentMapper("type").mappingSource();
         assertThat(mappingAfterMerge, equalTo(new CompressedXContent("{\"type\":{\"_ttl\":{\"enabled\":true,\"default\":604800000},\"properties\":{\"field\":{\"type\":\"string\"}}}}")));
@@ -274,8 +284,7 @@ public void testThatSimulatedMergingLeavesStateUntouched() throws Exception {
         // check if switching simulate flag off works if nothing was applied in the beginning
         indexService = createIndex("testindex", Settings.settingsBuilder().build(), "type");
         mappingWithTtlEnabled = getMappingWithTtlEnabled("7d");
-        mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingWithTtlEnabled.string()), true).mapping(), false, false);
-        assertFalse(mergeResult.hasConflicts());
+        indexService.mapperService().documentMapper("type").merge(indexService.mapperService().documentMapperParser().parse("type", mappingWithTtlEnabled.string()).mapping(), false);
         // make sure simulate flag actually worked - mappings applied
         mappingAfterMerge = indexService.mapperService().documentMapper("type").mappingSource();
         assertThat(mappingAfterMerge, equalTo(new CompressedXContent("{\"type\":{\"_ttl\":{\"enabled\":true,\"default\":604800000},\"properties\":{\"field\":{\"type\":\"string\"}}}}")));
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java
index 5149ab105754..cf5ac49059a4 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java
@@ -29,10 +29,12 @@
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.MergeResult;
+import org.elasticsearch.index.mapper.MergeMappingException;
+import org.elasticsearch.index.mapper.core.LongFieldMapper;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 
 import java.io.IOException;
+import java.util.Collections;
 import java.util.LinkedHashMap;
 
 import static org.elasticsearch.test.StreamsUtils.copyToStringFromClasspath;
@@ -75,9 +77,7 @@ public void testAllDisabledAfterDisabled() throws Exception {
     private void testNoConflictWhileMergingAndMappingChanged(XContentBuilder mapping, XContentBuilder mappingUpdate, XContentBuilder expectedMapping) throws IOException {
         IndexService indexService = createIndex("test", Settings.settingsBuilder().build(), "type", mapping);
         // simulate like in MetaDataMappingService#putMapping
-        MergeResult mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingUpdate.bytes()), true).mapping(), false, false);
-        // assure we have no conflicts
-        assertThat(mergeResult.buildConflicts().length, equalTo(0));
+        indexService.mapperService().documentMapper("type").merge(indexService.mapperService().documentMapperParser().parse("type", mappingUpdate.string()).mapping(), false);
         // make sure mappings applied
         CompressedXContent mappingAfterUpdate = indexService.mapperService().documentMapper("type").mappingSource();
         assertThat(mappingAfterUpdate.toString(), equalTo(expectedMapping.string()));
@@ -99,14 +99,131 @@ protected void testConflictWhileMergingAndMappingUnchanged(XContentBuilder mappi
         IndexService indexService = createIndex("test", Settings.settingsBuilder().build(), "type", mapping);
         CompressedXContent mappingBeforeUpdate = indexService.mapperService().documentMapper("type").mappingSource();
         // simulate like in MetaDataMappingService#putMapping
-        MergeResult mergeResult = indexService.mapperService().documentMapper("type").merge(indexService.mapperService().parse("type", new CompressedXContent(mappingUpdate.bytes()), true).mapping(), true, false);
-        // assure we have conflicts
-        assertThat(mergeResult.buildConflicts().length, equalTo(1));
+        try {
+            indexService.mapperService().documentMapper("type").merge(indexService.mapperService().documentMapperParser().parse("type", mappingUpdate.string()).mapping(), false);
+            fail();
+        } catch (MergeMappingException e) {
+            // expected
+        }
         // make sure simulate flag actually worked - no mappings applied
         CompressedXContent mappingAfterUpdate = indexService.mapperService().documentMapper("type").mappingSource();
         assertThat(mappingAfterUpdate, equalTo(mappingBeforeUpdate));
     }
 
+    public void testConflictSameType() throws Exception {
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("properties").startObject("foo").field("type", "long").endObject()
+                .endObject().endObject().endObject();
+        MapperService mapperService = createIndex("test", Settings.settingsBuilder().build(), "type", mapping).mapperService();
+
+        XContentBuilder update = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("properties").startObject("foo").field("type", "double").endObject()
+                .endObject().endObject().endObject();
+
+        try {
+            mapperService.merge(Collections.singletonMap("type", new CompressedXContent(update.string())), false, false);
+            fail();
+        } catch (MergeMappingException e) {
+            // expected
+        }
+
+        try {
+            mapperService.merge(Collections.singletonMap("type", new CompressedXContent(update.string())), false, false);
+            fail();
+        } catch (MergeMappingException e) {
+            // expected
+        }
+
+        assertTrue(mapperService.documentMapper("type").mapping().root().getMapper("foo") instanceof LongFieldMapper);
+    }
+
+    public void testConflictNewType() throws Exception {
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                .startObject("properties").startObject("foo").field("type", "long").endObject()
+                .endObject().endObject().endObject();
+        MapperService mapperService = createIndex("test", Settings.settingsBuilder().build(), "type1", mapping).mapperService();
+
+        XContentBuilder update = XContentFactory.jsonBuilder().startObject().startObject("type2")
+                .startObject("properties").startObject("foo").field("type", "double").endObject()
+                .endObject().endObject().endObject();
+
+        try {
+            mapperService.merge(Collections.singletonMap("type2", new CompressedXContent(update.string())), false, false);
+            fail();
+        } catch (IllegalArgumentException e) {
+            // expected
+            assertTrue(e.getMessage().contains("conflicts with existing mapping in other types"));
+        }
+
+        try {
+            mapperService.merge(Collections.singletonMap("type2", new CompressedXContent(update.string())), false, false);
+            fail();
+        } catch (IllegalArgumentException e) {
+            // expected
+            assertTrue(e.getMessage().contains("conflicts with existing mapping in other types"));
+        }
+
+        assertTrue(mapperService.documentMapper("type1").mapping().root().getMapper("foo") instanceof LongFieldMapper);
+        assertNull(mapperService.documentMapper("type2"));
+    }
+
+    // same as the testConflictNewType except that the mapping update is on an existing type
+    public void testConflictNewTypeUpdate() throws Exception {
+        XContentBuilder mapping1 = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                .startObject("properties").startObject("foo").field("type", "long").endObject()
+                .endObject().endObject().endObject();
+        XContentBuilder mapping2 = XContentFactory.jsonBuilder().startObject().startObject("type2").endObject().endObject();
+        MapperService mapperService = createIndex("test", Settings.settingsBuilder().build()).mapperService();
+
+        mapperService.merge(Collections.singletonMap("type1", new CompressedXContent(mapping1.string())), false, false);
+        mapperService.merge(Collections.singletonMap("type2", new CompressedXContent(mapping2.string())), false, false);
+
+        XContentBuilder update = XContentFactory.jsonBuilder().startObject().startObject("type2")
+                .startObject("properties").startObject("foo").field("type", "double").endObject()
+                .endObject().endObject().endObject();
+
+        try {
+            mapperService.merge(Collections.singletonMap("type2", new CompressedXContent(update.string())), false, false);
+            fail();
+        } catch (IllegalArgumentException e) {
+            // expected
+            assertTrue(e.getMessage().contains("conflicts with existing mapping in other types"));
+        }
+
+        try {
+            mapperService.merge(Collections.singletonMap("type2", new CompressedXContent(update.string())), false, false);
+            fail();
+        } catch (IllegalArgumentException e) {
+            // expected
+            assertTrue(e.getMessage().contains("conflicts with existing mapping in other types"));
+        }
+
+        assertTrue(mapperService.documentMapper("type1").mapping().root().getMapper("foo") instanceof LongFieldMapper);
+        assertNotNull(mapperService.documentMapper("type2"));
+        assertNull(mapperService.documentMapper("type2").mapping().root().getMapper("foo"));
+    }
+
+    public void testReuseMetaField() throws IOException {
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("properties").startObject("_id").field("type", "string").endObject()
+                .endObject().endObject().endObject();
+        MapperService mapperService = createIndex("test", Settings.settingsBuilder().build()).mapperService();
+
+        try {
+            mapperService.merge(Collections.singletonMap("type", new CompressedXContent(mapping.string())), false, false);
+            fail();
+        } catch (IllegalArgumentException e) {
+            assertTrue(e.getMessage().contains("Two fields are defined for path [_id]"));
+        }
+
+        try {
+            mapperService.merge(Collections.singletonMap("type", new CompressedXContent(mapping.string())), false, false);
+            fail();
+        } catch (IllegalArgumentException e) {
+            assertTrue(e.getMessage().contains("Two fields are defined for path [_id]"));
+        }
+    }
+
     public void testIndexFieldParsingBackcompat() throws IOException {
         IndexService indexService = createIndex("test", Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id).build());
         XContentBuilder indexMapping = XContentFactory.jsonBuilder();
@@ -118,9 +235,9 @@ public void testIndexFieldParsingBackcompat() throws IOException {
                 .endObject()
                 .endObject()
                 .endObject();
-        DocumentMapper documentMapper = indexService.mapperService().parse("type", new CompressedXContent(indexMapping.string()), true);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse("type", indexMapping.string());
         assertThat(documentMapper.indexMapper().enabled(), equalTo(enabled));
-        documentMapper = indexService.mapperService().parse("type", new CompressedXContent(documentMapper.mappingSource().string()), true);
+        documentMapper = indexService.mapperService().documentMapperParser().parse("type", documentMapper.mappingSource().string());
         assertThat(documentMapper.indexMapper().enabled(), equalTo(enabled));
     }
 
@@ -139,11 +256,11 @@ public void testTimestampParsing() throws IOException {
                 .endObject()
                 .endObject()
                 .endObject();
-        DocumentMapper documentMapper = indexService.mapperService().parse("type", new CompressedXContent(indexMapping.string()), true);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse("type", indexMapping.string());
         assertThat(documentMapper.timestampFieldMapper().enabled(), equalTo(enabled));
         assertTrue(documentMapper.timestampFieldMapper().fieldType().stored());
         assertTrue(documentMapper.timestampFieldMapper().fieldType().hasDocValues());
-        documentMapper = indexService.mapperService().parse("type", new CompressedXContent(documentMapper.mappingSource().string()), true);
+        documentMapper = indexService.mapperService().documentMapperParser().parse("type", documentMapper.mappingSource().string());
         assertThat(documentMapper.timestampFieldMapper().enabled(), equalTo(enabled));
         assertTrue(documentMapper.timestampFieldMapper().fieldType().hasDocValues());
         assertTrue(documentMapper.timestampFieldMapper().fieldType().stored());
@@ -152,9 +269,9 @@ public void testTimestampParsing() throws IOException {
     public void testSizeTimestampIndexParsing() throws IOException {
         IndexService indexService = createIndex("test", Settings.settingsBuilder().build());
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/update/default_mapping_with_disabled_root_types.json");
-        DocumentMapper documentMapper = indexService.mapperService().parse("type", new CompressedXContent(mapping), true);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse("type", mapping);
         assertThat(documentMapper.mappingSource().string(), equalTo(mapping));
-        documentMapper = indexService.mapperService().parse("type", new CompressedXContent(documentMapper.mappingSource().string()), true);
+        documentMapper = indexService.mapperService().documentMapperParser().parse("type", documentMapper.mappingSource().string());
         assertThat(documentMapper.mappingSource().string(), equalTo(mapping));
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
index f64dc1a55de6..34ebd5e78207 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -260,9 +260,11 @@ public void onRemoval(ShardId shardId, Accountable accountable) {
         queryShardContext = new QueryShardContext(idxSettings, proxy, bitsetFilterCache, indexFieldDataService, mapperService, similarityService, scriptService, indicesQueriesRegistry);
         //create some random type with some default field, those types will stick around for all of the subclasses
         currentTypes = new String[randomIntBetween(0, 5)];
+        Map<String, CompressedXContent> mappingSources1 = new HashMap<>();
+        Map<String, CompressedXContent> mappingSources2 = new HashMap<>();
         for (int i = 0; i < currentTypes.length; i++) {
             String type = randomAsciiOfLengthBetween(1, 10);
-            mapperService.merge(type, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(type,
+            mappingSources1.put(type, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(type,
                     STRING_FIELD_NAME, "type=string",
                     STRING_FIELD_NAME_2, "type=string",
                     INT_FIELD_NAME, "type=integer",
@@ -272,12 +274,14 @@ public void onRemoval(ShardId shardId, Accountable accountable) {
                     OBJECT_FIELD_NAME, "type=object",
                     GEO_POINT_FIELD_NAME, "type=geo_point,lat_lon=true,geohash=true,geohash_prefix=true",
                     GEO_SHAPE_FIELD_NAME, "type=geo_shape"
-            ).string()), false, false);
+            ).string()));
             // also add mappings for two inner field in the object field
-            mapperService.merge(type, new CompressedXContent("{\"properties\":{\""+OBJECT_FIELD_NAME+"\":{\"type\":\"object\","
-                    + "\"properties\":{\""+DATE_FIELD_NAME+"\":{\"type\":\"date\"},\""+INT_FIELD_NAME+"\":{\"type\":\"integer\"}}}}}"), false, false);
+            mappingSources2.put(type, new CompressedXContent("{\"properties\":{\""+OBJECT_FIELD_NAME+"\":{\"type\":\"object\","
+                    + "\"properties\":{\""+DATE_FIELD_NAME+"\":{\"type\":\"date\"},\""+INT_FIELD_NAME+"\":{\"type\":\"integer\"}}}}}"));
             currentTypes[i] = type;
         }
+        mapperService.merge(mappingSources1, false, false);
+        mapperService.merge(mappingSources2, false, false);
         namedWriteableRegistry = injector.getInstance(NamedWriteableRegistry.class);
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java
index 51da0fc39960..42c1ef6b853a 100644
--- a/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java
@@ -49,9 +49,10 @@
 
 import java.io.IOException;
 import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
 
 import static org.hamcrest.Matchers.containsString;
-
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.instanceOf;
 
@@ -63,15 +64,16 @@
     public void setUp() throws Exception {
         super.setUp();
         MapperService mapperService = queryShardContext().getMapperService();
-        mapperService.merge(PARENT_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(PARENT_TYPE,
+        Map<String, CompressedXContent> mappingSources = new HashMap<>();
+        mappingSources.put(PARENT_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(PARENT_TYPE,
                 STRING_FIELD_NAME, "type=string",
                 INT_FIELD_NAME, "type=integer",
                 DOUBLE_FIELD_NAME, "type=double",
                 BOOLEAN_FIELD_NAME, "type=boolean",
                 DATE_FIELD_NAME, "type=date",
                 OBJECT_FIELD_NAME, "type=object"
-        ).string()), false, false);
-        mapperService.merge(CHILD_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(CHILD_TYPE,
+        ).string()));
+        mappingSources.put(CHILD_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(CHILD_TYPE,
                 "_parent", "type=" + PARENT_TYPE,
                 STRING_FIELD_NAME, "type=string",
                 INT_FIELD_NAME, "type=integer",
@@ -79,7 +81,8 @@ public void setUp() throws Exception {
                 BOOLEAN_FIELD_NAME, "type=boolean",
                 DATE_FIELD_NAME, "type=date",
                 OBJECT_FIELD_NAME, "type=object"
-        ).string()), false, false);
+        ).string()));
+        mapperService.merge(mappingSources, false, false);
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java
index b391930c32c7..aa610cb5ab8c 100644
--- a/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java
@@ -44,9 +44,10 @@
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
 
 import static org.hamcrest.Matchers.containsString;
-
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.instanceOf;
 
@@ -58,15 +59,16 @@
     public void setUp() throws Exception {
         super.setUp();
         MapperService mapperService = queryShardContext().getMapperService();
-        mapperService.merge(PARENT_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(PARENT_TYPE,
+        Map<String, CompressedXContent> mappingSources = new HashMap<>();
+        mappingSources.put(PARENT_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(PARENT_TYPE,
                 STRING_FIELD_NAME, "type=string",
                 INT_FIELD_NAME, "type=integer",
                 DOUBLE_FIELD_NAME, "type=double",
                 BOOLEAN_FIELD_NAME, "type=boolean",
                 DATE_FIELD_NAME, "type=date",
                 OBJECT_FIELD_NAME, "type=object"
-        ).string()), false, false);
-        mapperService.merge(CHILD_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(CHILD_TYPE,
+        ).string()));
+        mappingSources.put(CHILD_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(CHILD_TYPE,
                 "_parent", "type=" + PARENT_TYPE,
                 STRING_FIELD_NAME, "type=string",
                 INT_FIELD_NAME, "type=integer",
@@ -74,7 +76,8 @@ public void setUp() throws Exception {
                 BOOLEAN_FIELD_NAME, "type=boolean",
                 DATE_FIELD_NAME, "type=date",
                 OBJECT_FIELD_NAME, "type=object"
-        ).string()), false, false);
+        ).string()));
+        mapperService.merge(mappingSources, false, false);
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java
index 48b28ba0e096..74eb92e44e81 100644
--- a/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.query;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.join.ScoreMode;
 import org.apache.lucene.search.join.ToParentBlockJoinQuery;
@@ -35,6 +36,7 @@
 import org.elasticsearch.test.TestSearchContext;
 
 import java.io.IOException;
+import java.util.Collections;
 
 import static org.hamcrest.CoreMatchers.instanceOf;
 
@@ -44,7 +46,7 @@
     public void setUp() throws Exception {
         super.setUp();
         MapperService mapperService = queryShardContext().getMapperService();
-        mapperService.merge("nested_doc", new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef("nested_doc",
+        mapperService.merge(Collections.singletonMap("nested_doc", new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef("nested_doc",
                 STRING_FIELD_NAME, "type=string",
                 INT_FIELD_NAME, "type=integer",
                 DOUBLE_FIELD_NAME, "type=double",
@@ -52,7 +54,7 @@ public void setUp() throws Exception {
                 DATE_FIELD_NAME, "type=date",
                 OBJECT_FIELD_NAME, "type=object",
                 "nested1", "type=nested"
-        ).string()), false, false);
+        ).string())), false, false);
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/search/MultiMatchQueryTests.java b/core/src/test/java/org/elasticsearch/index/search/MultiMatchQueryTests.java
index 1aa0978e6c8d..62280456d2b8 100644
--- a/core/src/test/java/org/elasticsearch/index/search/MultiMatchQueryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/search/MultiMatchQueryTests.java
@@ -32,6 +32,7 @@
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.util.Collections;
 
 import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
 
@@ -59,7 +60,7 @@ public void setup() throws IOException {
                 "        }\n" +
                 "    }\n" +
                 "}";
-        mapperService.merge("person", new CompressedXContent(mapping), true, false);
+        mapperService.merge(Collections.singletonMap("person", new CompressedXContent(mapping)), true, false);
         this.indexService = indexService;
     }
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
index b5ef5d9eb4f8..c071c3a6eb4e 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
@@ -48,6 +48,7 @@
 
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 
 import static org.hamcrest.Matchers.equalTo;
@@ -114,7 +115,7 @@ public void testResetRootDocId() throws Exception {
         IndexSearcher searcher = new IndexSearcher(directoryReader);
 
         IndexService indexService = createIndex("test");
-        indexService.mapperService().merge("test", new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef("test", "nested_field", "type=nested").string()), true, false);
+        indexService.mapperService().merge(Collections.singletonMap("test", new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef("test", "nested_field", "type=nested").string())), true, false);
         SearchContext searchContext = createSearchContext(indexService);
         AggregationContext context = new AggregationContext(searchContext);
 
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java
index 8153d207b7c0..5a56e0f69994 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java
@@ -392,8 +392,7 @@ public void testStoredFieldsWithoutSource() throws Exception {
         createIndex("test");
         client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet();
 
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type1").startObject("properties")
-                .startObject("_source").field("enabled", false).endObject()
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type1").startObject("_source").field("enabled", false).endObject().startObject("properties")
                 .startObject("byte_field").field("type", "byte").field("store", "yes").endObject()
                 .startObject("short_field").field("type", "short").field("store", "yes").endObject()
                 .startObject("integer_field").field("type", "integer").field("store", "yes").endObject()
@@ -556,8 +555,7 @@ public void testFieldsPulledFromFieldData() throws Exception {
         createIndex("test");
         client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet();
 
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type1").startObject("properties")
-                .startObject("_source").field("enabled", false).endObject()
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type1").startObject("_source").field("enabled", false).endObject().startObject("properties")
                 .startObject("string_field").field("type", "string").endObject()
                 .startObject("byte_field").field("type", "byte").endObject()
                 .startObject("short_field").field("type", "short").endObject()
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java
index 0023fc44e246..d91092e5f41b 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java
@@ -22,14 +22,12 @@
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.junit.Test;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.StreamsUtils.copyToBytesFromClasspath;
@@ -130,11 +128,11 @@ public void testMapperErrorWithDotTwoLevels169() throws Exception {
                 .endObject()
                 .endObject();
 
-        byte[] mapping = mappingBuilder.bytes().toBytes();
+        String mapping = mappingBuilder.string();
         MapperService mapperService = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY);
-        DocumentMapper docMapper = mapperService.parse("mail", new CompressedXContent(mapping), true);
+        DocumentMapper docMapper = mapperService.documentMapperParser().parse("mail", mapping);
         // this should not throw an exception
-        mapperService.parse("mail", new CompressedXContent(docMapper.mapping().toString()), true);
+        mapperService.documentMapperParser().parse("mail", docMapper.mapping().toString());
         // the mapping may not contain a field name with a dot
         assertFalse(docMapper.mapping().toString().contains("."));
     }
diff --git a/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java
index e07b76bfc92d..7a81ead8b173 100644
--- a/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java
+++ b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java
@@ -140,7 +140,7 @@ public void testThatDisablingWorksWhenMerging() throws Exception {
                 .endObject().endObject().string();
         DocumentMapper disabledMapper = parser.parse(disabledMapping);
 
-        enabledMapper.merge(disabledMapper.mapping(), false, false);
+        enabledMapper.merge(disabledMapper.mapping(), false);
         assertThat(enabledMapper.metadataMapper(SizeFieldMapper.class).enabled(), is(false));
     }
 }
\ No newline at end of file
