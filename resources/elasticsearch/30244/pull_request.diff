diff --git a/server/src/main/java/org/elasticsearch/action/bulk/MappingUpdatePerformer.java b/server/src/main/java/org/elasticsearch/action/bulk/MappingUpdatePerformer.java
index 7f16b7c4d6d0..1f228b0f355e 100644
--- a/server/src/main/java/org/elasticsearch/action/bulk/MappingUpdatePerformer.java
+++ b/server/src/main/java/org/elasticsearch/action/bulk/MappingUpdatePerformer.java
@@ -29,11 +29,4 @@
      */
     void updateMappings(Mapping update, ShardId shardId, String type);
 
-    /**
-     *  Throws a {@code ReplicationOperation.RetryOnPrimaryException} if the operation needs to be
-     * retried on the primary due to the mappings not being present yet, or a different exception if
-     * updating the mappings on the master failed.
-     */
-    void verifyMappings(Mapping update, ShardId shardId);
-
 }
diff --git a/server/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java b/server/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
index 260c75692e19..57ecb250f7ca 100644
--- a/server/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
+++ b/server/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
@@ -43,6 +43,7 @@
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
 import org.elasticsearch.cluster.service.ClusterService;
+import org.elasticsearch.common.CheckedSupplier;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
@@ -64,7 +65,9 @@
 import org.elasticsearch.transport.TransportRequestOptions;
 import org.elasticsearch.transport.TransportService;
 
+import java.io.IOException;
 import java.util.Map;
+import java.util.function.Function;
 import java.util.function.LongSupplier;
 
 /** Performs shard-level bulk (index, delete or update) operations */
@@ -137,12 +140,15 @@ private static BulkItemResultHolder executeIndexRequest(final IndexRequest index
                                                             final IndexShard primary,
                                                             final MappingUpdatePerformer mappingUpdater) throws Exception {
         Engine.IndexResult indexResult = executeIndexRequestOnPrimary(indexRequest, primary, mappingUpdater);
-        if (indexResult.hasFailure()) {
-            return new BulkItemResultHolder(null, indexResult, bulkItemRequest);
-        } else {
-            IndexResponse response = new IndexResponse(primary.shardId(), indexRequest.type(), indexRequest.id(),
+        switch (indexResult.getResultType()) {
+            case SUCCESS:
+                IndexResponse response = new IndexResponse(primary.shardId(), indexRequest.type(), indexRequest.id(),
                     indexResult.getSeqNo(), primary.getPrimaryTerm(), indexResult.getVersion(), indexResult.isCreated());
-            return new BulkItemResultHolder(response, indexResult, bulkItemRequest);
+                return new BulkItemResultHolder(response, indexResult, bulkItemRequest);
+            case FAILURE:
+                return new BulkItemResultHolder(null, indexResult, bulkItemRequest);
+            default:
+                throw new AssertionError("unknown result type for " + indexRequest + ": " + indexResult.getResultType());
         }
     }
 
@@ -151,19 +157,24 @@ private static BulkItemResultHolder executeDeleteRequest(final DeleteRequest del
                                                              final IndexShard primary,
                                                              final MappingUpdatePerformer mappingUpdater) throws Exception {
         Engine.DeleteResult deleteResult = executeDeleteRequestOnPrimary(deleteRequest, primary, mappingUpdater);
-        if (deleteResult.hasFailure()) {
-            return new BulkItemResultHolder(null, deleteResult, bulkItemRequest);
-        } else {
-            DeleteResponse response = new DeleteResponse(primary.shardId(), deleteRequest.type(), deleteRequest.id(),
+        switch (deleteResult.getResultType()) {
+            case SUCCESS:
+                DeleteResponse response = new DeleteResponse(primary.shardId(), deleteRequest.type(), deleteRequest.id(),
                     deleteResult.getSeqNo(), primary.getPrimaryTerm(), deleteResult.getVersion(), deleteResult.isFound());
-            return new BulkItemResultHolder(response, deleteResult, bulkItemRequest);
+                return new BulkItemResultHolder(response, deleteResult, bulkItemRequest);
+            case FAILURE:
+                return new BulkItemResultHolder(null, deleteResult, bulkItemRequest);
+            case MAPPING_UPDATE_REQUIRED:
+                throw new AssertionError("delete operation leaked a mapping update " + deleteRequest);
+            default:
+                throw new AssertionError("unknown result type for " + deleteRequest + ": " + deleteResult.getResultType());
         }
     }
 
     static Translog.Location calculateTranslogLocation(final Translog.Location originalLocation,
                                                        final BulkItemResultHolder bulkItemResult) {
         final Engine.Result operationResult = bulkItemResult.operationResult;
-        if (operationResult != null && operationResult.hasFailure() == false) {
+        if (operationResult != null && operationResult.getResultType() == Engine.Result.Type.SUCCESS) {
             return locationToSync(originalLocation, operationResult.getTranslogLocation());
         } else {
             return originalLocation;
@@ -186,13 +197,13 @@ static BulkItemResponse createPrimaryResponse(BulkItemResultHolder bulkItemResul
             assert response.getResult() == DocWriteResponse.Result.NOOP : "only noop updates can have a null operation";
             return new BulkItemResponse(replicaRequest.id(), opType, response);
 
-        } else if (operationResult.hasFailure() == false) {
+        } else if (operationResult.getResultType() == Engine.Result.Type.SUCCESS) {
             BulkItemResponse primaryResponse = new BulkItemResponse(replicaRequest.id(), opType, response);
             // set a blank ShardInfo so we can safely send it to the replicas. We won't use it in the real response though.
             primaryResponse.getResponse().setShardInfo(new ShardInfo());
             return primaryResponse;
 
-        } else {
+        } else if (operationResult.getResultType() == Engine.Result.Type.FAILURE) {
             DocWriteRequest docWriteRequest = replicaRequest.request();
             Exception failure = operationResult.getFailure();
             if (isConflictException(failure)) {
@@ -217,6 +228,8 @@ static BulkItemResponse createPrimaryResponse(BulkItemResultHolder bulkItemResul
                 assert replicaRequest.getPrimaryResponse() != null : "replica request must have a primary response";
                 return null;
             }
+        } else {
+            throw new AssertionError("unknown result type for " + request + ": " + operationResult.getResultType());
         }
     }
 
@@ -273,7 +286,7 @@ private static boolean isConflictException(final Exception e) {
      */
     static BulkItemResultHolder processUpdateResponse(final UpdateRequest updateRequest, final String concreteIndex,
                                                       final Engine.Result result, final UpdateHelper.Result translate,
-                                                      final IndexShard primary, final int bulkReqId) throws Exception {
+                                                      final IndexShard primary, final int bulkReqId) {
         assert result.getSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO : "failed result should not have a sequence number";
 
         Engine.Operation.TYPE opType = result.getOperationType();
@@ -371,12 +384,14 @@ static BulkItemResultHolder executeUpdateRequestOnce(UpdateRequest updateRequest
             // this is a noop operation
             final UpdateResponse updateResponse = translate.action();
             return new BulkItemResultHolder(updateResponse, result, primaryItemRequest);
-        } else if (result.hasFailure()) {
+        } else if (result.getResultType() == Engine.Result.Type.FAILURE) {
             // There was a result, and the result was a failure
             return new BulkItemResultHolder(null, result, primaryItemRequest);
-        } else {
+        } else if (result.getResultType() == Engine.Result.Type.SUCCESS) {
             // It was successful, we need to construct the response and return it
             return processUpdateResponse(updateRequest, concreteIndex, result, translate, primary, bulkReqId);
+        } else {
+            throw new AssertionError("unknown result type for " + updateRequest + ": " + result.getResultType());
         }
     }
 
@@ -504,6 +519,7 @@ static ReplicaItemExecutionMode replicaItemExecutionMode(final BulkItemRequest r
 
     private static Engine.Result performOpOnReplica(DocWriteResponse primaryResponse, DocWriteRequest docWriteRequest,
                                                     IndexShard replica) throws Exception {
+        final Engine.Result result;
         switch (docWriteRequest.opType()) {
             case CREATE:
             case INDEX:
@@ -513,24 +529,24 @@ static ReplicaItemExecutionMode replicaItemExecutionMode(final BulkItemRequest r
                     SourceToParse.source(shardId.getIndexName(),
                         indexRequest.type(), indexRequest.id(), indexRequest.source(), indexRequest.getContentType())
                         .routing(indexRequest.routing());
-                return replica.applyIndexOperationOnReplica(primaryResponse.getSeqNo(), primaryResponse.getVersion(),
+                result = replica.applyIndexOperationOnReplica(primaryResponse.getSeqNo(), primaryResponse.getVersion(),
                     indexRequest.versionType().versionTypeForReplicationAndRecovery(), indexRequest.getAutoGeneratedTimestamp(),
-                    indexRequest.isRetry(), sourceToParse, update -> {
-                        throw new TransportReplicationAction.RetryOnReplicaException(replica.shardId(),
-                            "Mappings are not available on the replica yet, triggered update: " + update);
-                    });
+                    indexRequest.isRetry(), sourceToParse);
+                break;
             case DELETE:
                 DeleteRequest deleteRequest = (DeleteRequest) docWriteRequest;
-                return replica.applyDeleteOperationOnReplica(primaryResponse.getSeqNo(), primaryResponse.getVersion(),
-                    deleteRequest.type(), deleteRequest.id(), deleteRequest.versionType().versionTypeForReplicationAndRecovery(),
-                    update -> {
-                        throw new TransportReplicationAction.RetryOnReplicaException(replica.shardId(),
-                            "Mappings are not available on the replica yet, triggered update: " + update);
-                    });
+                result =  replica.applyDeleteOperationOnReplica(primaryResponse.getSeqNo(), primaryResponse.getVersion(),
+                    deleteRequest.type(), deleteRequest.id(), deleteRequest.versionType().versionTypeForReplicationAndRecovery());
+                break;
             default:
                 throw new IllegalStateException("Unexpected request operation type on replica: "
                     + docWriteRequest.opType().getLowercase());
         }
+        if (result.getResultType() == Engine.Result.Type.MAPPING_UPDATE_REQUIRED) {
+            throw new TransportReplicationAction.RetryOnReplicaException(replica.shardId(),
+                "Mappings are not available on the replica yet, triggered update: " + result.getRequiredMappingUpdate());
+        }
+        return result;
     }
 
     /** Executes index operation on primary shard after updates mapping if dynamic mappings are found */
@@ -539,50 +555,61 @@ static ReplicaItemExecutionMode replicaItemExecutionMode(final BulkItemRequest r
         final SourceToParse sourceToParse =
             SourceToParse.source(request.index(), request.type(), request.id(), request.source(), request.getContentType())
                 .routing(request.routing());
-        try {
-            // if a mapping update is required to index this request, issue a mapping update on the master, and abort the
-            // current indexing operation so that it can be retried with the updated mapping from the master
-            // The early abort uses the RetryOnPrimaryException, but any other exception would be fine as well.
-            return primary.applyIndexOperationOnPrimary(request.version(), request.versionType(), sourceToParse,
-                request.getAutoGeneratedTimestamp(), request.isRetry(), update -> {
-                    mappingUpdater.updateMappings(update, primary.shardId(), sourceToParse.type());
-                    throw new ReplicationOperation.RetryOnPrimaryException(primary.shardId(), "Mapping updated");
-                });
-        } catch (ReplicationOperation.RetryOnPrimaryException e) {
-            return primary.applyIndexOperationOnPrimary(request.version(), request.versionType(), sourceToParse,
-                request.getAutoGeneratedTimestamp(), request.isRetry(), update -> mappingUpdater.verifyMappings(update, primary.shardId()));
-        }
+        return executeOnPrimaryWhileHandlingMappingUpdates(primary.shardId(), request.type(),
+            () ->
+                primary.applyIndexOperationOnPrimary(request.version(), request.versionType(), sourceToParse,
+                    request.getAutoGeneratedTimestamp(), request.isRetry()),
+            e -> new Engine.IndexResult(e, request.version()),
+            mappingUpdater);
     }
 
     private static Engine.DeleteResult executeDeleteRequestOnPrimary(DeleteRequest request, IndexShard primary,
                                                                      MappingUpdatePerformer mappingUpdater) throws Exception {
-        try {
-            return primary.applyDeleteOperationOnPrimary(request.version(), request.type(), request.id(), request.versionType(),
-                update -> {
-                    mappingUpdater.updateMappings(update, primary.shardId(), request.type());
-                    throw new ReplicationOperation.RetryOnPrimaryException(primary.shardId(), "Mapping updated");
-                });
-        } catch (ReplicationOperation.RetryOnPrimaryException e) {
-            return primary.applyDeleteOperationOnPrimary(request.version(), request.type(), request.id(), request.versionType(),
-                update -> mappingUpdater.verifyMappings(update, primary.shardId()));
-        }
+        return executeOnPrimaryWhileHandlingMappingUpdates(primary.shardId(), request.type(),
+            () -> primary.applyDeleteOperationOnPrimary(request.version(), request.type(), request.id(), request.versionType()),
+            e -> new Engine.DeleteResult(e, request.version()),
+            mappingUpdater);
     }
 
-    class ConcreteMappingUpdatePerformer implements MappingUpdatePerformer {
-
-        public void updateMappings(final Mapping update, final ShardId shardId, final String type) {
-            if (update != null) {
-                // can throw timeout exception when updating mappings or ISE for attempting to
-                // update default mappings which are bubbled up
-                mappingUpdatedAction.updateMappingOnMaster(shardId.getIndex(), type, update);
+    private static <T extends Engine.Result> T executeOnPrimaryWhileHandlingMappingUpdates(ShardId shardId, String type,
+                                                                                           CheckedSupplier<T, IOException> toExecute,
+                                                                                           Function<Exception, T> onError,
+                                                                                           MappingUpdatePerformer mappingUpdater)
+        throws IOException {
+        T result = toExecute.get();
+        if (result.getResultType() == Engine.Result.Type.MAPPING_UPDATE_REQUIRED) {
+            // try to update the mappings and try again.
+            try {
+                mappingUpdater.updateMappings(result.getRequiredMappingUpdate(), shardId, type);
+            } catch (Exception e) {
+                // failure to update the mapping should translate to a failure of specific requests. Other requests
+                // still need to be executed and replicated.
+                return onError.apply(e);
             }
-        }
 
-        public void verifyMappings(final Mapping update, final ShardId shardId) {
-            if (update != null) {
+            result = toExecute.get();
+
+            if (result.getResultType() == Engine.Result.Type.MAPPING_UPDATE_REQUIRED) {
+                // double mapping update. We assume that the successful mapping update wasn't yet processed on the node
+                // and retry the entire request again.
                 throw new ReplicationOperation.RetryOnPrimaryException(shardId,
-                        "Dynamic mappings are not available on the node that holds the primary yet");
+                    "Dynamic mappings are not available on the node that holds the primary yet");
             }
         }
+        assert result.getFailure() instanceof ReplicationOperation.RetryOnPrimaryException == false :
+            "IndexShard shouldn't use RetryOnPrimaryException. got " + result.getFailure();
+        return result;
+
+    }
+
+    class ConcreteMappingUpdatePerformer implements MappingUpdatePerformer {
+
+        public void updateMappings(final Mapping update, final ShardId shardId, final String type) {
+            assert update != null;
+            assert shardId != null;
+            // can throw timeout exception when updating mappings or ISE for attempting to
+            // update default mappings which are bubbled up
+            mappingUpdatedAction.updateMappingOnMaster(shardId.getIndex(), type, update);
+        }
     }
 }
diff --git a/server/src/main/java/org/elasticsearch/action/resync/TransportResyncReplicationAction.java b/server/src/main/java/org/elasticsearch/action/resync/TransportResyncReplicationAction.java
index c182fb24ffb1..3dd2bd4df580 100644
--- a/server/src/main/java/org/elasticsearch/action/resync/TransportResyncReplicationAction.java
+++ b/server/src/main/java/org/elasticsearch/action/resync/TransportResyncReplicationAction.java
@@ -122,11 +122,11 @@ protected WriteReplicaResult shardOperationOnReplica(ResyncReplicationRequest re
         Translog.Location location = null;
         for (Translog.Operation operation : request.getOperations()) {
             try {
-                final Engine.Result operationResult = replica.applyTranslogOperation(operation, Engine.Operation.Origin.REPLICA,
-                    update -> {
-                        throw new TransportReplicationAction.RetryOnReplicaException(replica.shardId(),
-                            "Mappings are not available on the replica yet, triggered update: " + update);
-                    });
+                final Engine.Result operationResult = replica.applyTranslogOperation(operation, Engine.Operation.Origin.REPLICA);
+                if (operationResult.getResultType() == Engine.Result.Type.MAPPING_UPDATE_REQUIRED) {
+                    throw new TransportReplicationAction.RetryOnReplicaException(replica.shardId(),
+                        "Mappings are not available on the replica yet, triggered update: " + operationResult.getRequiredMappingUpdate());
+                }
                 location = syncOperationResultOrThrow(operationResult, location);
             } catch (Exception e) {
                 // if its not a failure to be ignored, let it bubble up
diff --git a/server/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java b/server/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java
index bdddcddaa2e1..b14fd156b735 100644
--- a/server/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java
+++ b/server/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java
@@ -33,23 +33,17 @@
 import org.elasticsearch.cluster.service.ClusterService;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.VersionType;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.mapper.Mapping;
-import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.translog.Translog;
 import org.elasticsearch.index.translog.Translog.Location;
 import org.elasticsearch.indices.IndicesService;
-import org.elasticsearch.node.NodeClosedException;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportException;
 import org.elasticsearch.transport.TransportResponse;
 import org.elasticsearch.transport.TransportService;
 
-import java.io.IOException;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
@@ -78,7 +72,7 @@ protected TransportWriteAction(Settings settings, String actionName, TransportSe
     protected static Location syncOperationResultOrThrow(final Engine.Result operationResult,
                                                          final Location currentLocation) throws Exception {
         final Location location;
-        if (operationResult.hasFailure()) {
+        if (operationResult.getFailure() != null) {
             // check if any transient write operation failures should be bubbled up
             Exception failure = operationResult.getFailure();
             assert failure instanceof MapperParsingException : "expected mapper parsing failures. got " + failure;
diff --git a/server/src/main/java/org/elasticsearch/index/IndexingSlowLog.java b/server/src/main/java/org/elasticsearch/index/IndexingSlowLog.java
index b75cda5b6ca7..8293f873c65e 100644
--- a/server/src/main/java/org/elasticsearch/index/IndexingSlowLog.java
+++ b/server/src/main/java/org/elasticsearch/index/IndexingSlowLog.java
@@ -144,7 +144,7 @@ private void setReformat(boolean reformat) {
 
     @Override
     public void postIndex(ShardId shardId, Engine.Index indexOperation, Engine.IndexResult result) {
-        if (result.hasFailure() == false) {
+        if (result.getResultType() == Engine.Result.Type.SUCCESS) {
             final ParsedDocument doc = indexOperation.parsedDoc();
             final long tookInNanos = result.getTook();
             if (indexWarnThreshold >= 0 && tookInNanos > indexWarnThreshold) {
diff --git a/server/src/main/java/org/elasticsearch/index/engine/Engine.java b/server/src/main/java/org/elasticsearch/index/engine/Engine.java
index 4c782cb50041..e1ebd0ecc299 100644
--- a/server/src/main/java/org/elasticsearch/index/engine/Engine.java
+++ b/server/src/main/java/org/elasticsearch/index/engine/Engine.java
@@ -58,6 +58,7 @@
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.ReleasableLock;
 import org.elasticsearch.index.VersionType;
+import org.elasticsearch.index.mapper.Mapping;
 import org.elasticsearch.index.mapper.ParseContext.Document;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.merge.MergeStats;
@@ -295,27 +296,45 @@ public Condition newCondition() {
      **/
     public abstract static class Result {
         private final Operation.TYPE operationType;
+        private final Result.Type resultType;
         private final long version;
         private final long seqNo;
         private final Exception failure;
         private final SetOnce<Boolean> freeze = new SetOnce<>();
+        private final Mapping requiredMappingUpdate;
         private Translog.Location translogLocation;
         private long took;
 
         protected Result(Operation.TYPE operationType, Exception failure, long version, long seqNo) {
             this.operationType = operationType;
-            this.failure = failure;
+            this.failure = Objects.requireNonNull(failure);
             this.version = version;
             this.seqNo = seqNo;
+            this.requiredMappingUpdate = null;
+            this.resultType = Type.FAILURE;
         }
 
         protected Result(Operation.TYPE operationType, long version, long seqNo) {
-            this(operationType, null, version, seqNo);
+            this.operationType = operationType;
+            this.version = version;
+            this.seqNo = seqNo;
+            this.failure = null;
+            this.requiredMappingUpdate = null;
+            this.resultType = Type.SUCCESS;
+        }
+
+        protected Result(Operation.TYPE operationType, Mapping requiredMappingUpdate) {
+            this.operationType = operationType;
+            this.version = Versions.NOT_FOUND;
+            this.seqNo = SequenceNumbers.UNASSIGNED_SEQ_NO;
+            this.failure = null;
+            this.requiredMappingUpdate = requiredMappingUpdate;
+            this.resultType = Type.MAPPING_UPDATE_REQUIRED;
         }
 
-        /** whether the operation had failure */
-        public boolean hasFailure() {
-            return failure != null;
+        /** whether the operation was successful, has failed or was aborted due to a mapping update */
+        public Type getResultType() {
+            return resultType;
         }
 
         /** get the updated document version */
@@ -332,6 +351,14 @@ public long getSeqNo() {
             return seqNo;
         }
 
+        /**
+         * If the operation was aborted due to missing mappings, this method will return the mappings
+         * that are required to complete the operation.
+         */
+        public Mapping getRequiredMappingUpdate() {
+            return requiredMappingUpdate;
+        }
+
         /** get the translog location after executing the operation */
         public Translog.Location getTranslogLocation() {
             return translogLocation;
@@ -371,6 +398,11 @@ void freeze() {
             freeze.set(true);
         }
 
+        public enum Type {
+            SUCCESS,
+            FAILURE,
+            MAPPING_UPDATE_REQUIRED
+        }
     }
 
     public static class IndexResult extends Result {
@@ -383,9 +415,8 @@ public IndexResult(long version, long seqNo, boolean created) {
         }
 
         /**
-         * use in case of index operation failed before getting to internal engine
-         * (e.g while preparing operation or updating mappings)
-         * */
+         * use in case of the index operation failed before getting to internal engine
+         **/
         public IndexResult(Exception failure, long version) {
             this(failure, version, SequenceNumbers.UNASSIGNED_SEQ_NO);
         }
@@ -395,6 +426,11 @@ public IndexResult(Exception failure, long version, long seqNo) {
             this.created = false;
         }
 
+        public IndexResult(Mapping requiredMappingUpdate) {
+            super(Operation.TYPE.INDEX, requiredMappingUpdate);
+            this.created = false;
+        }
+
         public boolean isCreated() {
             return created;
         }
@@ -410,11 +446,23 @@ public DeleteResult(long version, long seqNo, boolean found) {
             this.found = found;
         }
 
+        /**
+         * use in case of the delete operation failed before getting to internal engine
+         **/
+        public DeleteResult(Exception failure, long version) {
+            this(failure, version, SequenceNumbers.UNASSIGNED_SEQ_NO, false);
+        }
+
         public DeleteResult(Exception failure, long version, long seqNo, boolean found) {
             super(Operation.TYPE.DELETE, failure, version, seqNo);
             this.found = found;
         }
 
+        public DeleteResult(Mapping requiredMappingUpdate) {
+            super(Operation.TYPE.DELETE, requiredMappingUpdate);
+            this.found = false;
+        }
+
         public boolean isFound() {
             return found;
         }
diff --git a/server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java b/server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
index f89595c1c23a..bca84f81a29c 100644
--- a/server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
+++ b/server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
@@ -765,7 +765,7 @@ public IndexResult index(Index index) throws IOException {
                 final IndexResult indexResult;
                 if (plan.earlyResultOnPreFlightError.isPresent()) {
                     indexResult = plan.earlyResultOnPreFlightError.get();
-                    assert indexResult.hasFailure();
+                    assert indexResult.getResultType() == Result.Type.FAILURE : indexResult.getResultType();
                 } else if (plan.indexIntoLucene) {
                     indexResult = indexIntoLucene(index, plan);
                 } else {
@@ -774,7 +774,7 @@ public IndexResult index(Index index) throws IOException {
                 }
                 if (index.origin() != Operation.Origin.LOCAL_TRANSLOG_RECOVERY) {
                     final Translog.Location location;
-                    if (indexResult.hasFailure() == false) {
+                    if (indexResult.getResultType() == Result.Type.SUCCESS) {
                         location = translog.add(new Translog.Index(index, indexResult));
                     } else if (indexResult.getSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO) {
                         // if we have document failure, record it as a no-op in the translog with the generated seq_no
@@ -784,7 +784,7 @@ public IndexResult index(Index index) throws IOException {
                     }
                     indexResult.setTranslogLocation(location);
                 }
-                if (plan.indexIntoLucene && indexResult.hasFailure() == false) {
+                if (plan.indexIntoLucene && indexResult.getResultType() == Result.Type.SUCCESS) {
                     final Translog.Location translogLocation = trackTranslogLocation.get() ? indexResult.getTranslogLocation() : null;
                     versionMap.maybePutIndexUnderLock(index.uid().bytes(),
                         new IndexVersionValue(translogLocation, plan.versionForIndexing, plan.seqNoForIndexing, index.primaryTerm()));
@@ -1087,7 +1087,7 @@ public DeleteResult delete(Delete delete) throws IOException {
             }
             if (delete.origin() != Operation.Origin.LOCAL_TRANSLOG_RECOVERY) {
                 final Translog.Location location;
-                if (deleteResult.hasFailure() == false) {
+                if (deleteResult.getResultType() == Result.Type.SUCCESS) {
                     location = translog.add(new Translog.Delete(delete, deleteResult));
                 } else if (deleteResult.getSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO) {
                     location = translog.add(new Translog.NoOp(deleteResult.getSeqNo(),
diff --git a/server/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/server/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index def6362e334e..92240e9b4637 100644
--- a/server/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/server/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -636,22 +636,21 @@ private IndexShardState changeState(IndexShardState newState, String reason) {
     }
 
     public Engine.IndexResult applyIndexOperationOnPrimary(long version, VersionType versionType, SourceToParse sourceToParse,
-                                                           long autoGeneratedTimestamp, boolean isRetry,
-                                                           Consumer<Mapping> onMappingUpdate) throws IOException {
+                                                           long autoGeneratedTimestamp, boolean isRetry) throws IOException {
         return applyIndexOperation(SequenceNumbers.UNASSIGNED_SEQ_NO, primaryTerm, version, versionType, autoGeneratedTimestamp,
-            isRetry, Engine.Operation.Origin.PRIMARY, sourceToParse, onMappingUpdate);
+            isRetry, Engine.Operation.Origin.PRIMARY, sourceToParse);
     }
 
     public Engine.IndexResult applyIndexOperationOnReplica(long seqNo, long version, VersionType versionType,
-                                                           long autoGeneratedTimeStamp, boolean isRetry, SourceToParse sourceToParse,
-                                                           Consumer<Mapping> onMappingUpdate) throws IOException {
+                                                           long autoGeneratedTimeStamp, boolean isRetry, SourceToParse sourceToParse)
+        throws IOException {
         return applyIndexOperation(seqNo, primaryTerm, version, versionType, autoGeneratedTimeStamp, isRetry,
-            Engine.Operation.Origin.REPLICA, sourceToParse, onMappingUpdate);
+            Engine.Operation.Origin.REPLICA, sourceToParse);
     }
 
     private Engine.IndexResult applyIndexOperation(long seqNo, long opPrimaryTerm, long version, VersionType versionType,
                                                    long autoGeneratedTimeStamp, boolean isRetry, Engine.Operation.Origin origin,
-                                                   SourceToParse sourceToParse, Consumer<Mapping> onMappingUpdate) throws IOException {
+                                                   SourceToParse sourceToParse) throws IOException {
         assert opPrimaryTerm <= this.primaryTerm : "op term [ " + opPrimaryTerm + " ] > shard term [" + this.primaryTerm + "]";
         assert versionType.validateVersionForWrites(version);
         ensureWriteAllowed(origin);
@@ -662,14 +661,15 @@ private IndexShardState changeState(IndexShardState newState, String reason) {
                 autoGeneratedTimeStamp, isRetry);
             Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
             if (update != null) {
-                // wrap this in the outer catch block, as the master might also throw a MapperParsingException when updating the mapping
-                onMappingUpdate.accept(update);
+                return new Engine.IndexResult(update);
             }
-        } catch (MapperParsingException | IllegalArgumentException | TypeMissingException e) {
-            return new Engine.IndexResult(e, version, seqNo);
         } catch (Exception e) {
+            // We treat any exception during parsing and or mapping update as a document level failure
+            // with the exception side effects of closing the shard. Since we don't have the shard, we
+            // can not raise an exception that may block any replication of previous operations to the
+            // replicas
             verifyNotClosed(e);
-            throw e;
+            return new Engine.IndexResult(e, version, seqNo);
         }
 
         return index(getEngine(), operation);
@@ -726,21 +726,19 @@ private IndexShardState changeState(IndexShardState newState, String reason) {
         return engine.noOp(noOp);
     }
 
-    public Engine.DeleteResult applyDeleteOperationOnPrimary(long version, String type, String id, VersionType versionType,
-                                                             Consumer<Mapping> onMappingUpdate) throws IOException {
+    public Engine.DeleteResult applyDeleteOperationOnPrimary(long version, String type, String id, VersionType versionType)
+        throws IOException {
         return applyDeleteOperation(SequenceNumbers.UNASSIGNED_SEQ_NO, primaryTerm, version, type, id, versionType,
-            Engine.Operation.Origin.PRIMARY, onMappingUpdate);
+            Engine.Operation.Origin.PRIMARY);
     }
 
     public Engine.DeleteResult applyDeleteOperationOnReplica(long seqNo, long version, String type, String id,
-                                                             VersionType versionType,
-                                                             Consumer<Mapping> onMappingUpdate) throws IOException {
-        return applyDeleteOperation(seqNo, primaryTerm, version, type, id, versionType, Engine.Operation.Origin.REPLICA, onMappingUpdate);
+                                                             VersionType versionType) throws IOException {
+        return applyDeleteOperation(seqNo, primaryTerm, version, type, id, versionType, Engine.Operation.Origin.REPLICA);
     }
 
     private Engine.DeleteResult applyDeleteOperation(long seqNo, long opPrimaryTerm, long version, String type, String id,
-                                                     VersionType versionType, Engine.Operation.Origin origin,
-                                                     Consumer<Mapping> onMappingUpdate) throws IOException {
+                                                     VersionType versionType, Engine.Operation.Origin origin) throws IOException {
         assert opPrimaryTerm <= this.primaryTerm : "op term [ " + opPrimaryTerm + " ] > shard term [" + this.primaryTerm + "]";
         assert versionType.validateVersionForWrites(version);
         ensureWriteAllowed(origin);
@@ -755,7 +753,7 @@ private IndexShardState changeState(IndexShardState newState, String reason) {
         try {
             Mapping update = docMapper(type).getMapping();
             if (update != null) {
-                onMappingUpdate.accept(update);
+                return new Engine.DeleteResult(update);
             }
         } catch (MapperParsingException | IllegalArgumentException | TypeMissingException e) {
             return new Engine.DeleteResult(e, version, seqNo, false);
@@ -1197,8 +1195,7 @@ public void prepareForIndexRecovery() {
         assert currentEngineReference.get() == null;
     }
 
-    public Engine.Result applyTranslogOperation(Translog.Operation operation, Engine.Operation.Origin origin,
-                                                Consumer<Mapping> onMappingUpdate) throws IOException {
+    public Engine.Result applyTranslogOperation(Translog.Operation operation, Engine.Operation.Origin origin) throws IOException {
         final Engine.Result result;
         switch (operation.opType()) {
             case INDEX:
@@ -1208,13 +1205,12 @@ public void prepareForIndexRecovery() {
                 result = applyIndexOperation(index.seqNo(), index.primaryTerm(), index.version(),
                     index.versionType().versionTypeForReplicationAndRecovery(), index.getAutoGeneratedIdTimestamp(), true, origin,
                     source(shardId.getIndexName(), index.type(), index.id(), index.source(),
-                        XContentHelper.xContentType(index.source()))
-                        .routing(index.routing()), onMappingUpdate);
+                        XContentHelper.xContentType(index.source())).routing(index.routing()));
                 break;
             case DELETE:
                 final Translog.Delete delete = (Translog.Delete) operation;
                 result = applyDeleteOperation(delete.seqNo(), delete.primaryTerm(), delete.version(), delete.type(), delete.id(),
-                    delete.versionType().versionTypeForReplicationAndRecovery(), origin, onMappingUpdate);
+                    delete.versionType().versionTypeForReplicationAndRecovery(), origin);
                 break;
             case NO_OP:
                 final Translog.NoOp noOp = (Translog.NoOp) operation;
@@ -1235,10 +1231,18 @@ int runTranslogRecovery(Engine engine, Translog.Snapshot snapshot) throws IOExce
         while ((operation = snapshot.next()) != null) {
             try {
                 logger.trace("[translog] recover op {}", operation);
-                Engine.Result result = applyTranslogOperation(operation, Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY, update -> {
-                    throw new IllegalArgumentException("unexpected mapping update: " + update);
-                });
-                ExceptionsHelper.reThrowIfNotNull(result.getFailure());
+                Engine.Result result = applyTranslogOperation(operation, Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY);
+                switch (result.getResultType()) {
+                    case FAILURE:
+                        throw result.getFailure();
+                    case MAPPING_UPDATE_REQUIRED:
+                        throw new IllegalArgumentException("unexpected mapping update: " + result.getRequiredMappingUpdate());
+                    case SUCCESS:
+                        break;
+                    default:
+                        throw new AssertionError("Unknown result type [" + result.getResultType() + "]");
+                }
+
                 opsRecovered++;
                 recoveryState.getTranslog().incrementRecoveredOperations();
             } catch (Exception e) {
@@ -1246,7 +1250,7 @@ int runTranslogRecovery(Engine engine, Translog.Snapshot snapshot) throws IOExce
                     // mainly for MapperParsingException and Failure to detect xcontent
                     logger.info("ignoring recovery of a corrupt translog entry", e);
                 } else {
-                    throw e;
+                    throw ExceptionsHelper.convertToRuntime(e);
                 }
             }
         }
diff --git a/server/src/main/java/org/elasticsearch/index/shard/InternalIndexingStats.java b/server/src/main/java/org/elasticsearch/index/shard/InternalIndexingStats.java
index ada869a1d9c0..e7b7b719aed6 100644
--- a/server/src/main/java/org/elasticsearch/index/shard/InternalIndexingStats.java
+++ b/server/src/main/java/org/elasticsearch/index/shard/InternalIndexingStats.java
@@ -66,7 +66,7 @@ IndexingStats stats(boolean isThrottled, long currentThrottleInMillis, String...
 
     @Override
     public Engine.Index preIndex(ShardId shardId, Engine.Index operation) {
-        if (!operation.origin().isRecovery()) {
+        if (operation.origin().isRecovery() == false) {
             totalStats.indexCurrent.inc();
             typeStats(operation.type()).indexCurrent.inc();
         }
@@ -75,17 +75,22 @@ IndexingStats stats(boolean isThrottled, long currentThrottleInMillis, String...
 
     @Override
     public void postIndex(ShardId shardId, Engine.Index index, Engine.IndexResult result) {
-        if (result.hasFailure() == false) {
-            if (!index.origin().isRecovery()) {
-                long took = result.getTook();
-                totalStats.indexMetric.inc(took);
-                totalStats.indexCurrent.dec();
-                StatsHolder typeStats = typeStats(index.type());
-                typeStats.indexMetric.inc(took);
-                typeStats.indexCurrent.dec();
-            }
-        } else {
-            postIndex(shardId, index, result.getFailure());
+        switch (result.getResultType()) {
+            case SUCCESS:
+                if (index.origin().isRecovery() == false) {
+                    long took = result.getTook();
+                    totalStats.indexMetric.inc(took);
+                    totalStats.indexCurrent.dec();
+                    StatsHolder typeStats = typeStats(index.type());
+                    typeStats.indexMetric.inc(took);
+                    typeStats.indexCurrent.dec();
+                }
+                break;
+            case FAILURE:
+                postIndex(shardId, index, result.getFailure());
+                break;
+            default:
+                throw new IllegalArgumentException("unknown result type: " + result.getResultType());
         }
     }
 
@@ -111,17 +116,22 @@ public void postIndex(ShardId shardId, Engine.Index index, Exception ex) {
 
     @Override
     public void postDelete(ShardId shardId, Engine.Delete delete, Engine.DeleteResult result) {
-        if (result.hasFailure() == false) {
-            if (!delete.origin().isRecovery()) {
-                long took = result.getTook();
-                totalStats.deleteMetric.inc(took);
-                totalStats.deleteCurrent.dec();
-                StatsHolder typeStats = typeStats(delete.type());
-                typeStats.deleteMetric.inc(took);
-                typeStats.deleteCurrent.dec();
-            }
-        } else {
-            postDelete(shardId, delete, result.getFailure());
+        switch (result.getResultType()) {
+            case SUCCESS:
+                if (!delete.origin().isRecovery()) {
+                    long took = result.getTook();
+                    totalStats.deleteMetric.inc(took);
+                    totalStats.deleteCurrent.dec();
+                    StatsHolder typeStats = typeStats(delete.type());
+                    typeStats.deleteMetric.inc(took);
+                    typeStats.deleteCurrent.dec();
+                }
+                break;
+            case FAILURE:
+                postDelete(shardId, delete, result.getFailure());
+                break;
+            default:
+                throw new IllegalArgumentException("unknown result type: " + result.getResultType());
         }
     }
 
diff --git a/server/src/main/java/org/elasticsearch/indices/IndexingMemoryController.java b/server/src/main/java/org/elasticsearch/indices/IndexingMemoryController.java
index d8e2ec535476..ac5a5047464b 100644
--- a/server/src/main/java/org/elasticsearch/indices/IndexingMemoryController.java
+++ b/server/src/main/java/org/elasticsearch/indices/IndexingMemoryController.java
@@ -34,8 +34,8 @@
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.IndexingOperationListener;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.threadpool.Scheduler.Cancellable;
+import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.threadpool.ThreadPool.Names;
 
 import java.io.Closeable;
@@ -210,7 +210,7 @@ public void postDelete(ShardId shardId, Engine.Delete delete, Engine.DeleteResul
 
     /** called by IndexShard to record estimated bytes written to translog for the operation */
     private void recordOperationBytes(Engine.Operation operation, Engine.Result result) {
-        if (result.hasFailure() == false) {
+        if (result.getResultType() == Engine.Result.Type.SUCCESS) {
             statusChecker.bytesWritten(operation.estimatedSizeInBytes());
         }
     }
diff --git a/server/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java b/server/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
index 244bb462df6a..57deb4666dac 100644
--- a/server/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
+++ b/server/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
@@ -394,10 +394,11 @@ public long indexTranslogOperations(List<Translog.Operation> operations, int tot
             throw new IndexShardNotRecoveringException(shardId, indexShard().state());
         }
         for (Translog.Operation operation : operations) {
-            Engine.Result result = indexShard().applyTranslogOperation(operation, Engine.Operation.Origin.PEER_RECOVERY, update -> {
+            Engine.Result result = indexShard().applyTranslogOperation(operation, Engine.Operation.Origin.PEER_RECOVERY);
+            if (result.getResultType() == Engine.Result.Type.MAPPING_UPDATE_REQUIRED) {
                 throw new MapperException("mapping updates are not allowed [" + operation + "]");
-            });
-            assert result.hasFailure() == false : "unexpected failure while replicating translog entry: " + result.getFailure();
+            }
+            assert result.getFailure() == null: "unexpected failure while replicating translog entry: " + result.getFailure();
             ExceptionsHelper.reThrowIfNotNull(result.getFailure());
         }
         // update stats only after all operations completed (to ensure that mapping updates don't mess with stats)
diff --git a/server/src/test/java/org/elasticsearch/action/bulk/TransportShardBulkActionTests.java b/server/src/test/java/org/elasticsearch/action/bulk/TransportShardBulkActionTests.java
index dd76564ca328..f4a72dccdcc7 100644
--- a/server/src/test/java/org/elasticsearch/action/bulk/TransportShardBulkActionTests.java
+++ b/server/src/test/java/org/elasticsearch/action/bulk/TransportShardBulkActionTests.java
@@ -82,7 +82,7 @@
 
     private IndexMetaData indexMetaData() throws IOException {
         return IndexMetaData.builder("index")
-                .putMapping("type",
+                .putMapping("_doc",
                         "{\"properties\":{\"foo\":{\"type\":\"text\",\"fields\":" +
                                 "{\"keyword\":{\"type\":\"keyword\",\"ignore_above\":256}}}}}")
                 .settings(idxSettings)
@@ -91,7 +91,7 @@ private IndexMetaData indexMetaData() throws IOException {
 
     public void testShouldExecuteReplicaItem() throws Exception {
         // Successful index request should be replicated
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id")
                 .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar");
         DocWriteResponse response = new IndexResponse(shardId, "type", "id", 1, 17, 1, randomBoolean());
         BulkItemRequest request = new BulkItemRequest(0, writeRequest);
@@ -100,7 +100,7 @@ public void testShouldExecuteReplicaItem() throws Exception {
                 equalTo(ReplicaItemExecutionMode.NORMAL));
 
         // Failed index requests without sequence no should not be replicated
-        writeRequest = new IndexRequest("index", "type", "id")
+        writeRequest = new IndexRequest("index", "_doc", "id")
                 .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar");
         request = new BulkItemRequest(0, writeRequest);
         request.setPrimaryResponse(
@@ -137,9 +137,7 @@ public void testExecuteBulkIndexRequest() throws Exception {
 
         BulkItemRequest[] items = new BulkItemRequest[1];
         boolean create = randomBoolean();
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
-                .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar")
-                .create(create);
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id").source(Requests.INDEX_CONTENT_TYPE).create(create);
         BulkItemRequest primaryRequest = new BulkItemRequest(0, writeRequest);
         items[0] = primaryRequest;
         BulkShardRequest bulkShardRequest =
@@ -166,9 +164,7 @@ public void testExecuteBulkIndexRequest() throws Exception {
         // Assert that the document actually made it there
         assertDocCount(shard, 1);
 
-        writeRequest = new IndexRequest("index", "type", "id")
-                .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar")
-                .create(true);
+        writeRequest = new IndexRequest("index", "_doc", "id").source(Requests.INDEX_CONTENT_TYPE).create(true);
         primaryRequest = new BulkItemRequest(0, writeRequest);
         items[0] = primaryRequest;
         bulkShardRequest = new BulkShardRequest(shardId, RefreshPolicy.NONE, items);
@@ -176,7 +172,7 @@ public void testExecuteBulkIndexRequest() throws Exception {
         Translog.Location secondLocation =
                 TransportShardBulkAction.executeBulkItemRequest( metaData,
                         shard, bulkShardRequest, newLocation, 0, updateHelper,
-                        threadPool::absoluteTimeInMillis, new NoopMappingUpdatePerformer());
+                        threadPool::absoluteTimeInMillis, new ThrowingMappingUpdatePerformer(new RuntimeException("fail")));
 
         // Translog should not change, since the document was not indexed due to a version conflict
         assertThat(secondLocation, equalTo(newLocation));
@@ -193,7 +189,7 @@ public void testExecuteBulkIndexRequest() throws Exception {
 
         BulkItemResponse.Failure failure = primaryResponse.getFailure();
         assertThat(failure.getIndex(), equalTo("index"));
-        assertThat(failure.getType(), equalTo("type"));
+        assertThat(failure.getType(), equalTo("_doc"));
         assertThat(failure.getId(), equalTo("id"));
         assertThat(failure.getCause().getClass(), equalTo(VersionConflictEngineException.class));
         assertThat(failure.getCause().getMessage(),
@@ -212,8 +208,8 @@ public void testSkipBulkIndexRequestIfAborted() throws Exception {
 
         BulkItemRequest[] items = new BulkItemRequest[randomIntBetween(2, 5)];
         for (int i = 0; i < items.length; i++) {
-            DocWriteRequest writeRequest = new IndexRequest("index", "type", "id_" + i)
-                .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar-" + i)
+            DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id_" + i)
+                .source(Requests.INDEX_CONTENT_TYPE)
                 .opType(DocWriteRequest.OpType.INDEX);
             items[i] = new BulkItemRequest(i, writeRequest);
         }
@@ -240,7 +236,7 @@ public void testSkipBulkIndexRequestIfAborted() throws Exception {
             BulkItemResponse response = result.finalResponseIfSuccessful.getResponses()[i];
             assertThat(response.getItemId(), equalTo(i));
             assertThat(response.getIndex(), equalTo("index"));
-            assertThat(response.getType(), equalTo("type"));
+            assertThat(response.getType(), equalTo("_doc"));
             assertThat(response.getId(), equalTo("id_" + i));
             assertThat(response.getOpType(), equalTo(DocWriteRequest.OpType.INDEX));
             if (response.getItemId() == rejectItem.id()) {
@@ -262,7 +258,7 @@ public void testExecuteBulkIndexRequestWithRejection() throws Exception {
         IndexShard shard = newStartedShard(true);
 
         BulkItemRequest[] items = new BulkItemRequest[1];
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id")
                 .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar");
         items[0] = new BulkItemRequest(0, writeRequest);
         BulkShardRequest bulkShardRequest =
@@ -272,26 +268,20 @@ public void testExecuteBulkIndexRequestWithRejection() throws Exception {
         UpdateHelper updateHelper = null;
 
         // Pretend the mappings haven't made it to the node yet, and throw a rejection
-        RuntimeException err = new ReplicationOperation.RetryOnPrimaryException(shardId, "rejection");
-
-        try {
-            TransportShardBulkAction.executeBulkItemRequest(metaData, shard, bulkShardRequest,
-                    location, 0, updateHelper, threadPool::absoluteTimeInMillis,
-                    new ThrowingVerifyingMappingUpdatePerformer(err));
-            fail("should have thrown a retry exception");
-        } catch (ReplicationOperation.RetryOnPrimaryException e) {
-            assertThat(e, equalTo(err));
-        }
+        expectThrows(ReplicationOperation.RetryOnPrimaryException.class,
+            () -> TransportShardBulkAction.executeBulkItemRequest(metaData, shard, bulkShardRequest,
+                location, 0, updateHelper, threadPool::absoluteTimeInMillis,
+                new NoopMappingUpdatePerformer()));
 
         closeShards(shard);
     }
 
-    public void testExecuteBulkIndexRequestWithConflictingMappings() throws Exception {
+    public void testExecuteBulkIndexRequestWithErrorWhileUpdatingMapping() throws Exception {
         IndexMetaData metaData = indexMetaData();
         IndexShard shard = newStartedShard(true);
 
         BulkItemRequest[] items = new BulkItemRequest[1];
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id")
                 .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar");
         items[0] = new BulkItemRequest(0, writeRequest);
         BulkShardRequest bulkShardRequest =
@@ -300,8 +290,8 @@ public void testExecuteBulkIndexRequestWithConflictingMappings() throws Exceptio
         Translog.Location location = new Translog.Location(0, 0, 0);
         UpdateHelper updateHelper = null;
 
-        // Return a mapping conflict (IAE) when trying to update the mapping
-        RuntimeException err = new IllegalArgumentException("mapping conflict");
+        // Return an exception when trying to update the mapping
+        RuntimeException err = new RuntimeException("some kind of exception");
 
         Translog.Location newLocation = TransportShardBulkAction.executeBulkItemRequest(metaData,
                 shard, bulkShardRequest, location, 0, updateHelper,
@@ -318,13 +308,12 @@ public void testExecuteBulkIndexRequestWithConflictingMappings() throws Exceptio
         assertThat(primaryResponse.getId(), equalTo("id"));
         assertThat(primaryResponse.getOpType(), equalTo(DocWriteRequest.OpType.INDEX));
         assertTrue(primaryResponse.isFailed());
-        assertThat(primaryResponse.getFailureMessage(), containsString("mapping conflict"));
+        assertThat(primaryResponse.getFailureMessage(), containsString("some kind of exception"));
         BulkItemResponse.Failure failure = primaryResponse.getFailure();
         assertThat(failure.getIndex(), equalTo("index"));
-        assertThat(failure.getType(), equalTo("type"));
+        assertThat(failure.getType(), equalTo("_doc"));
         assertThat(failure.getId(), equalTo("id"));
         assertThat(failure.getCause(), equalTo(err));
-        assertThat(failure.getStatus(), equalTo(RestStatus.BAD_REQUEST));
 
         closeShards(shard);
     }
@@ -334,7 +323,7 @@ public void testExecuteBulkDeleteRequest() throws Exception {
         IndexShard shard = newStartedShard(true);
 
         BulkItemRequest[] items = new BulkItemRequest[1];
-        DocWriteRequest writeRequest = new DeleteRequest("index", "type", "id");
+        DocWriteRequest writeRequest = new DeleteRequest("index", "_doc", "id");
         items[0] = new BulkItemRequest(0, writeRequest);
         BulkShardRequest bulkShardRequest =
                 new BulkShardRequest(shardId, RefreshPolicy.NONE, items);
@@ -366,16 +355,16 @@ public void testExecuteBulkDeleteRequest() throws Exception {
         assertThat(response.getResult(), equalTo(DocWriteResponse.Result.NOT_FOUND));
         assertThat(response.getShardId(), equalTo(shard.shardId()));
         assertThat(response.getIndex(), equalTo("index"));
-        assertThat(response.getType(), equalTo("type"));
+        assertThat(response.getType(), equalTo("_doc"));
         assertThat(response.getId(), equalTo("id"));
         assertThat(response.getVersion(), equalTo(1L));
         assertThat(response.getSeqNo(), equalTo(0L));
         assertThat(response.forcedRefresh(), equalTo(false));
 
         // Now do the same after indexing the document, it should now find and delete the document
-        indexDoc(shard, "type", "id", "{\"foo\": \"bar\"}");
+        indexDoc(shard, "_doc", "id", "{}");
 
-        writeRequest = new DeleteRequest("index", "type", "id");
+        writeRequest = new DeleteRequest("index", "_doc", "id");
         items[0] = new BulkItemRequest(0, writeRequest);
         bulkShardRequest = new BulkShardRequest(shardId, RefreshPolicy.NONE, items);
 
@@ -405,7 +394,7 @@ public void testExecuteBulkDeleteRequest() throws Exception {
         assertThat(response.getResult(), equalTo(DocWriteResponse.Result.DELETED));
         assertThat(response.getShardId(), equalTo(shard.shardId()));
         assertThat(response.getIndex(), equalTo("index"));
-        assertThat(response.getType(), equalTo("type"));
+        assertThat(response.getType(), equalTo("_doc"));
         assertThat(response.getId(), equalTo("id"));
         assertThat(response.getVersion(), equalTo(3L));
         assertThat(response.getSeqNo(), equalTo(2L));
@@ -416,11 +405,11 @@ public void testExecuteBulkDeleteRequest() throws Exception {
     }
 
     public void testNoopUpdateReplicaRequest() throws Exception {
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id")
                 .source(Requests.INDEX_CONTENT_TYPE, "field", "value");
         BulkItemRequest replicaRequest = new BulkItemRequest(0, writeRequest);
 
-        DocWriteResponse noopUpdateResponse = new UpdateResponse(shardId, "index", "id", 0,
+        DocWriteResponse noopUpdateResponse = new UpdateResponse(shardId, "_doc", "id", 0,
                 DocWriteResponse.Result.NOOP);
         BulkItemResultHolder noopResults = new BulkItemResultHolder(noopUpdateResponse, null,
                 replicaRequest);
@@ -447,8 +436,7 @@ public void testNoopUpdateReplicaRequest() throws Exception {
     }
 
     public void testUpdateReplicaRequestWithFailure() throws Exception {
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
-                .source(Requests.INDEX_CONTENT_TYPE, "field", "value");
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id").source(Requests.INDEX_CONTENT_TYPE);
         BulkItemRequest replicaRequest = new BulkItemRequest(0, writeRequest);
 
         Exception err = new ElasticsearchException("I'm dead <(x.x)>");
@@ -477,18 +465,17 @@ public void testUpdateReplicaRequestWithFailure() throws Exception {
         assertThat(primaryResponse.getFailureMessage(), containsString("I'm dead <(x.x)>"));
         BulkItemResponse.Failure failure = primaryResponse.getFailure();
         assertThat(failure.getIndex(), equalTo("index"));
-        assertThat(failure.getType(), equalTo("type"));
+        assertThat(failure.getType(), equalTo("_doc"));
         assertThat(failure.getId(), equalTo("id"));
         assertThat(failure.getCause(), equalTo(err));
         assertThat(failure.getStatus(), equalTo(RestStatus.INTERNAL_SERVER_ERROR));
     }
 
     public void testUpdateReplicaRequestWithConflictFailure() throws Exception {
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
-                .source(Requests.INDEX_CONTENT_TYPE, "field", "value");
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id").source(Requests.INDEX_CONTENT_TYPE);
         BulkItemRequest replicaRequest = new BulkItemRequest(0, writeRequest);
 
-        Exception err = new VersionConflictEngineException(shardId, "type", "id",
+        Exception err = new VersionConflictEngineException(shardId, "_doc", "id",
                 "I'm conflicted <(;_;)>");
         Engine.IndexResult indexResult = new Engine.IndexResult(err, 0, 0);
         BulkItemResultHolder failedResults = new BulkItemResultHolder(null, indexResult,
@@ -515,21 +502,21 @@ public void testUpdateReplicaRequestWithConflictFailure() throws Exception {
         assertThat(primaryResponse.getFailureMessage(), containsString("I'm conflicted <(;_;)>"));
         BulkItemResponse.Failure failure = primaryResponse.getFailure();
         assertThat(failure.getIndex(), equalTo("index"));
-        assertThat(failure.getType(), equalTo("type"));
+        assertThat(failure.getType(), equalTo("_doc"));
         assertThat(failure.getId(), equalTo("id"));
         assertThat(failure.getCause(), equalTo(err));
         assertThat(failure.getStatus(), equalTo(RestStatus.CONFLICT));
     }
 
     public void testUpdateReplicaRequestWithSuccess() throws Exception {
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
-                .source(Requests.INDEX_CONTENT_TYPE, "field", "value");
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id")
+                .source(Requests.INDEX_CONTENT_TYPE);
         BulkItemRequest replicaRequest = new BulkItemRequest(0, writeRequest);
 
         boolean created = randomBoolean();
         Translog.Location resultLocation = new Translog.Location(42, 42, 42);
         Engine.IndexResult indexResult = new FakeResult(1, 1, created, resultLocation);
-        DocWriteResponse indexResponse = new IndexResponse(shardId, "index", "id", 1, 17, 1, created);
+        DocWriteResponse indexResponse = new IndexResponse(shardId, "_doc", "id", 1, 17, 1, created);
         BulkItemResultHolder goodResults =
                 new BulkItemResultHolder(indexResponse, indexResult, replicaRequest);
 
@@ -558,8 +545,8 @@ public void testUpdateReplicaRequestWithSuccess() throws Exception {
     public void testCalculateTranslogLocation() throws Exception {
         final Translog.Location original = new Translog.Location(0, 0, 0);
 
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
-                .source(Requests.INDEX_CONTENT_TYPE, "field", "value");
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id")
+            .source(Requests.INDEX_CONTENT_TYPE);
         BulkItemRequest replicaRequest = new BulkItemRequest(0, writeRequest);
         BulkItemResultHolder results = new BulkItemResultHolder(null, null, replicaRequest);
 
@@ -567,7 +554,7 @@ public void testCalculateTranslogLocation() throws Exception {
                 equalTo(original));
 
         boolean created = randomBoolean();
-        DocWriteResponse indexResponse = new IndexResponse(shardId, "index", "id", 1, 17, 1, created);
+        DocWriteResponse indexResponse = new IndexResponse(shardId, "_doc", "id", 1, 17, 1, created);
         Translog.Location newLocation = new Translog.Location(1, 1, 1);
         final long version = randomNonNegativeLong();
         final long seqNo = randomNonNegativeLong();
@@ -580,10 +567,7 @@ public void testCalculateTranslogLocation() throws Exception {
 
     public void testNoOpReplicationOnPrimaryDocumentFailure() throws Exception {
         final IndexShard shard = spy(newStartedShard(false));
-        BulkItemRequest itemRequest = new BulkItemRequest(0,
-                new IndexRequest("index", "type")
-                        .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar")
-        );
+        BulkItemRequest itemRequest = new BulkItemRequest(0, new IndexRequest("index", "_doc").source(Requests.INDEX_CONTENT_TYPE));
         final String failureMessage = "simulated primary failure";
         final IOException exception = new IOException(failureMessage);
         itemRequest.setPrimaryResponse(new BulkItemResponse(0,
@@ -592,7 +576,7 @@ public void testNoOpReplicationOnPrimaryDocumentFailure() throws Exception {
                         DocWriteRequest.OpType.DELETE,
                         DocWriteRequest.OpType.INDEX
                 ),
-                new BulkItemResponse.Failure("index", "type", "1",
+                new BulkItemResponse.Failure("index", "_doc", "1",
                     exception, 1L)
         ));
         BulkItemRequest[] itemRequests = new BulkItemRequest[1];
@@ -609,33 +593,23 @@ public void testMappingUpdateParsesCorrectNumberOfTimes() throws Exception {
         logger.info("--> metadata.getIndex(): {}", metaData.getIndex());
         final IndexShard shard = spy(newStartedShard(true));
 
-        IndexRequest request = new IndexRequest("index", "type", "id")
+        IndexRequest request = new IndexRequest("index", "_doc", "id")
                 .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar");
 
         final AtomicInteger updateCalled = new AtomicInteger(0);
-        final AtomicInteger verifyCalled = new AtomicInteger(0);
-        TransportShardBulkAction.executeIndexRequestOnPrimary(request, shard,
-                new MappingUpdatePerformer() {
-                    @Override
-                    public void updateMappings(Mapping update, ShardId shardId, String type) {
-                        // There should indeed be a mapping update
-                        assertNotNull(update);
-                        updateCalled.incrementAndGet();
-                    }
-
-                    @Override
-                    public void verifyMappings(Mapping update, ShardId shardId) {
-                        // No-op, will be called
-                        logger.info("--> verifying mappings noop");
-                        verifyCalled.incrementAndGet();
-                    }
-        });
+        expectThrows(ReplicationOperation.RetryOnPrimaryException.class,
+            () -> TransportShardBulkAction.executeIndexRequestOnPrimary(request, shard,
+                (update, shardId, type) -> {
+                    // There should indeed be a mapping update
+                    assertNotNull(update);
+                    updateCalled.incrementAndGet();
+                }));
 
         assertThat("mappings were \"updated\" once", updateCalled.get(), equalTo(1));
-        assertThat("mappings were \"verified\" once", verifyCalled.get(), equalTo(1));
+
 
         // Verify that the shard "executed" the operation twice
-        verify(shard, times(2)).applyIndexOperationOnPrimary(anyLong(), any(), any(), anyLong(), anyBoolean(), any());
+        verify(shard, times(2)).applyIndexOperationOnPrimary(anyLong(), any(), any(), anyLong(), anyBoolean());
 
         // Update the mapping, so the next mapping updater doesn't do anything
         final MapperService mapperService = shard.mapperService();
@@ -643,21 +617,11 @@ public void verifyMappings(Mapping update, ShardId shardId) {
         mapperService.updateMapping(metaData);
 
         TransportShardBulkAction.executeIndexRequestOnPrimary(request, shard,
-                new MappingUpdatePerformer() {
-                    @Override
-                    public void updateMappings(Mapping update, ShardId shardId, String type) {
-                        fail("should not have had to update the mappings");
-                    }
-
-                    @Override
-                    public void verifyMappings(Mapping update, ShardId shardId) {
-                        fail("should not have had to update the mappings");
-                    }
-        });
+            (update, shardId, type) -> fail("should not have had to update the mappings"));
 
         // Verify that the shard "executed" the operation only once (2 for previous invocations plus
         // 1 for this execution)
-        verify(shard, times(3)).applyIndexOperationOnPrimary(anyLong(), any(), any(), anyLong(), anyBoolean(), any());
+        verify(shard, times(3)).applyIndexOperationOnPrimary(anyLong(), any(), any(), anyLong(), anyBoolean());
 
         closeShards(shard);
     }
@@ -678,16 +642,16 @@ public IndexResultWithLocation(long version, long seqNo, boolean created, Transl
     public void testProcessUpdateResponse() throws Exception {
         IndexShard shard = newStartedShard(false);
 
-        UpdateRequest updateRequest = new UpdateRequest("index", "type", "id");
+        UpdateRequest updateRequest = new UpdateRequest("index", "_doc", "id");
         BulkItemRequest request = new BulkItemRequest(0, updateRequest);
-        Exception err = new VersionConflictEngineException(shardId, "type", "id",
+        Exception err = new VersionConflictEngineException(shardId, "_doc", "id",
                 "I'm conflicted <(;_;)>");
         Engine.IndexResult indexResult = new Engine.IndexResult(err, 0, 0);
         Engine.DeleteResult deleteResult = new Engine.DeleteResult(1, 1, true);
         DocWriteResponse.Result docWriteResult = DocWriteResponse.Result.CREATED;
         DocWriteResponse.Result deleteWriteResult = DocWriteResponse.Result.DELETED;
-        IndexRequest indexRequest = new IndexRequest("index", "type", "id");
-        DeleteRequest deleteRequest = new DeleteRequest("index", "type", "id");
+        IndexRequest indexRequest = new IndexRequest("index", "_doc", "id");
+        DeleteRequest deleteRequest = new DeleteRequest("index", "_doc", "id");
         UpdateHelper.Result translate = new UpdateHelper.Result(indexRequest, docWriteResult,
                 new HashMap<String, Object>(), XContentType.JSON);
         UpdateHelper.Result translateDelete = new UpdateHelper.Result(deleteRequest, deleteWriteResult,
@@ -733,30 +697,28 @@ public void testExecuteUpdateRequestOnce() throws Exception {
         IndexShard shard = newStartedShard(true);
 
         Map<String, Object> source = new HashMap<>();
-        source.put("foo", "bar");
         BulkItemRequest[] items = new BulkItemRequest[1];
         boolean create = randomBoolean();
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
-                .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar")
-                .create(create);
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id").source(Requests.INDEX_CONTENT_TYPE).create(create);
         BulkItemRequest primaryRequest = new BulkItemRequest(0, writeRequest);
         items[0] = primaryRequest;
         BulkShardRequest bulkShardRequest =
                 new BulkShardRequest(shardId, RefreshPolicy.NONE, items);
 
         Translog.Location location = new Translog.Location(0, 0, 0);
-        IndexRequest indexRequest = new IndexRequest("index", "type", "id");
+        IndexRequest indexRequest = new IndexRequest("index", "_doc", "id");
         indexRequest.source(source);
 
         DocWriteResponse.Result docWriteResult = DocWriteResponse.Result.CREATED;
         UpdateHelper.Result translate = new UpdateHelper.Result(indexRequest, docWriteResult,
                 new HashMap<String, Object>(), XContentType.JSON);
         UpdateHelper updateHelper = new MockUpdateHelper(translate);
-        UpdateRequest updateRequest = new UpdateRequest("index", "type", "id");
+        UpdateRequest updateRequest = new UpdateRequest("index", "_doc", "id");
         updateRequest.upsert(source);
 
         BulkItemResultHolder holder = TransportShardBulkAction.executeUpdateRequestOnce(updateRequest, shard, metaData,
-                "index", updateHelper, threadPool::absoluteTimeInMillis, primaryRequest, 0, new NoopMappingUpdatePerformer());
+                "index", updateHelper, threadPool::absoluteTimeInMillis, primaryRequest, 0,
+            new ThrowingMappingUpdatePerformer(new RuntimeException()));
 
         assertFalse(holder.isVersionConflict());
         assertNotNull(holder.response);
@@ -785,7 +747,7 @@ public void testExecuteUpdateRequestOnceWithFailure() throws Exception {
         source.put("foo", "bar");
         BulkItemRequest[] items = new BulkItemRequest[1];
         boolean create = randomBoolean();
-        DocWriteRequest writeRequest = new IndexRequest("index", "type", "id")
+        DocWriteRequest writeRequest = new IndexRequest("index", "_doc", "id")
                 .source(Requests.INDEX_CONTENT_TYPE, "foo", "bar")
                 .create(create);
         BulkItemRequest primaryRequest = new BulkItemRequest(0, writeRequest);
@@ -794,13 +756,13 @@ public void testExecuteUpdateRequestOnceWithFailure() throws Exception {
                 new BulkShardRequest(shardId, RefreshPolicy.NONE, items);
 
         Translog.Location location = new Translog.Location(0, 0, 0);
-        IndexRequest indexRequest = new IndexRequest("index", "type", "id");
+        IndexRequest indexRequest = new IndexRequest("index", "_doc", "id");
         indexRequest.source(source);
 
         DocWriteResponse.Result docWriteResult = DocWriteResponse.Result.CREATED;
         Exception prepareFailure = new IllegalArgumentException("I failed to do something!");
         UpdateHelper updateHelper = new FailingUpdateHelper(prepareFailure);
-        UpdateRequest updateRequest = new UpdateRequest("index", "type", "id");
+        UpdateRequest updateRequest = new UpdateRequest("index", "_doc", "id");
         updateRequest.upsert(source);
 
         BulkItemResultHolder holder = TransportShardBulkAction.executeUpdateRequestOnce(updateRequest, shard, metaData,
@@ -812,7 +774,7 @@ public void testExecuteUpdateRequestOnceWithFailure() throws Exception {
         assertNotNull(holder.replicaRequest);
 
         Engine.IndexResult opResult = (Engine.IndexResult) holder.operationResult;
-        assertTrue(opResult.hasFailure());
+        assertThat(opResult.getResultType(), equalTo(Engine.Result.Type.FAILURE));
         assertFalse(opResult.isCreated());
         Exception e = opResult.getFailure();
         assertThat(e.getMessage(), containsString("I failed to do something!"));
@@ -822,7 +784,7 @@ public void testExecuteUpdateRequestOnceWithFailure() throws Exception {
         assertThat(replicaBulkRequest.request(), instanceOf(IndexRequest.class));
         IndexRequest replicaRequest = (IndexRequest) replicaBulkRequest.request();
         assertThat(replicaRequest.index(), equalTo("index"));
-        assertThat(replicaRequest.type(), equalTo("type"));
+        assertThat(replicaRequest.type(), equalTo("_doc"));
         assertThat(replicaRequest.id(), equalTo("id"));
         assertThat(replicaRequest.sourceAsMap(), equalTo(source));
 
@@ -889,9 +851,6 @@ protected FakeResult(long version, long seqNo, boolean created, Translog.Locatio
     public static class NoopMappingUpdatePerformer implements MappingUpdatePerformer {
         public void updateMappings(Mapping update, ShardId shardId, String type) {
         }
-
-        public void verifyMappings(Mapping update, ShardId shardId) {
-        }
     }
 
     /** Always throw the given exception */
@@ -904,24 +863,5 @@ public void verifyMappings(Mapping update, ShardId shardId) {
         public void updateMappings(Mapping update, ShardId shardId, String type) {
             throw e;
         }
-
-        public void verifyMappings(Mapping update, ShardId shardId) {
-            fail("should not have gotten to this point");
-        }
-    }
-
-    /** Always throw the given exception */
-    private class ThrowingVerifyingMappingUpdatePerformer implements MappingUpdatePerformer {
-        private final RuntimeException e;
-        ThrowingVerifyingMappingUpdatePerformer(RuntimeException e) {
-            this.e = e;
-        }
-
-        public void updateMappings(Mapping update, ShardId shardId, String type) {
-        }
-
-        public void verifyMappings(Mapping update, ShardId shardId) {
-            throw e;
-        }
     }
 }
diff --git a/server/src/test/java/org/elasticsearch/discovery/MasterDisruptionIT.java b/server/src/test/java/org/elasticsearch/discovery/MasterDisruptionIT.java
index 43e3b2ef01b6..f7716c6f146f 100644
--- a/server/src/test/java/org/elasticsearch/discovery/MasterDisruptionIT.java
+++ b/server/src/test/java/org/elasticsearch/discovery/MasterDisruptionIT.java
@@ -20,6 +20,10 @@
 package org.elasticsearch.discovery;
 
 import org.apache.logging.log4j.message.ParameterizedMessage;
+import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;
+import org.elasticsearch.action.admin.indices.stats.ShardStats;
+import org.elasticsearch.action.bulk.BulkRequestBuilder;
+import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -30,14 +34,17 @@
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.discovery.zen.ElectMasterService;
 import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.monitor.jvm.HotThreads;
 import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.disruption.BlockMasterServiceOnMaster;
 import org.elasticsearch.test.disruption.IntermittentLongGCDisruption;
 import org.elasticsearch.test.disruption.LongGCDisruption;
 import org.elasticsearch.test.disruption.NetworkDisruption;
 import org.elasticsearch.test.disruption.NetworkDisruption.TwoPartitions;
+import org.elasticsearch.test.disruption.ServiceDisruptionScheme;
 import org.elasticsearch.test.disruption.SingleNodeDisruption;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 
@@ -449,6 +456,56 @@ public void testVerifyApiBlocksDuringPartition() throws Exception {
 
     }
 
+    @TestLogging(
+        "_root:DEBUG,"
+            + "org.elasticsearch.action.bulk:TRACE,"
+            + "org.elasticsearch.action.get:TRACE,"
+            + "org.elasticsearch.cluster.service:TRACE,"
+            + "org.elasticsearch.discovery:TRACE,"
+            + "org.elasticsearch.indices.cluster:TRACE,"
+            + "org.elasticsearch.indices.recovery:TRACE,"
+            + "org.elasticsearch.index.seqno:TRACE,"
+            + "org.elasticsearch.index.shard:TRACE")
+    public void testMappingTimeout() throws Exception {
+        startCluster(3);
+        createIndex("test", Settings.builder()
+            .put("index.number_of_shards", 1)
+            .put("index.number_of_replicas", 1)
+            .put("index.routing.allocation.exclude._name", internalCluster().getMasterName())
+        .build());
+
+        // create one field
+        index("test", "doc", "1", "{ \"f\": 1 }");
+
+        ensureGreen();
+
+        assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(
+            Settings.builder().put("indices.mapping.dynamic_timeout", "1ms")));
+
+        ServiceDisruptionScheme disruption = new BlockMasterServiceOnMaster(random());
+        setDisruptionScheme(disruption);
+
+        disruption.startDisrupting();
+
+        BulkRequestBuilder bulk = client().prepareBulk();
+        bulk.add(client().prepareIndex("test", "doc", "2").setSource("{ \"f\": 1 }", XContentType.JSON));
+        bulk.add(client().prepareIndex("test", "doc", "3").setSource("{ \"g\": 1 }", XContentType.JSON));
+        bulk.add(client().prepareIndex("test", "doc", "4").setSource("{ \"f\": 1 }", XContentType.JSON));
+        BulkResponse bulkResponse = bulk.get();
+        assertTrue(bulkResponse.hasFailures());
+
+        disruption.stopDisrupting();
+
+        assertBusy(() -> {
+            IndicesStatsResponse stats = client().admin().indices().prepareStats("test").clear().get();
+            for (ShardStats shardStats : stats.getShards()) {
+                assertThat(shardStats.getShardRouting().toString(),
+                    shardStats.getSeqNoStats().getGlobalCheckpoint(), equalTo(shardStats.getSeqNoStats().getLocalCheckpoint()));
+            }
+        });
+
+    }
+
     void assertDiscoveryCompleted(List<String> nodes) throws InterruptedException {
         for (final String node : nodes) {
             assertTrue(
diff --git a/server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java b/server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
index e769485443a0..e399c02d6cc8 100644
--- a/server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
+++ b/server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
@@ -1380,7 +1380,7 @@ public void testVersioningCreateExistsException() throws IOException {
 
         create = new Engine.Index(newUid(doc), doc, SequenceNumbers.UNASSIGNED_SEQ_NO, 0, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0, -1, false);
         indexResult = engine.index(create);
-        assertTrue(indexResult.hasFailure());
+        assertThat(indexResult.getResultType(), equalTo(Engine.Result.Type.FAILURE));
         assertThat(indexResult.getFailure(), instanceOf(VersionConflictEngineException.class));
     }
 
@@ -1478,7 +1478,7 @@ private void assertOpsOnReplica(List<Engine.Operation> ops, InternalEngine repli
                 // intentional
                 assertThat(result.isCreated(), equalTo(firstOp));
                 assertThat(result.getVersion(), equalTo(op.version()));
-                assertThat(result.hasFailure(), equalTo(false));
+                assertThat(result.getResultType(), equalTo(Engine.Result.Type.SUCCESS));
 
             } else {
                 Engine.DeleteResult result = replicaEngine.delete((Engine.Delete) op);
@@ -1489,7 +1489,7 @@ private void assertOpsOnReplica(List<Engine.Operation> ops, InternalEngine repli
                 // intentional
                 assertThat(result.isFound(), equalTo(firstOp == false));
                 assertThat(result.getVersion(), equalTo(op.version()));
-                assertThat(result.hasFailure(), equalTo(false));
+                assertThat(result.getResultType(), equalTo(Engine.Result.Type.SUCCESS));
             }
             if (randomBoolean()) {
                 engine.refresh("test");
@@ -1621,13 +1621,13 @@ private int assertOpsOnPrimary(List<Engine.Operation> ops, long currentOpVersion
                     Engine.IndexResult result = engine.index(indexWithVersion.apply(conflictingVersion, index));
                     assertThat(result.isCreated(), equalTo(false));
                     assertThat(result.getVersion(), equalTo(lastOpVersion));
-                    assertThat(result.hasFailure(), equalTo(true));
+                    assertThat(result.getResultType(), equalTo(Engine.Result.Type.FAILURE));
                     assertThat(result.getFailure(), instanceOf(VersionConflictEngineException.class));
                 } else {
                     Engine.IndexResult result = engine.index(versionedOp ? indexWithVersion.apply(correctVersion, index) : index);
                     assertThat(result.isCreated(), equalTo(docDeleted));
                     assertThat(result.getVersion(), equalTo(Math.max(lastOpVersion + 1, 1)));
-                    assertThat(result.hasFailure(), equalTo(false));
+                    assertThat(result.getResultType(), equalTo(Engine.Result.Type.SUCCESS));
                     assertThat(result.getFailure(), nullValue());
                     lastFieldValue = index.docs().get(0).get("value");
                     docDeleted = false;
@@ -1641,13 +1641,13 @@ private int assertOpsOnPrimary(List<Engine.Operation> ops, long currentOpVersion
                     Engine.DeleteResult result = engine.delete(delWithVersion.apply(conflictingVersion, delete));
                     assertThat(result.isFound(), equalTo(docDeleted == false));
                     assertThat(result.getVersion(), equalTo(lastOpVersion));
-                    assertThat(result.hasFailure(), equalTo(true));
+                    assertThat(result.getResultType(), equalTo(Engine.Result.Type.FAILURE));
                     assertThat(result.getFailure(), instanceOf(VersionConflictEngineException.class));
                 } else {
                     Engine.DeleteResult result = engine.delete(versionedOp ? delWithVersion.apply(correctVersion, delete) : delete);
                     assertThat(result.isFound(), equalTo(docDeleted == false));
                     assertThat(result.getVersion(), equalTo(Math.max(lastOpVersion + 1, 1)));
-                    assertThat(result.hasFailure(), equalTo(false));
+                    assertThat(result.getResultType(), equalTo(Engine.Result.Type.SUCCESS));
                     assertThat(result.getFailure(), nullValue());
                     docDeleted = true;
                     lastOpVersion = result.getVersion();
@@ -1725,14 +1725,14 @@ public void testNonInternalVersioningOnPrimary() throws IOException {
                     assertThat(result.getSeqNo(), equalTo(seqNo));
                     assertThat(result.isCreated(), equalTo(docDeleted));
                     assertThat(result.getVersion(), equalTo(op.version()));
-                    assertThat(result.hasFailure(), equalTo(false));
+                    assertThat(result.getResultType(), equalTo(Engine.Result.Type.SUCCESS));
                     assertThat(result.getFailure(), nullValue());
                     docDeleted = false;
                     highestOpVersion = op.version();
                 } else {
                     assertThat(result.isCreated(), equalTo(false));
                     assertThat(result.getVersion(), equalTo(highestOpVersion));
-                    assertThat(result.hasFailure(), equalTo(true));
+                    assertThat(result.getResultType(), equalTo(Engine.Result.Type.FAILURE));
                     assertThat(result.getFailure(), instanceOf(VersionConflictEngineException.class));
                 }
             } else {
@@ -1743,14 +1743,14 @@ public void testNonInternalVersioningOnPrimary() throws IOException {
                     assertThat(result.getSeqNo(), equalTo(seqNo));
                     assertThat(result.isFound(), equalTo(docDeleted == false));
                     assertThat(result.getVersion(), equalTo(op.version()));
-                    assertThat(result.hasFailure(), equalTo(false));
+                    assertThat(result.getResultType(), equalTo(Engine.Result.Type.SUCCESS));
                     assertThat(result.getFailure(), nullValue());
                     docDeleted = true;
                     highestOpVersion = op.version();
                 } else {
                     assertThat(result.isFound(), equalTo(docDeleted == false));
                     assertThat(result.getVersion(), equalTo(highestOpVersion));
-                    assertThat(result.hasFailure(), equalTo(true));
+                    assertThat(result.getResultType(), equalTo(Engine.Result.Type.FAILURE));
                     assertThat(result.getFailure(), instanceOf(VersionConflictEngineException.class));
                 }
             }
@@ -1863,7 +1863,7 @@ public void testConcurrentGetAndSetOnPrimary() throws IOException, InterruptedEx
                             get.version(), VersionType.INTERNAL,
                             PRIMARY, System.currentTimeMillis(), -1, false);
                         Engine.IndexResult indexResult = engine.index(index);
-                        if (indexResult.hasFailure() == false) {
+                        if (indexResult.getResultType() == Engine.Result.Type.SUCCESS) {
                             history.add(new OpAndVersion(indexResult.getVersion(), removed, added));
                         }
 
@@ -2005,7 +2005,7 @@ public void testSeqNoAndCheckpoints() throws IOException {
                         "test", id, newUid(id), SequenceNumbers.UNASSIGNED_SEQ_NO, 0,
                         rarely() ? 100 : Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
                     final Engine.DeleteResult result = initialEngine.delete(delete);
-                    if (!result.hasFailure()) {
+                    if (result.getResultType() == Engine.Result.Type.SUCCESS) {
                         assertThat(result.getSeqNo(), equalTo(primarySeqNo + 1));
                         assertThat(initialEngine.getLocalCheckpointTracker().getMaxSeqNo(), equalTo(primarySeqNo + 1));
                         indexedIds.remove(id);
@@ -2023,7 +2023,7 @@ public void testSeqNoAndCheckpoints() throws IOException {
                         rarely() ? 100 : Versions.MATCH_ANY, VersionType.INTERNAL,
                         PRIMARY, 0, -1, false);
                     final Engine.IndexResult result = initialEngine.index(index);
-                    if (!result.hasFailure()) {
+                    if (result.getResultType() == Engine.Result.Type.SUCCESS) {
                         assertThat(result.getSeqNo(), equalTo(primarySeqNo + 1));
                         assertThat(initialEngine.getLocalCheckpointTracker().getMaxSeqNo(), equalTo(primarySeqNo + 1));
                         indexedIds.add(id);
@@ -2297,7 +2297,7 @@ public void testEnableGcDeletes() throws Exception {
             // Try to index uid=1 with a too-old version, should fail:
             Engine.Index index = new Engine.Index(newUid(doc), doc, SequenceNumbers.UNASSIGNED_SEQ_NO, 0, 2, VersionType.EXTERNAL, Engine.Operation.Origin.PRIMARY, System.nanoTime(), -1, false);
             Engine.IndexResult indexResult = engine.index(index);
-            assertTrue(indexResult.hasFailure());
+            assertThat(indexResult.getResultType(), equalTo(Engine.Result.Type.FAILURE));
             assertThat(indexResult.getFailure(), instanceOf(VersionConflictEngineException.class));
 
             // Get should still not find the document
@@ -2307,7 +2307,7 @@ public void testEnableGcDeletes() throws Exception {
             // Try to index uid=2 with a too-old version, should fail:
             Engine.Index index1 = new Engine.Index(newUid(doc), doc, SequenceNumbers.UNASSIGNED_SEQ_NO, 0, 2, VersionType.EXTERNAL, Engine.Operation.Origin.PRIMARY, System.nanoTime(), -1, false);
             indexResult = engine.index(index1);
-            assertTrue(indexResult.hasFailure());
+            assertThat(indexResult.getResultType(), equalTo(Engine.Result.Type.FAILURE));
             assertThat(indexResult.getFailure(), instanceOf(VersionConflictEngineException.class));
 
             // Get should not find the document
@@ -3186,7 +3186,7 @@ public void testRetryWithAutogeneratedIdWorksAndNoDuplicateDocs() throws IOExcep
 
         index = new Engine.Index(newUid(doc), doc, indexResult.getSeqNo(), index.primaryTerm(), indexResult.getVersion(), index.versionType().versionTypeForReplicationAndRecovery(), REPLICA, System.nanoTime(), autoGeneratedIdTimestamp, isRetry);
         indexResult = replicaEngine.index(index);
-        assertThat(indexResult.hasFailure(), equalTo(false));
+        assertThat(indexResult.getResultType(), equalTo(Engine.Result.Type.SUCCESS));
         replicaEngine.refresh("test");
         try (Engine.Searcher searcher = replicaEngine.acquireSearcher("test")) {
             TopDocs topDocs = searcher.searcher().search(new MatchAllDocsQuery(), 10);
diff --git a/server/src/test/java/org/elasticsearch/index/mapper/TextFieldMapperTests.java b/server/src/test/java/org/elasticsearch/index/mapper/TextFieldMapperTests.java
index 459fcb1d3773..bd1d25eac570 100644
--- a/server/src/test/java/org/elasticsearch/index/mapper/TextFieldMapperTests.java
+++ b/server/src/test/java/org/elasticsearch/index/mapper/TextFieldMapperTests.java
@@ -236,7 +236,7 @@ public void testDefaultPositionIncrementGap() throws IOException {
 
         IndexShard shard = indexService.getShard(0);
         shard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL,
-            sourceToParse, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, update -> {});
+            sourceToParse, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false);
         shard.refresh("test");
         try (Engine.Searcher searcher = shard.acquireSearcher("test")) {
             LeafReader leaf = searcher.getDirectoryReader().leaves().get(0).reader();
@@ -278,7 +278,7 @@ public void testPositionIncrementGap() throws IOException {
 
         IndexShard shard = indexService.getShard(0);
         shard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL,
-            sourceToParse, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, update -> {});
+            sourceToParse, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false);
         shard.refresh("test");
         try (Engine.Searcher searcher = shard.acquireSearcher("test")) {
             LeafReader leaf = searcher.getDirectoryReader().leaves().get(0).reader();
diff --git a/server/src/test/java/org/elasticsearch/index/replication/IndexLevelReplicationTests.java b/server/src/test/java/org/elasticsearch/index/replication/IndexLevelReplicationTests.java
index 2d2aaac7bbd2..736dc40e6867 100644
--- a/server/src/test/java/org/elasticsearch/index/replication/IndexLevelReplicationTests.java
+++ b/server/src/test/java/org/elasticsearch/index/replication/IndexLevelReplicationTests.java
@@ -251,7 +251,7 @@ protected EngineFactory getEngineFactory(ShardRouting routing) {
             // test only primary
             shards.startPrimary();
             BulkItemResponse response = shards.index(
-                    new IndexRequest(index.getName(), "testDocumentFailureReplication", "1")
+                    new IndexRequest(index.getName(), "type", "1")
                             .source("{}", XContentType.JSON)
             );
             assertTrue(response.isFailed());
@@ -265,7 +265,7 @@ protected EngineFactory getEngineFactory(ShardRouting routing) {
             }
             shards.startReplicas(nReplica);
             response = shards.index(
-                    new IndexRequest(index.getName(), "testDocumentFailureReplication", "1")
+                    new IndexRequest(index.getName(), "type", "1")
                             .source("{}", XContentType.JSON)
             );
             assertTrue(response.isFailed());
@@ -281,7 +281,7 @@ public void testRequestFailureReplication() throws Exception {
         try (ReplicationGroup shards = createGroup(0)) {
             shards.startAll();
             BulkItemResponse response = shards.index(
-                    new IndexRequest(index.getName(), "testRequestFailureException", "1")
+                    new IndexRequest(index.getName(), "type", "1")
                             .source("{}", XContentType.JSON)
                             .version(2)
             );
@@ -300,7 +300,7 @@ public void testRequestFailureReplication() throws Exception {
             }
             shards.startReplicas(nReplica);
             response = shards.index(
-                    new IndexRequest(index.getName(), "testRequestFailureException", "1")
+                    new IndexRequest(index.getName(), "type", "1")
                             .source("{}", XContentType.JSON)
                             .version(2)
             );
diff --git a/server/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java b/server/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java
index 323b0364dfb9..a34963a47515 100644
--- a/server/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java
+++ b/server/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java
@@ -22,7 +22,6 @@
 import org.apache.logging.log4j.Logger;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexableField;
-import org.elasticsearch.core.internal.io.IOUtils;
 import org.elasticsearch.action.admin.indices.flush.FlushRequest;
 import org.elasticsearch.action.bulk.BulkShardRequest;
 import org.elasticsearch.action.index.IndexRequest;
@@ -33,6 +32,7 @@
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.core.internal.io.IOUtils;
 import org.elasticsearch.index.IndexSettings;
 import org.elasticsearch.index.VersionType;
 import org.elasticsearch.index.engine.Engine;
@@ -183,8 +183,7 @@ public void testRecoveryToReplicaThatReceivedExtraDocument() throws Exception {
                     VersionType.EXTERNAL,
                     randomNonNegativeLong(),
                     false,
-                    SourceToParse.source("index", "type", "replica", new BytesArray("{}"), XContentType.JSON),
-                    mapping -> {});
+                    SourceToParse.source("index", "type", "replica", new BytesArray("{}"), XContentType.JSON));
             shards.promoteReplicaToPrimary(promotedReplica).get();
             oldPrimary.close("demoted", randomBoolean());
             oldPrimary.store().close();
@@ -199,9 +198,7 @@ public void testRecoveryToReplicaThatReceivedExtraDocument() throws Exception {
                         VersionType.INTERNAL,
                         SourceToParse.source("index", "type", "primary", new BytesArray("{}"), XContentType.JSON),
                         randomNonNegativeLong(),
-                        false,
-                        mapping -> {
-                        });
+                        false);
             }
             final IndexShard recoveredReplica =
                     shards.addReplicaWithExistingPath(remainingReplica.shardPath(), remainingReplica.routingEntry().currentNodeId());
diff --git a/server/src/test/java/org/elasticsearch/index/shard/IndexShardIT.java b/server/src/test/java/org/elasticsearch/index/shard/IndexShardIT.java
index bc34aa60c492..d6d50b24d1f6 100644
--- a/server/src/test/java/org/elasticsearch/index/shard/IndexShardIT.java
+++ b/server/src/test/java/org/elasticsearch/index/shard/IndexShardIT.java
@@ -99,6 +99,7 @@
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
+import static org.elasticsearch.index.shard.IndexShardTestCase.getTranslog;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
@@ -107,7 +108,6 @@
 import static org.hamcrest.Matchers.containsString;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
-import static org.elasticsearch.index.shard.IndexShardTestCase.getTranslog;
 
 public class IndexShardIT extends ESSingleNodeTestCase {
 
@@ -342,7 +342,7 @@ public void testMaybeFlush() throws Exception {
         assertFalse(shard.shouldPeriodicallyFlush());
         shard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL,
             SourceToParse.source("test", "test", "1", new BytesArray("{}"), XContentType.JSON),
-            IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, update -> {});
+            IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false);
         assertTrue(shard.shouldPeriodicallyFlush());
         final Translog translog = getTranslog(shard);
         assertEquals(2, translog.stats().getUncommittedOperations());
@@ -379,7 +379,7 @@ public void testMaybeRollTranslogGeneration() throws Exception {
                         .put("index.number_of_shards", 1)
                         .put("index.translog.generation_threshold_size", generationThreshold + "b")
                         .build();
-        createIndex("test", settings);
+        createIndex("test", settings, "test");
         ensureGreen("test");
         final IndicesService indicesService = getInstanceFromNode(IndicesService.class);
         final IndexService test = indicesService.indexService(resolveIndex("test"));
@@ -392,7 +392,7 @@ public void testMaybeRollTranslogGeneration() throws Exception {
             assertThat(translog.currentFileGeneration(), equalTo(generation + rolls));
             final Engine.IndexResult result = shard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL,
                 SourceToParse.source("test", "test", "1", new BytesArray("{}"), XContentType.JSON),
-                IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, update -> {});
+                IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false);
             final Translog.Location location = result.getTranslogLocation();
             shard.afterWriteOperation();
             if (location.translogLocation + location.size > generationThreshold) {
diff --git a/server/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/server/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index e945bc12705b..31e51ed43d40 100644
--- a/server/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/server/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -84,7 +84,6 @@
 import org.elasticsearch.index.fielddata.IndexFieldDataService;
 import org.elasticsearch.index.mapper.IdFieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.Mapping;
 import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.seqno.SequenceNumbers;
@@ -1063,7 +1062,7 @@ public void onFailure(Exception e) {
      */
     public void testSnapshotStore() throws IOException {
         final IndexShard shard = newStartedShard(true);
-        indexDoc(shard, "test", "0");
+        indexDoc(shard, "_doc", "0");
         flushShard(shard);
 
         final IndexShard newShard = reinitShard(shard);
@@ -1139,9 +1138,9 @@ public void testMinimumCompatVersion() throws IOException {
         IndexShard test = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
         recoverShardFromStore(test);
 
-        indexDoc(test, "test", "test");
+        indexDoc(test, "_doc", "test");
         assertEquals(versionCreated.luceneVersion, test.minimumCompatibleVersion());
-        indexDoc(test, "test", "test");
+        indexDoc(test, "_doc", "test");
         assertEquals(versionCreated.luceneVersion, test.minimumCompatibleVersion());
         test.getEngine().flush();
         assertEquals(Version.CURRENT.luceneVersion, test.minimumCompatibleVersion());
@@ -1188,19 +1187,19 @@ public void testRefreshMetric() throws IOException {
         long initialTotalTime = shard.refreshStats().getTotalTimeInMillis();
         // check time advances
         for (int i = 1; shard.refreshStats().getTotalTimeInMillis() == initialTotalTime; i++) {
-            indexDoc(shard, "test", "test");
+            indexDoc(shard, "_doc", "test");
             assertThat(shard.refreshStats().getTotal(), equalTo(2L + i - 1));
             shard.refresh("test");
             assertThat(shard.refreshStats().getTotal(), equalTo(2L + i));
             assertThat(shard.refreshStats().getTotalTimeInMillis(), greaterThanOrEqualTo(initialTotalTime));
         }
         long refreshCount = shard.refreshStats().getTotal();
-        indexDoc(shard, "test", "test");
+        indexDoc(shard, "_doc", "test");
         try (Engine.GetResult ignored = shard.get(new Engine.Get(true, false, "test", "test",
             new Term(IdFieldMapper.NAME, Uid.encodeId("test"))))) {
             assertThat(shard.refreshStats().getTotal(), equalTo(refreshCount+1));
         }
-        indexDoc(shard, "test", "test");
+        indexDoc(shard, "_doc", "test");
         shard.writeIndexingBuffer();
         assertThat(shard.refreshStats().getTotal(), equalTo(refreshCount+2));
         closeShards(shard);
@@ -1208,7 +1207,7 @@ public void testRefreshMetric() throws IOException {
 
     public void testIndexingOperationsListeners() throws IOException {
         IndexShard shard = newStartedShard(true);
-        indexDoc(shard, "test", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(shard, "_doc", "0", "{\"foo\" : \"bar\"}");
         shard.updateLocalCheckpointForShard(shard.shardRouting.allocationId().getId(), 0);
         AtomicInteger preIndex = new AtomicInteger();
         AtomicInteger postIndexCreate = new AtomicInteger();
@@ -1227,14 +1226,19 @@ public void testIndexingOperationsListeners() throws IOException {
 
             @Override
             public void postIndex(ShardId shardId, Engine.Index index, Engine.IndexResult result) {
-                if (result.hasFailure() == false) {
-                    if (result.isCreated()) {
-                        postIndexCreate.incrementAndGet();
-                    } else {
-                        postIndexUpdate.incrementAndGet();
-                    }
-                } else {
-                    postIndex(shardId, index, result.getFailure());
+                switch (result.getResultType()) {
+                    case SUCCESS:
+                        if (result.isCreated()) {
+                            postIndexCreate.incrementAndGet();
+                        } else {
+                            postIndexUpdate.incrementAndGet();
+                        }
+                        break;
+                    case FAILURE:
+                        postIndex(shardId, index, result.getFailure());
+                        break;
+                    default:
+                        fail("unexpected result type:" + result.getResultType());
                 }
             }
 
@@ -1251,10 +1255,15 @@ public void postIndex(ShardId shardId, Engine.Index index, Exception ex) {
 
             @Override
             public void postDelete(ShardId shardId, Engine.Delete delete, Engine.DeleteResult result) {
-                if (result.hasFailure() == false) {
-                    postDelete.incrementAndGet();
-                } else {
-                    postDelete(shardId, delete, result.getFailure());
+                switch (result.getResultType()) {
+                    case SUCCESS:
+                        postDelete.incrementAndGet();
+                        break;
+                    case FAILURE:
+                        postDelete(shardId, delete, result.getFailure());
+                        break;
+                    default:
+                        fail("unexpected result type:" + result.getResultType());
                 }
             }
 
@@ -1266,7 +1275,7 @@ public void postDelete(ShardId shardId, Engine.Delete delete, Exception ex) {
         });
         recoverShardFromStore(shard);
 
-        indexDoc(shard, "test", "1");
+        indexDoc(shard, "_doc", "1");
         assertEquals(1, preIndex.get());
         assertEquals(1, postIndexCreate.get());
         assertEquals(0, postIndexUpdate.get());
@@ -1275,7 +1284,7 @@ public void postDelete(ShardId shardId, Engine.Delete delete, Exception ex) {
         assertEquals(0, postDelete.get());
         assertEquals(0, postDeleteException.get());
 
-        indexDoc(shard, "test", "1");
+        indexDoc(shard, "_doc", "1");
         assertEquals(2, preIndex.get());
         assertEquals(1, postIndexCreate.get());
         assertEquals(1, postIndexUpdate.get());
@@ -1284,7 +1293,7 @@ public void postDelete(ShardId shardId, Engine.Delete delete, Exception ex) {
         assertEquals(0, postDelete.get());
         assertEquals(0, postDeleteException.get());
 
-        deleteDoc(shard, "test", "1");
+        deleteDoc(shard, "_doc", "1");
 
         assertEquals(2, preIndex.get());
         assertEquals(1, postIndexCreate.get());
@@ -1298,7 +1307,7 @@ public void postDelete(ShardId shardId, Engine.Delete delete, Exception ex) {
         shard.state = IndexShardState.STARTED; // It will generate exception
 
         try {
-            indexDoc(shard, "test", "1");
+            indexDoc(shard, "_doc", "1");
             fail();
         } catch (AlreadyClosedException e) {
 
@@ -1312,7 +1321,7 @@ public void postDelete(ShardId shardId, Engine.Delete delete, Exception ex) {
         assertEquals(1, postDelete.get());
         assertEquals(0, postDeleteException.get());
         try {
-            deleteDoc(shard, "test", "1");
+            deleteDoc(shard, "_doc", "1");
             fail();
         } catch (AlreadyClosedException e) {
 
@@ -1458,7 +1467,7 @@ public void testRelocatedShardCanNotBeRevived() throws IOException, InterruptedE
         closeShards(shard);
     }
 
-    public void testShardCanNotBeMarkedAsRelocatedIfRelocationCancelled() throws IOException, InterruptedException {
+    public void testShardCanNotBeMarkedAsRelocatedIfRelocationCancelled() throws IOException {
         final IndexShard shard = newStartedShard(true);
         final ShardRouting originalRouting = shard.routingEntry();
         IndexShardTestCase.updateRoutingEntry(shard, ShardRoutingHelper.relocate(originalRouting, "other_node"));
@@ -1531,19 +1540,18 @@ public void testRecoverFromStoreWithOutOfOrderDelete() throws IOException {
          * - If flush and then recover from the existing store, delete #1 will be removed while index #0 is still retained and replayed.
          */
         final IndexShard shard = newStartedShard(false);
-        final Consumer<Mapping> mappingConsumer = getMappingUpdater(shard, "test");
-        shard.applyDeleteOperationOnReplica(1, 2, "test", "id", VersionType.EXTERNAL, mappingConsumer);
+        shard.applyDeleteOperationOnReplica(1, 2, "_doc", "id", VersionType.EXTERNAL);
         shard.getEngine().rollTranslogGeneration(); // isolate the delete in it's own generation
         shard.applyIndexOperationOnReplica(0, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-            SourceToParse.source(shard.shardId().getIndexName(), "test", "id", new BytesArray("{}"), XContentType.JSON), mappingConsumer);
+            SourceToParse.source(shard.shardId().getIndexName(), "_doc", "id", new BytesArray("{}"), XContentType.JSON));
         shard.applyIndexOperationOnReplica(3, 3, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-            SourceToParse.source(shard.shardId().getIndexName(), "test", "id-3", new BytesArray("{}"), XContentType.JSON), mappingConsumer);
+            SourceToParse.source(shard.shardId().getIndexName(), "_doc", "id-3", new BytesArray("{}"), XContentType.JSON));
         // Flushing a new commit with local checkpoint=1 allows to skip the translog gen #1 in recovery.
         shard.flush(new FlushRequest().force(true).waitIfOngoing(true));
         shard.applyIndexOperationOnReplica(2, 3, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-            SourceToParse.source(shard.shardId().getIndexName(), "test", "id-2", new BytesArray("{}"), XContentType.JSON), mappingConsumer);
+            SourceToParse.source(shard.shardId().getIndexName(), "_doc", "id-2", new BytesArray("{}"), XContentType.JSON));
         shard.applyIndexOperationOnReplica(5, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-            SourceToParse.source(shard.shardId().getIndexName(), "test", "id-5", new BytesArray("{}"), XContentType.JSON), mappingConsumer);
+            SourceToParse.source(shard.shardId().getIndexName(), "_doc", "id-5", new BytesArray("{}"), XContentType.JSON));
 
         final int translogOps;
         if (randomBoolean()) {
@@ -1580,7 +1588,7 @@ public void testRecoverFromStore() throws IOException {
         int totalOps = randomInt(10);
         int translogOps = totalOps;
         for (int i = 0; i < totalOps; i++) {
-            indexDoc(shard, "test", Integer.toString(i));
+            indexDoc(shard, "_doc", Integer.toString(i));
         }
         if (randomBoolean()) {
             shard.updateLocalCheckpointForShard(shard.shardRouting.allocationId().getId(), totalOps - 1);
@@ -1608,7 +1616,7 @@ public void testPrimaryHandOffUpdatesLocalCheckpoint() throws IOException {
         final IndexShard primarySource = newStartedShard(true);
         int totalOps = randomInt(10);
         for (int i = 0; i < totalOps; i++) {
-            indexDoc(primarySource, "test", Integer.toString(i));
+            indexDoc(primarySource, "_doc", Integer.toString(i));
         }
         IndexShardTestCase.updateRoutingEntry(primarySource, primarySource.routingEntry().relocate(randomAlphaOfLength(10), -1));
         final IndexShard primaryTarget = newShard(primarySource.routingEntry().getTargetRelocatingShard());
@@ -1626,15 +1634,15 @@ public void testPrimaryHandOffUpdatesLocalCheckpoint() throws IOException {
     /* This test just verifies that we fill up local checkpoint up to max seen seqID on primary recovery */
     public void testRecoverFromStoreWithNoOps() throws IOException {
         final IndexShard shard = newStartedShard(true);
-        indexDoc(shard, "test", "0");
-        Engine.IndexResult test = indexDoc(shard, "test", "1");
+        indexDoc(shard, "_doc", "0");
+        Engine.IndexResult test = indexDoc(shard, "_doc", "1");
         // start a replica shard and index the second doc
         final IndexShard otherShard = newStartedShard(false);
         updateMappings(otherShard, shard.indexSettings().getIndexMetaData());
-        SourceToParse sourceToParse = SourceToParse.source(shard.shardId().getIndexName(), "test", "1",
+        SourceToParse sourceToParse = SourceToParse.source(shard.shardId().getIndexName(), "_doc", "1",
             new BytesArray("{}"), XContentType.JSON);
         otherShard.applyIndexOperationOnReplica(1, 1,
-            VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse, update -> {});
+            VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse);
 
         final ShardRouting primaryShardRouting = shard.routingEntry();
         IndexShard newShard = reinitShard(otherShard, ShardRoutingHelper.initWithSameId(primaryShardRouting,
@@ -1676,7 +1684,7 @@ public void testRecoverFromStoreWithNoOps() throws IOException {
 
     public void testRecoverFromCleanStore() throws IOException {
         final IndexShard shard = newStartedShard(true);
-        indexDoc(shard, "test", "0");
+        indexDoc(shard, "_doc", "0");
         if (randomBoolean()) {
             flushShard(shard);
         }
@@ -1699,7 +1707,7 @@ public void testRecoverFromCleanStore() throws IOException {
 
     public void testFailIfIndexNotPresentInRecoverFromStore() throws Exception {
         final IndexShard shard = newStartedShard(true);
-        indexDoc(shard, "test", "0");
+        indexDoc(shard, "_doc", "0");
         if (randomBoolean()) {
             flushShard(shard);
         }
@@ -1739,7 +1747,7 @@ public void testFailIfIndexNotPresentInRecoverFromStore() throws Exception {
         assertDocCount(newShard, 0);
         // we can't issue this request through a client because of the inconsistencies we created with the cluster state
         // doing it directly instead
-        indexDoc(newShard, "test", "0");
+        indexDoc(newShard, "_doc", "0");
         newShard.refresh("test");
         assertDocCount(newShard, 1);
 
@@ -1749,21 +1757,20 @@ public void testFailIfIndexNotPresentInRecoverFromStore() throws Exception {
     public void testRecoverFromStoreRemoveStaleOperations() throws Exception {
         final IndexShard shard = newStartedShard(false);
         final String indexName = shard.shardId().getIndexName();
-        final Consumer<Mapping> mapping = getMappingUpdater(shard, "doc");
         // Index #0, index #1
         shard.applyIndexOperationOnReplica(0, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-            SourceToParse.source(indexName, "doc", "doc-0", new BytesArray("{}"), XContentType.JSON), mapping);
+            SourceToParse.source(indexName, "_doc", "doc-0", new BytesArray("{}"), XContentType.JSON));
         flushShard(shard);
         shard.updateGlobalCheckpointOnReplica(0, "test"); // stick the global checkpoint here.
         shard.applyIndexOperationOnReplica(1, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-            SourceToParse.source(indexName, "doc", "doc-1", new BytesArray("{}"), XContentType.JSON), mapping);
+            SourceToParse.source(indexName, "_doc", "doc-1", new BytesArray("{}"), XContentType.JSON));
         flushShard(shard);
         assertThat(getShardDocUIDs(shard), containsInAnyOrder("doc-0", "doc-1"));
         // Simulate resync (without rollback): Noop #1, index #2
         acquireReplicaOperationPermitBlockingly(shard, shard.primaryTerm + 1);
         shard.markSeqNoAsNoop(1, "test");
         shard.applyIndexOperationOnReplica(2, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-            SourceToParse.source(indexName, "doc", "doc-2", new BytesArray("{}"), XContentType.JSON), mapping);
+            SourceToParse.source(indexName, "_doc", "doc-2", new BytesArray("{}"), XContentType.JSON));
         flushShard(shard);
         assertThat(getShardDocUIDs(shard), containsInAnyOrder("doc-0", "doc-1", "doc-2"));
         // Recovering from store should discard doc #1
@@ -1800,11 +1807,11 @@ public void testRestoreShard() throws IOException {
         final IndexShard source = newStartedShard(true);
         IndexShard target = newStartedShard(true);
 
-        indexDoc(source, "test", "0");
+        indexDoc(source, "_doc", "0");
         if (randomBoolean()) {
             source.refresh("test");
         }
-        indexDoc(target, "test", "1");
+        indexDoc(target, "_doc", "1");
         target.refresh("test");
         assertDocs(target, "1");
         flushShard(source); // only flush source
@@ -1850,8 +1857,8 @@ public void restoreShard(IndexShard shard, SnapshotId snapshotId, Version versio
 
     public void testSearcherWrapperIsUsed() throws IOException {
         IndexShard shard = newStartedShard(true);
-        indexDoc(shard, "test", "0", "{\"foo\" : \"bar\"}");
-        indexDoc(shard, "test", "1", "{\"foobar\" : \"bar\"}");
+        indexDoc(shard, "_doc", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(shard, "_doc", "1", "{\"foobar\" : \"bar\"}");
         shard.refresh("test");
 
         Engine.GetResult getResult = shard.get(new Engine.Get(false, false, "test", "1", new Term(IdFieldMapper.NAME, Uid.encodeId("1"))));
@@ -1915,14 +1922,14 @@ public IndexSearcher wrap(IndexSearcher searcher) throws EngineException {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\", \"fielddata\": true }}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\", \"fielddata\": true }}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
         IndexShard shard = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, wrapper);
         recoverShardFromStore(shard);
-        indexDoc(shard, "test", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(shard, "_doc", "0", "{\"foo\" : \"bar\"}");
         shard.refresh("created segment 1");
-        indexDoc(shard, "test", "1", "{\"foobar\" : \"bar\"}");
+        indexDoc(shard, "_doc", "1", "{\"foobar\" : \"bar\"}");
         shard.refresh("created segment 2");
 
         // test global ordinals are evicted
@@ -1955,9 +1962,9 @@ public IndexSearcher wrap(IndexSearcher searcher) throws EngineException {
 
     public void testIndexingOperationListenersIsInvokedOnRecovery() throws IOException {
         IndexShard shard = newStartedShard(true);
-        indexDoc(shard, "test", "0", "{\"foo\" : \"bar\"}");
-        deleteDoc(shard, "test", "0");
-        indexDoc(shard, "test", "1", "{\"foo\" : \"bar\"}");
+        indexDoc(shard, "_doc", "0", "{\"foo\" : \"bar\"}");
+        deleteDoc(shard, "_doc", "0");
+        indexDoc(shard, "_doc", "1", "{\"foo\" : \"bar\"}");
         shard.refresh("test");
 
         final AtomicInteger preIndex = new AtomicInteger();
@@ -2007,7 +2014,7 @@ public void postDelete(ShardId shardId, Engine.Delete delete, Engine.DeleteResul
 
     public void testSearchIsReleaseIfWrapperFails() throws IOException {
         IndexShard shard = newStartedShard(true);
-        indexDoc(shard, "test", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(shard, "_doc", "0", "{\"foo\" : \"bar\"}");
         shard.refresh("test");
         IndexSearcherWrapper wrapper = new IndexSearcherWrapper() {
             @Override
@@ -2043,13 +2050,13 @@ public void testTranslogRecoverySyncsTranslog() throws IOException {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
         IndexShard primary = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
         recoverShardFromStore(primary);
 
-        indexDoc(primary, "test", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(primary, "_doc", "0", "{\"foo\" : \"bar\"}");
         IndexShard replica = newShard(primary.shardId(), false, "n2", metaData, null);
         recoverReplica(replica, primary, (shard, discoveryNode) ->
             new RecoveryTarget(shard, discoveryNode, recoveryListener, aLong -> {
@@ -2071,7 +2078,7 @@ public void testRecoverFromTranslog() throws IOException {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, randomLongBetween(1, Long.MAX_VALUE)).build();
         IndexShard primary = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
@@ -2080,11 +2087,11 @@ public void testRecoverFromTranslog() throws IOException {
         int numCorruptEntries = 0;
         for (int i = 0; i < numTotalEntries; i++) {
             if (randomBoolean()) {
-                operations.add(new Translog.Index("test", "1", 0, primary.getPrimaryTerm(), 1, VersionType.INTERNAL,
+                operations.add(new Translog.Index("_doc", "1", 0, primary.getPrimaryTerm(), 1, VersionType.INTERNAL,
                     "{\"foo\" : \"bar\"}".getBytes(Charset.forName("UTF-8")), null, -1));
             } else {
                 // corrupt entry
-                operations.add(new Translog.Index("test", "2", 1,  primary.getPrimaryTerm(), 1, VersionType.INTERNAL,
+                operations.add(new Translog.Index("_doc", "2", 1,  primary.getPrimaryTerm(), 1, VersionType.INTERNAL,
                     "{\"foo\" : \"bar}".getBytes(Charset.forName("UTF-8")), null, -1));
                 numCorruptEntries++;
             }
@@ -2124,7 +2131,7 @@ public int totalOperations() {
 
     public void testShardActiveDuringInternalRecovery() throws IOException {
         IndexShard shard = newStartedShard(true);
-        indexDoc(shard, "type", "0");
+        indexDoc(shard, "_doc", "0");
         shard = reinitShard(shard);
         DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT);
         shard.markAsRecovering("for testing", new RecoveryState(shard.routingEntry(), localNode, null));
@@ -2145,13 +2152,13 @@ public void testShardActiveDuringPeerRecovery() throws IOException {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
         IndexShard primary = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
         recoverShardFromStore(primary);
 
-        indexDoc(primary, "test", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(primary, "_doc", "0", "{\"foo\" : \"bar\"}");
         IndexShard replica = newShard(primary.shardId(), false, "n2", metaData, null);
         DiscoveryNode localNode = new DiscoveryNode("foo", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT);
         replica.markAsRecovering("for testing", new RecoveryState(replica.routingEntry(), localNode, localNode));
@@ -2178,13 +2185,13 @@ public void testRefreshListenersDuringPeerRecovery() throws IOException {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
         IndexShard primary = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
         recoverShardFromStore(primary);
 
-        indexDoc(primary, "test", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(primary, "_doc", "0", "{\"foo\" : \"bar\"}");
         Consumer<IndexShard> assertListenerCalled = shard -> {
             AtomicBoolean called = new AtomicBoolean();
             shard.addRefreshListener(null, b -> {
@@ -2230,15 +2237,15 @@ public void testRecoverFromLocalShard() throws IOException {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("source")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
 
         IndexShard sourceShard = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
         recoverShardFromStore(sourceShard);
 
-        indexDoc(sourceShard, "test", "0", "{\"foo\" : \"bar\"}");
-        indexDoc(sourceShard, "test", "1", "{\"foo\" : \"bar\"}");
+        indexDoc(sourceShard, "_doc", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(sourceShard, "_doc", "1", "{\"foo\" : \"bar\"}");
         sourceShard.refresh("test");
 
 
@@ -2290,8 +2297,8 @@ public void testRecoverFromLocalShard() throws IOException {
             closeShards(newShard);
         }
 
-        assertThat(requestedMappingUpdates, hasKey("test"));
-        assertThat(requestedMappingUpdates.get("test").get().source().string(), equalTo("{\"properties\":{\"foo\":{\"type\":\"text\"}}}"));
+        assertThat(requestedMappingUpdates, hasKey("_doc"));
+        assertThat(requestedMappingUpdates.get("_doc").get().source().string(), equalTo("{\"properties\":{\"foo\":{\"type\":\"text\"}}}"));
 
         closeShards(sourceShard, targetShard);
     }
@@ -2306,7 +2313,7 @@ public void testDocStats() throws IOException {
             final long numDocsToDelete = randomIntBetween((int) Math.ceil(Math.nextUp(numDocs / 10.0)), Math.toIntExact(numDocs));
             for (int i = 0; i < numDocs; i++) {
                 final String id = Integer.toString(i);
-                indexDoc(indexShard, "test", id);
+                indexDoc(indexShard, "_doc", id);
             }
             if (randomBoolean()) {
                 indexShard.refresh("test");
@@ -2328,8 +2335,8 @@ public void testDocStats() throws IOException {
                 IntStream.range(0, Math.toIntExact(numDocs)).boxed().collect(Collectors.toList()));
             for (final Integer i : ids) {
                 final String id = Integer.toString(i);
-                deleteDoc(indexShard, "test", id);
-                indexDoc(indexShard, "test", id);
+                deleteDoc(indexShard, "_doc", id);
+                indexDoc(indexShard, "_doc", id);
             }
 
             // flush the buffered deletes
@@ -2386,7 +2393,7 @@ public void testEstimateTotalDocSize() throws Exception {
                         .field("point", randomFloat())
                         .field("description", randomUnicodeOfCodepointLength(100))
                     .endObject());
-                indexDoc(indexShard, "doc", Integer.toString(i), doc);
+                indexDoc(indexShard, "_doc", Integer.toString(i), doc);
             }
 
             assertThat("Without flushing, segment sizes should be zero",
@@ -2413,7 +2420,7 @@ public void testEstimateTotalDocSize() throws Exception {
                 if (randomBoolean()) {
                     deleteDoc(indexShard, "doc", Integer.toString(i));
                 } else {
-                    indexDoc(indexShard, "doc", Integer.toString(i), "{\"foo\": \"bar\"}");
+                    indexDoc(indexShard, "_doc", Integer.toString(i), "{\"foo\": \"bar\"}");
                 }
             }
             if (randomBoolean()) {
@@ -2442,11 +2449,11 @@ public void testEstimateTotalDocSize() throws Exception {
      */
     public void testReadSnapshotConcurrently() throws IOException, InterruptedException {
         IndexShard indexShard = newStartedShard();
-        indexDoc(indexShard, "doc", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(indexShard, "_doc", "0", "{}");
         if (randomBoolean()) {
             indexShard.refresh("test");
         }
-        indexDoc(indexShard, "doc", "1", "{\"foo\" : \"bar\"}");
+        indexDoc(indexShard, "_doc", "1", "{}");
         indexShard.flush(new FlushRequest());
         closeShards(indexShard);
 
@@ -2490,7 +2497,7 @@ public void testReadSnapshotAndCheckIndexConcurrently() throws Exception {
         IndexShard indexShard = newStartedShard(isPrimary);
         final long numDocs = between(10, 100);
         for (long i = 0; i < numDocs; i++) {
-            indexDoc(indexShard, "doc", Long.toString(i), "{\"foo\" : \"bar\"}");
+            indexDoc(indexShard, "_doc", Long.toString(i), "{}");
             if (randomBoolean()) {
                 indexShard.refresh("test");
             }
@@ -2577,11 +2584,10 @@ private Result indexOnReplicaWithGaps(
         for (int i = offset + 1; i < operations; i++) {
             if (!rarely() || i == operations - 1) { // last operation can't be a gap as it's not a gap anymore
                 final String id = Integer.toString(i);
-                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "test", id,
+                SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), "_doc", id,
                         new BytesArray("{}"), XContentType.JSON);
                 indexShard.applyIndexOperationOnReplica(i,
-                        1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse,
-                        getMappingUpdater(indexShard, sourceToParse.type()));
+                    1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse);
                 if (!gap && i == localCheckpoint + 1) {
                     localCheckpoint++;
                 }
@@ -2702,12 +2708,12 @@ public void testIsSearchIdle() throws Exception {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
         IndexShard primary = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
         recoverShardFromStore(primary);
-        indexDoc(primary, "test", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(primary, "_doc", "0", "{\"foo\" : \"bar\"}");
         assertTrue(primary.getEngine().refreshNeeded());
         assertTrue(primary.scheduledRefresh());
         assertFalse(primary.isSearchIdle());
@@ -2740,12 +2746,12 @@ public void testScheduledRefresh() throws IOException, InterruptedException {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
         IndexShard primary = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
         recoverShardFromStore(primary);
-        indexDoc(primary, "test", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(primary, "_doc", "0", "{\"foo\" : \"bar\"}");
         assertTrue(primary.getEngine().refreshNeeded());
         assertTrue(primary.scheduledRefresh());
         IndexScopedSettings scopedSettings = primary.indexSettings().getScopedSettings();
@@ -2753,7 +2759,7 @@ public void testScheduledRefresh() throws IOException, InterruptedException {
         scopedSettings.applySettings(settings);
 
         assertFalse(primary.getEngine().refreshNeeded());
-        indexDoc(primary, "test", "1", "{\"foo\" : \"bar\"}");
+        indexDoc(primary, "_doc", "1", "{\"foo\" : \"bar\"}");
         assertTrue(primary.getEngine().refreshNeeded());
         long lastSearchAccess = primary.getLastSearcherAccess();
         assertFalse(primary.scheduledRefresh());
@@ -2792,7 +2798,7 @@ public void testScheduledRefresh() throws IOException, InterruptedException {
         });
         latch1.await();
 
-        indexDoc(primary, "test", "2", "{\"foo\" : \"bar\"}");
+        indexDoc(primary, "_doc", "2", "{\"foo\" : \"bar\"}");
         assertFalse(primary.scheduledRefresh());
         assertTrue(primary.isSearchIdle());
         primary.checkIdle(0);
@@ -2809,15 +2815,15 @@ public void testRefreshIsNeededWithRefreshListeners() throws IOException, Interr
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
         IndexShard primary = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
         recoverShardFromStore(primary);
-        indexDoc(primary, "test", "0", "{\"foo\" : \"bar\"}");
+        indexDoc(primary, "_doc", "0", "{\"foo\" : \"bar\"}");
         assertTrue(primary.getEngine().refreshNeeded());
         assertTrue(primary.scheduledRefresh());
-        Engine.IndexResult doc = indexDoc(primary, "test", "1", "{\"foo\" : \"bar\"}");
+        Engine.IndexResult doc = indexDoc(primary, "_doc", "1", "{\"foo\" : \"bar\"}");
         CountDownLatch latch = new CountDownLatch(1);
         primary.addRefreshListener(doc.getTranslogLocation(), r -> latch.countDown());
         assertEquals(1, latch.getCount());
@@ -2829,7 +2835,7 @@ public void testRefreshIsNeededWithRefreshListeners() throws IOException, Interr
         settings = Settings.builder().put(settings).put(IndexSettings.INDEX_SEARCH_IDLE_AFTER.getKey(), TimeValue.ZERO).build();
         scopedSettings.applySettings(settings);
 
-        doc = indexDoc(primary, "test", "2", "{\"foo\" : \"bar\"}");
+        doc = indexDoc(primary, "_doc", "2", "{\"foo\" : \"bar\"}");
         CountDownLatch latch1 = new CountDownLatch(1);
         primary.addRefreshListener(doc.getTranslogLocation(), r -> latch1.countDown());
         assertEquals(1, latch1.getCount());
@@ -2845,12 +2851,12 @@ public void testSegmentMemoryTrackedInBreaker() throws Exception {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
         IndexShard primary = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
         recoverShardFromStore(primary);
-        indexDoc(primary, "test", "0", "{\"foo\" : \"foo\"}");
+        indexDoc(primary, "_doc", "0", "{\"foo\" : \"foo\"}");
         primary.refresh("forced refresh");
 
         SegmentsStats ss = primary.segmentStats(randomBoolean());
@@ -2858,9 +2864,9 @@ public void testSegmentMemoryTrackedInBreaker() throws Exception {
         assertThat(ss.getMemoryInBytes(), equalTo(breaker.getUsed()));
         final long preRefreshBytes = ss.getMemoryInBytes();
 
-        indexDoc(primary, "test", "1", "{\"foo\" : \"bar\"}");
-        indexDoc(primary, "test", "2", "{\"foo\" : \"baz\"}");
-        indexDoc(primary, "test", "3", "{\"foo\" : \"eggplant\"}");
+        indexDoc(primary, "_doc", "1", "{\"foo\" : \"bar\"}");
+        indexDoc(primary, "_doc", "2", "{\"foo\" : \"baz\"}");
+        indexDoc(primary, "_doc", "3", "{\"foo\" : \"eggplant\"}");
 
         ss = primary.segmentStats(randomBoolean());
         breaker = primary.circuitBreakerService.getBreaker(CircuitBreaker.ACCOUNTING);
@@ -2873,7 +2879,7 @@ public void testSegmentMemoryTrackedInBreaker() throws Exception {
         assertThat(breaker.getUsed(), equalTo(ss.getMemoryInBytes()));
         assertThat(breaker.getUsed(), greaterThan(preRefreshBytes));
 
-        indexDoc(primary, "test", "4", "{\"foo\": \"potato\"}");
+        indexDoc(primary, "_doc", "4", "{\"foo\": \"potato\"}");
         // Forces a refresh with the INTERNAL scope
         ((InternalEngine) primary.getEngine()).writeIndexingBuffer();
 
@@ -2884,7 +2890,7 @@ public void testSegmentMemoryTrackedInBreaker() throws Exception {
         final long postRefreshBytes = ss.getMemoryInBytes();
 
         // Deleting a doc causes its memory to be freed from the breaker
-        deleteDoc(primary, "test", "0");
+        deleteDoc(primary, "_doc", "0");
         primary.refresh("force refresh");
 
         ss = primary.segmentStats(randomBoolean());
@@ -2903,7 +2909,7 @@ public void testSegmentMemoryTrackedWithRandomSearchers() throws Exception {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
         IndexShard primary = newShard(new ShardId(metaData.getIndex(), 0), true, "n1", metaData, null);
@@ -2923,13 +2929,13 @@ public void testSegmentMemoryTrackedWithRandomSearchers() throws Exception {
                         if (randomBoolean()) {
                             String id = "id-" + threadName + "-" + i;
                             logger.debug("--> {} indexing {}", threadName, id);
-                            indexDoc(primary, "test", id, "{\"foo\" : \"" + randomAlphaOfLength(10) + "\"}");
+                            indexDoc(primary, "_doc", id, "{\"foo\" : \"" + randomAlphaOfLength(10) + "\"}");
                         }
 
                         if (randomBoolean() && i > 10) {
                             String id = "id-" + threadName + "-" + randomIntBetween(0, i - 1);
                             logger.debug("--> {}, deleting {}", threadName, id);
-                            deleteDoc(primary, "test", id);
+                            deleteDoc(primary, "_doc", id);
                         }
 
                         if (randomBoolean()) {
@@ -2998,7 +3004,7 @@ public void testFlushOnInactive() throws Exception {
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .build();
         IndexMetaData metaData = IndexMetaData.builder("test")
-            .putMapping("test", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
+            .putMapping("_doc", "{ \"properties\": { \"foo\":  { \"type\": \"text\"}}}")
             .settings(settings)
             .primaryTerm(0, 1).build();
         ShardRouting shardRouting = TestShardRouting.newShardRouting(new ShardId(metaData.getIndex(), 0), "n1", true, ShardRoutingState
@@ -3019,7 +3025,7 @@ public void onShardInactive(IndexShard indexShard) {
         primaryRef.set(primary);
         recoverShardFromStore(primary);
         for (int i = 0; i < 3; i++) {
-            indexDoc(primary, "test", "" + i, "{\"foo\" : \"" + randomAlphaOfLength(10) + "\"}");
+            indexDoc(primary, "_doc", "" + i, "{\"foo\" : \"" + randomAlphaOfLength(10) + "\"}");
             primary.refresh("test"); // produce segments
         }
         List<Segment> segments = primary.segments(false);
diff --git a/server/src/test/java/org/elasticsearch/index/shard/IndexingOperationListenerTests.java b/server/src/test/java/org/elasticsearch/index/shard/IndexingOperationListenerTests.java
index 91e439dcda98..037b5d68fb86 100644
--- a/server/src/test/java/org/elasticsearch/index/shard/IndexingOperationListenerTests.java
+++ b/server/src/test/java/org/elasticsearch/index/shard/IndexingOperationListenerTests.java
@@ -57,10 +57,15 @@ public void testListenersAreExecuted() {
             @Override
             public void postIndex(ShardId shardId, Engine.Index index, Engine.IndexResult result) {
                 assertThat(shardId, is(randomShardId));
-                if (result.hasFailure() == false) {
-                    postIndex.incrementAndGet();
-                } else {
-                    postIndex(shardId, index, result.getFailure());
+                switch (result.getResultType()) {
+                    case SUCCESS:
+                        postIndex.incrementAndGet();
+                        break;
+                    case FAILURE:
+                        postIndex(shardId, index, result.getFailure());
+                        break;
+                    default:
+                        throw new IllegalArgumentException("unknown result type: " + result.getResultType());
                 }
             }
 
@@ -80,10 +85,15 @@ public void postIndex(ShardId shardId, Engine.Index index, Exception ex) {
             @Override
             public void postDelete(ShardId shardId, Engine.Delete delete, Engine.DeleteResult result) {
                 assertThat(shardId, is(randomShardId));
-                if (result.hasFailure() == false) {
-                    postDelete.incrementAndGet();
-                } else {
-                    postDelete(shardId, delete, result.getFailure());
+                switch (result.getResultType()) {
+                    case SUCCESS:
+                        postDelete.incrementAndGet();
+                        break;
+                    case FAILURE:
+                        postDelete(shardId, delete, result.getFailure());
+                        break;
+                    default:
+                        throw new IllegalArgumentException("unknown result type: " + result.getResultType());
                 }
             }
 
diff --git a/server/src/test/java/org/elasticsearch/index/shard/PrimaryReplicaSyncerTests.java b/server/src/test/java/org/elasticsearch/index/shard/PrimaryReplicaSyncerTests.java
index 12c3804a1a7b..1257aea3d14f 100644
--- a/server/src/test/java/org/elasticsearch/index/shard/PrimaryReplicaSyncerTests.java
+++ b/server/src/test/java/org/elasticsearch/index/shard/PrimaryReplicaSyncerTests.java
@@ -67,8 +67,8 @@ public void testSyncerSendsOffCorrectDocuments() throws Exception {
         for (int i = 0; i < numDocs; i++) {
             // Index doc but not advance local checkpoint.
             shard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL,
-                SourceToParse.source(shard.shardId().getIndexName(), "test", Integer.toString(i), new BytesArray("{}"), XContentType.JSON),
-                IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, getMappingUpdater(shard, "test"));
+                SourceToParse.source(shard.shardId().getIndexName(), "_doc", Integer.toString(i), new BytesArray("{}"), XContentType.JSON),
+                IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false);
         }
 
         long globalCheckPoint = numDocs > 0 ? randomIntBetween(0, numDocs - 1) : 0;
@@ -121,8 +121,8 @@ public void testSyncerOnClosingShard() throws Exception {
         for (int i = 0; i < numDocs; i++) {
             // Index doc but not advance local checkpoint.
             shard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL,
-                SourceToParse.source(shard.shardId().getIndexName(), "test", Integer.toString(i), new BytesArray("{}"), XContentType.JSON),
-                IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, getMappingUpdater(shard, "test"));
+                SourceToParse.source(shard.shardId().getIndexName(), "_doc", Integer.toString(i), new BytesArray("{}"), XContentType.JSON),
+                IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false);
         }
 
         String allocationId = shard.routingEntry().allocationId().getId();
diff --git a/server/src/test/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetServiceTests.java b/server/src/test/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetServiceTests.java
index 91b35594772c..3b50fa649150 100644
--- a/server/src/test/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetServiceTests.java
+++ b/server/src/test/java/org/elasticsearch/indices/recovery/PeerRecoveryTargetServiceTests.java
@@ -52,7 +52,7 @@ public void testGetStartingSeqNo() throws Exception {
             final long initDocs = scaledRandomIntBetween(1, 10);
             {
                 for (int i = 0; i < initDocs; i++) {
-                    indexDoc(replica, "doc", Integer.toString(i));
+                    indexDoc(replica, "_doc", Integer.toString(i));
                     if (randomBoolean()) {
                         flushShard(replica);
                     }
@@ -68,7 +68,7 @@ public void testGetStartingSeqNo() throws Exception {
             final int moreDocs = randomIntBetween(1, 10);
             {
                 for (int i = 0; i < moreDocs; i++) {
-                    indexDoc(replica, "doc", Long.toString(i));
+                    indexDoc(replica, "_doc", Long.toString(i));
                     if (randomBoolean()) {
                         flushShard(replica);
                     }
diff --git a/server/src/test/java/org/elasticsearch/indices/recovery/RecoveryTests.java b/server/src/test/java/org/elasticsearch/indices/recovery/RecoveryTests.java
index 4e9d0ccb22e1..537409f35d17 100644
--- a/server/src/test/java/org/elasticsearch/indices/recovery/RecoveryTests.java
+++ b/server/src/test/java/org/elasticsearch/indices/recovery/RecoveryTests.java
@@ -122,23 +122,23 @@ public void testRecoveryWithOutOfOrderDelete() throws Exception {
             final String indexName = orgReplica.shardId().getIndexName();
 
             // delete #1
-            orgReplica.applyDeleteOperationOnReplica(1, 2, "type", "id", VersionType.EXTERNAL, u -> {});
+            orgReplica.applyDeleteOperationOnReplica(1, 2, "type", "id", VersionType.EXTERNAL);
             getTranslog(orgReplica).rollGeneration(); // isolate the delete in it's own generation
             // index #0
             orgReplica.applyIndexOperationOnReplica(0, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-                SourceToParse.source(indexName, "type", "id", new BytesArray("{}"), XContentType.JSON), u -> {});
+                SourceToParse.source(indexName, "type", "id", new BytesArray("{}"), XContentType.JSON));
             // index #3
             orgReplica.applyIndexOperationOnReplica(3, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-                SourceToParse.source(indexName, "type", "id-3", new BytesArray("{}"), XContentType.JSON), u -> {});
+                SourceToParse.source(indexName, "type", "id-3", new BytesArray("{}"), XContentType.JSON));
             // Flushing a new commit with local checkpoint=1 allows to delete the translog gen #1.
             orgReplica.flush(new FlushRequest().force(true).waitIfOngoing(true));
             // index #2
             orgReplica.applyIndexOperationOnReplica(2, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-                SourceToParse.source(indexName, "type", "id-2", new BytesArray("{}"), XContentType.JSON), u -> {});
+                SourceToParse.source(indexName, "type", "id-2", new BytesArray("{}"), XContentType.JSON));
             orgReplica.updateGlobalCheckpointOnReplica(3L, "test");
             // index #5 -> force NoOp #4.
             orgReplica.applyIndexOperationOnReplica(5, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false,
-                SourceToParse.source(indexName, "type", "id-5", new BytesArray("{}"), XContentType.JSON), u -> {});
+                SourceToParse.source(indexName, "type", "id-5", new BytesArray("{}"), XContentType.JSON));
 
             final int translogOps;
             if (randomBoolean()) {
@@ -247,9 +247,11 @@ public void testPeerRecoverySendSafeCommitInFileBased() throws Exception {
         int numDocs = between(1, 100);
         long globalCheckpoint = 0;
         for (int i = 0; i < numDocs; i++) {
-            primaryShard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL,
-                SourceToParse.source(primaryShard.shardId().getIndexName(), "test", Integer.toString(i), new BytesArray("{}"),
-                    XContentType.JSON), IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, getMappingUpdater(primaryShard, "test"));
+            Engine.IndexResult result = primaryShard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL,
+                SourceToParse.source(primaryShard.shardId().getIndexName(), "_doc", Integer.toString(i), new BytesArray("{}"),
+                    XContentType.JSON),
+                IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false);
+            assertThat(result.getResultType(), equalTo(Engine.Result.Type.SUCCESS));
             if (randomBoolean()) {
                 globalCheckpoint = randomLongBetween(globalCheckpoint, i);
                 primaryShard.updateLocalCheckpointForShard(primaryShard.routingEntry().allocationId().getId(), globalCheckpoint);
diff --git a/server/src/test/java/org/elasticsearch/repositories/blobstore/BlobStoreRepositoryRestoreTests.java b/server/src/test/java/org/elasticsearch/repositories/blobstore/BlobStoreRepositoryRestoreTests.java
index 63236b165545..69ea552835b9 100644
--- a/server/src/test/java/org/elasticsearch/repositories/blobstore/BlobStoreRepositoryRestoreTests.java
+++ b/server/src/test/java/org/elasticsearch/repositories/blobstore/BlobStoreRepositoryRestoreTests.java
@@ -20,13 +20,13 @@
 package org.elasticsearch.repositories.blobstore;
 
 import org.apache.lucene.store.Directory;
-import org.elasticsearch.core.internal.io.IOUtils;
 import org.apache.lucene.util.TestUtil;
 import org.elasticsearch.cluster.metadata.RepositoryMetaData;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingHelper;
 import org.elasticsearch.common.UUIDs;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.core.internal.io.IOUtils;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.env.TestEnvironment;
 import org.elasticsearch.index.shard.IndexShard;
@@ -72,7 +72,7 @@ public void testRestoreSnapshotWithExistingFiles() throws IOException {
             final int numDocs = scaledRandomIntBetween(1, 500);
             recoverShardFromStore(shard);
             for (int i = 0; i < numDocs; i++) {
-                indexDoc(shard, "doc", Integer.toString(i));
+                indexDoc(shard, "_doc", Integer.toString(i));
                 if (rarely()) {
                     flushShard(shard, false);
                 }
@@ -138,7 +138,7 @@ public void testSnapshotWithConflictingName() throws IOException {
             final int numDocs = scaledRandomIntBetween(1, 500);
             recoverShardFromStore(shard);
             for (int i = 0; i < numDocs; i++) {
-                indexDoc(shard, "doc", Integer.toString(i));
+                indexDoc(shard, "_doc", Integer.toString(i));
                 if (rarely()) {
                     flushShard(shard, false);
                 }
diff --git a/test/framework/src/main/java/org/elasticsearch/index/shard/IndexShardTestCase.java b/test/framework/src/main/java/org/elasticsearch/index/shard/IndexShardTestCase.java
index a0e1cfc33411..74cc8cb9141d 100644
--- a/test/framework/src/main/java/org/elasticsearch/index/shard/IndexShardTestCase.java
+++ b/test/framework/src/main/java/org/elasticsearch/index/shard/IndexShardTestCase.java
@@ -25,10 +25,10 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.flush.FlushRequest;
 import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.support.replication.TransportReplicationAction;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
@@ -58,7 +58,6 @@
 import org.elasticsearch.index.engine.EngineTestCase;
 import org.elasticsearch.index.mapper.IdFieldMapper;
 import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.Mapping;
 import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.seqno.ReplicationTracker;
@@ -94,7 +93,6 @@
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.function.BiFunction;
-import java.util.function.Consumer;
 
 import static org.elasticsearch.cluster.routing.TestShardRouting.newShardRouting;
 import static org.hamcrest.Matchers.contains;
@@ -184,7 +182,8 @@ protected IndexShard newShard(
             .build();
         IndexMetaData.Builder metaData = IndexMetaData.builder(shardRouting.getIndexName())
             .settings(settings)
-            .primaryTerm(0, primaryTerm);
+            .primaryTerm(0, primaryTerm)
+            .putMapping("_doc", "{ \"properties\": {} }");
         return newShard(shardRouting, metaData.build(), listeners);
     }
 
@@ -559,27 +558,27 @@ protected void assertDocs(IndexShard shard, String... ids) throws IOException {
         throws IOException {
         SourceToParse sourceToParse = SourceToParse.source(shard.shardId().getIndexName(), type, id, new BytesArray(source), xContentType);
         sourceToParse.routing(routing);
+        Engine.IndexResult result;
         if (shard.routingEntry().primary()) {
-            final Engine.IndexResult result = shard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL, sourceToParse,
-                IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, getMappingUpdater(shard, type));
+            result = shard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL, sourceToParse,
+                IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false);
+            if (result.getResultType() == Engine.Result.Type.MAPPING_UPDATE_REQUIRED) {
+                updateMappings(shard, IndexMetaData.builder(shard.indexSettings().getIndexMetaData())
+                    .putMapping(type, result.getRequiredMappingUpdate().toString()).build());
+                result = shard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL, sourceToParse,
+                    IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false);
+            }
             shard.updateLocalCheckpointForShard(shard.routingEntry().allocationId().getId(),
                 shard.getEngine().getLocalCheckpointTracker().getCheckpoint());
-            return result;
         } else {
-            return shard.applyIndexOperationOnReplica(shard.seqNoStats().getMaxSeqNo() + 1, 0,
-                VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse, getMappingUpdater(shard, type));
-        }
-    }
-
-    protected Consumer<Mapping> getMappingUpdater(IndexShard shard, String type) {
-        return update -> {
-            try {
-                updateMappings(shard, IndexMetaData.builder(shard.indexSettings().getIndexMetaData())
-                    .putMapping(type, update.toString()).build());
-            } catch (IOException e) {
-                ExceptionsHelper.reThrowIfNotNull(e);
+            result = shard.applyIndexOperationOnReplica(shard.seqNoStats().getMaxSeqNo() + 1, 0,
+                VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse);
+            if (result.getResultType() == Engine.Result.Type.MAPPING_UPDATE_REQUIRED) {
+                throw new TransportReplicationAction.RetryOnReplicaException(shard.shardId,
+                    "Mappings are not available on the replica yet, triggered update: " + result.getRequiredMappingUpdate());
             }
-        };
+        }
+        return result;
     }
 
     protected void updateMappings(IndexShard shard, IndexMetaData indexMetadata) {
@@ -589,10 +588,9 @@ protected void updateMappings(IndexShard shard, IndexMetaData indexMetadata) {
 
     protected Engine.DeleteResult deleteDoc(IndexShard shard, String type, String id) throws IOException {
         if (shard.routingEntry().primary()) {
-            return shard.applyDeleteOperationOnPrimary(Versions.MATCH_ANY, type, id, VersionType.INTERNAL, update -> {});
+            return shard.applyDeleteOperationOnPrimary(Versions.MATCH_ANY, type, id, VersionType.INTERNAL);
         } else {
-            return shard.applyDeleteOperationOnReplica(shard.seqNoStats().getMaxSeqNo() + 1,
-                0L, type, id, VersionType.EXTERNAL, update -> {});
+            return shard.applyDeleteOperationOnReplica(shard.seqNoStats().getMaxSeqNo() + 1, 0L, type, id, VersionType.EXTERNAL);
         }
     }
 
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java b/test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
index d127f1a6b363..cd1aa6b020d3 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
@@ -19,7 +19,6 @@
 package org.elasticsearch.test;
 
 import com.carrotsearch.randomizedtesting.RandomizedContext;
-import org.elasticsearch.core.internal.io.IOUtils;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.admin.indices.get.GetIndexResponse;
@@ -38,6 +37,7 @@
 import org.elasticsearch.common.util.concurrent.EsExecutors;
 import org.elasticsearch.common.xcontent.NamedXContentRegistry;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.core.internal.io.IOUtils;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.index.Index;
@@ -252,7 +252,7 @@ protected IndexService createIndex(String index, Settings settings, String type,
      */
     protected IndexService createIndex(String index, Settings settings, String type, Object... mappings) {
         CreateIndexRequestBuilder createIndexRequestBuilder = client().admin().indices().prepareCreate(index).setSettings(settings);
-        if (type != null && mappings != null) {
+        if (type != null) {
             createIndexRequestBuilder.addMapping(type, mappings);
         }
         return createIndex(index, createIndexRequestBuilder);
diff --git a/test/framework/src/main/java/org/elasticsearch/test/disruption/BlockMasterServiceOnMaster.java b/test/framework/src/main/java/org/elasticsearch/test/disruption/BlockMasterServiceOnMaster.java
new file mode 100644
index 000000000000..0547ce70f2f9
--- /dev/null
+++ b/test/framework/src/main/java/org/elasticsearch/test/disruption/BlockMasterServiceOnMaster.java
@@ -0,0 +1,107 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.test.disruption;
+
+import org.apache.logging.log4j.core.util.Throwables;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
+import org.elasticsearch.cluster.service.ClusterService;
+import org.elasticsearch.common.Priority;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.test.InternalTestCluster;
+
+import java.util.Random;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicReference;
+
+public class BlockMasterServiceOnMaster extends SingleNodeDisruption {
+
+    AtomicReference<CountDownLatch> disruptionLatch = new AtomicReference<>();
+
+
+    public BlockMasterServiceOnMaster(Random random) {
+        super(random);
+    }
+
+
+    @Override
+    public void startDisrupting() {
+        disruptedNode = cluster.getMasterName();
+        final String disruptionNodeCopy = disruptedNode;
+        if (disruptionNodeCopy == null) {
+            return;
+        }
+        ClusterService clusterService = cluster.getInstance(ClusterService.class, disruptionNodeCopy);
+        if (clusterService == null) {
+            return;
+        }
+        logger.info("blocking master service on node [{}]", disruptionNodeCopy);
+        boolean success = disruptionLatch.compareAndSet(null, new CountDownLatch(1));
+        assert success : "startDisrupting called without waiting on stopDisrupting to complete";
+        final CountDownLatch started = new CountDownLatch(1);
+        clusterService.getMasterService().submitStateUpdateTask("service_disruption_block", new ClusterStateUpdateTask() {
+            @Override
+            public Priority priority() {
+                return Priority.IMMEDIATE;
+            }
+
+            @Override
+            public ClusterState execute(ClusterState currentState) throws Exception {
+                started.countDown();
+                CountDownLatch latch = disruptionLatch.get();
+                if (latch != null) {
+                    try {
+                        latch.await();
+                    } catch (InterruptedException e) {
+                        Throwables.rethrow(e);
+                    }
+                }
+                return currentState;
+            }
+
+            @Override
+            public void onFailure(String source, Exception e) {
+                logger.error("unexpected error during disruption", e);
+            }
+        });
+        try {
+            started.await();
+        } catch (InterruptedException e) {
+        }
+    }
+
+    @Override
+    public void stopDisrupting() {
+        CountDownLatch latch = disruptionLatch.get();
+        if (latch != null) {
+            latch.countDown();
+        }
+
+    }
+
+    @Override
+    public void removeAndEnsureHealthy(InternalTestCluster cluster) {
+        removeFromCluster(cluster);
+    }
+
+    @Override
+    public TimeValue expectedTimeToHeal() {
+        return TimeValue.timeValueMinutes(0);
+    }
+}
diff --git a/x-pack/plugin/watcher/src/test/java/org/elasticsearch/xpack/watcher/WatcherIndexingListenerTests.java b/x-pack/plugin/watcher/src/test/java/org/elasticsearch/xpack/watcher/WatcherIndexingListenerTests.java
index 582ef6abe1e5..752f97b6b1a2 100644
--- a/x-pack/plugin/watcher/src/test/java/org/elasticsearch/xpack/watcher/WatcherIndexingListenerTests.java
+++ b/x-pack/plugin/watcher/src/test/java/org/elasticsearch/xpack/watcher/WatcherIndexingListenerTests.java
@@ -31,7 +31,6 @@
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.VersionUtils;
 import org.elasticsearch.xpack.core.watcher.watch.ClockMock;
 import org.elasticsearch.xpack.core.watcher.watch.Watch;
 import org.elasticsearch.xpack.core.watcher.watch.WatchStatus;
@@ -230,7 +229,7 @@ public void testPostIndexDontInvokeForOtherDocuments() throws Exception {
         when(operation.id()).thenReturn("_id");
         when(operation.type()).thenReturn(Watch.DOC_TYPE);
         when(shardId.getIndexName()).thenReturn("anything");
-        when(result.hasFailure()).thenReturn(false);
+        when(result.getResultType()).thenReturn(Engine.Result.Type.SUCCESS);
 
         listener.postIndex(shardId, operation, new ElasticsearchParseException("whatever"));
         verifyZeroInteractions(triggerService);
@@ -730,4 +729,4 @@ private static DiscoveryNode newNode(String nodeId) {
         return new DiscoveryNode(nodeId, ESTestCase.buildNewFakeTransportAddress(), Collections.emptyMap(),
                 new HashSet<>(asList(DiscoveryNode.Role.values())), Version.CURRENT);
     }
-}
\ No newline at end of file
+}
