diff --git a/src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java b/src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java
index 7fe8849ce27d..d9cacd94f302 100644
--- a/src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java
+++ b/src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java
@@ -674,7 +674,15 @@ protected Searcher newSearcher(String source, IndexSearcher searcher, SearcherMa
 
     @Override
     public boolean refreshNeeded() {
-        return dirty;
+        try {
+            // we are either dirty due to a document added or due to a
+            // finished merge - either way we should refresh
+            return dirty || !searcherManager.isSearcherCurrent();
+        } catch (IOException e) {
+            logger.error("failed to access searcher manager", e);
+            failEngine(e);
+            throw new EngineException(shardId, "failed to access searcher manager",e);
+        }
     }
 
     @Override
@@ -706,7 +714,7 @@ public void refresh(Refresh refresh) throws EngineException {
                 // maybeRefresh will only allow one refresh to execute, and the rest will "pass through",
                 // but, we want to make sure not to loose ant refresh calls, if one is taking time
                 synchronized (refreshMutex) {
-                    if (dirty || refresh.force()) {
+                    if (refreshNeeded() || refresh.force()) {
                         // we set dirty to false, even though the refresh hasn't happened yet
                         // as the refresh only holds for data indexed before it. Any data indexed during
                         // the refresh will not be part of it and will set the dirty flag back to true
@@ -926,7 +934,7 @@ private void refreshVersioningTable(long time) {
 
     @Override
     public void maybeMerge() throws EngineException {
-        if (!possibleMergeNeeded) {
+        if (!possibleMergeNeeded()) {
             return;
         }
         possibleMergeNeeded = false;
diff --git a/src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java b/src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java
index a20c2ae61a61..c59669ae5382 100644
--- a/src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java
+++ b/src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java
@@ -21,7 +21,6 @@
 
 import org.apache.lucene.index.LogByteSizeMergePolicy;
 import org.apache.lucene.index.MergePolicy;
-import org.apache.lucene.index.SegmentInfos;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.Preconditions;
 import org.elasticsearch.common.inject.Inject;
@@ -31,7 +30,6 @@
 import org.elasticsearch.index.settings.IndexSettingsService;
 import org.elasticsearch.index.store.Store;
 
-import java.io.IOException;
 import java.util.Set;
 import java.util.concurrent.CopyOnWriteArraySet;
 
@@ -41,13 +39,14 @@
 public class LogByteSizeMergePolicyProvider extends AbstractMergePolicyProvider<LogByteSizeMergePolicy> {
 
     private final IndexSettingsService indexSettingsService;
-
+    public static final String MAX_MERGE_BYTE_SIZE_KEY = "index.merge.policy.max_merge_sizes";
+    public static final String MIN_MERGE_BYTE_SIZE_KEY = "index.merge.policy.min_merge_size";
+    public static final String MERGE_FACTORY_KEY = "index.merge.policy.merge_factor";
     private volatile ByteSizeValue minMergeSize;
     private volatile ByteSizeValue maxMergeSize;
     private volatile int mergeFactor;
     private volatile int maxMergeDocs;
     private final boolean calibrateSizeByDeletes;
-    private boolean asyncMerge;
 
     private final Set<CustomLogByteSizeMergePolicy> policies = new CopyOnWriteArraySet<>();
 
@@ -63,21 +62,15 @@ public LogByteSizeMergePolicyProvider(Store store, IndexSettingsService indexSet
         this.mergeFactor = componentSettings.getAsInt("merge_factor", LogByteSizeMergePolicy.DEFAULT_MERGE_FACTOR);
         this.maxMergeDocs = componentSettings.getAsInt("max_merge_docs", LogByteSizeMergePolicy.DEFAULT_MAX_MERGE_DOCS);
         this.calibrateSizeByDeletes = componentSettings.getAsBoolean("calibrate_size_by_deletes", true);
-        this.asyncMerge = indexSettings.getAsBoolean("index.merge.async", true);
-        logger.debug("using [log_bytes_size] merge policy with merge_factor[{}], min_merge_size[{}], max_merge_size[{}], max_merge_docs[{}], calibrate_size_by_deletes[{}], async_merge[{}]",
-                mergeFactor, minMergeSize, maxMergeSize, maxMergeDocs, calibrateSizeByDeletes, asyncMerge);
+        logger.debug("using [log_bytes_size] merge policy with merge_factor[{}], min_merge_size[{}], max_merge_size[{}], max_merge_docs[{}], calibrate_size_by_deletes[{}]",
+                mergeFactor, minMergeSize, maxMergeSize, maxMergeDocs, calibrateSizeByDeletes);
 
         indexSettingsService.addListener(applySettings);
     }
 
     @Override
     public LogByteSizeMergePolicy newMergePolicy() {
-        CustomLogByteSizeMergePolicy mergePolicy;
-        if (asyncMerge) {
-            mergePolicy = new EnableMergeLogByteSizeMergePolicy(this);
-        } else {
-            mergePolicy = new CustomLogByteSizeMergePolicy(this);
-        }
+        final CustomLogByteSizeMergePolicy  mergePolicy = new CustomLogByteSizeMergePolicy(this);
         mergePolicy.setMinMergeMB(minMergeSize.mbFrac());
         mergePolicy.setMaxMergeMB(maxMergeSize.mbFrac());
         mergePolicy.setMergeFactor(mergeFactor);
@@ -173,19 +166,4 @@ public MergePolicy clone() {
         }
     }
 
-    public static class EnableMergeLogByteSizeMergePolicy extends CustomLogByteSizeMergePolicy {
-
-        public EnableMergeLogByteSizeMergePolicy(LogByteSizeMergePolicyProvider provider) {
-            super(provider);
-        }
-
-        @Override
-        public MergeSpecification findMerges(MergeTrigger trigger, SegmentInfos infos) throws IOException {
-            // we don't enable merges while indexing documents, we do them in the background
-            if (trigger == MergeTrigger.SEGMENT_FLUSH) {
-                return null;
-            }
-            return super.findMerges(trigger, infos);
-        }
-    }
 }
diff --git a/src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java b/src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java
index bdb9ad4268f5..94e8573459f5 100644
--- a/src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java
+++ b/src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.index.merge.policy;
 
 import org.apache.lucene.index.LogDocMergePolicy;
-import org.apache.lucene.index.MergePolicy;
-import org.apache.lucene.index.SegmentInfos;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.Preconditions;
 import org.elasticsearch.common.inject.Inject;
@@ -29,7 +27,6 @@
 import org.elasticsearch.index.settings.IndexSettingsService;
 import org.elasticsearch.index.store.Store;
 
-import java.io.IOException;
 import java.util.Set;
 import java.util.concurrent.CopyOnWriteArraySet;
 
@@ -39,12 +36,13 @@
 public class LogDocMergePolicyProvider extends AbstractMergePolicyProvider<LogDocMergePolicy> {
 
     private final IndexSettingsService indexSettingsService;
-
+    public static final String MAX_MERGE_DOCS_KEY = "index.merge.policy.max_merge_docs";
+    public static final String MIN_MERGE_DOCS_KEY = "index.merge.policy.min_merge_docs";
+    public static final String MERGE_FACTORY_KEY = "index.merge.policy.merge_factor";
     private volatile int minMergeDocs;
     private volatile int maxMergeDocs;
     private volatile int mergeFactor;
     private final boolean calibrateSizeByDeletes;
-    private boolean asyncMerge;
 
     private final Set<CustomLogDocMergePolicy> policies = new CopyOnWriteArraySet<>();
 
@@ -60,9 +58,8 @@ public LogDocMergePolicyProvider(Store store, IndexSettingsService indexSettings
         this.maxMergeDocs = componentSettings.getAsInt("max_merge_docs", LogDocMergePolicy.DEFAULT_MAX_MERGE_DOCS);
         this.mergeFactor = componentSettings.getAsInt("merge_factor", LogDocMergePolicy.DEFAULT_MERGE_FACTOR);
         this.calibrateSizeByDeletes = componentSettings.getAsBoolean("calibrate_size_by_deletes", true);
-        this.asyncMerge = indexSettings.getAsBoolean("index.merge.async", true);
-        logger.debug("using [log_doc] merge policy with merge_factor[{}], min_merge_docs[{}], max_merge_docs[{}], calibrate_size_by_deletes[{}], async_merge[{}]",
-                mergeFactor, minMergeDocs, maxMergeDocs, calibrateSizeByDeletes, asyncMerge);
+        logger.debug("using [log_doc] merge policy with merge_factor[{}], min_merge_docs[{}], max_merge_docs[{}], calibrate_size_by_deletes[{}]",
+                mergeFactor, minMergeDocs, maxMergeDocs, calibrateSizeByDeletes);
 
         indexSettingsService.addListener(applySettings);
     }
@@ -74,12 +71,7 @@ public void close() throws ElasticsearchException {
 
     @Override
     public LogDocMergePolicy newMergePolicy() {
-        CustomLogDocMergePolicy mergePolicy;
-        if (asyncMerge) {
-            mergePolicy = new EnableMergeLogDocMergePolicy(this);
-        } else {
-            mergePolicy = new CustomLogDocMergePolicy(this);
-        }
+        final CustomLogDocMergePolicy mergePolicy = new CustomLogDocMergePolicy(this);
         mergePolicy.setMinMergeDocs(minMergeDocs);
         mergePolicy.setMaxMergeDocs(maxMergeDocs);
         mergePolicy.setMergeFactor(mergeFactor);
@@ -150,27 +142,4 @@ public void close() {
             provider.policies.remove(this);
         }
     }
-
-    public static class EnableMergeLogDocMergePolicy extends CustomLogDocMergePolicy {
-
-        public EnableMergeLogDocMergePolicy(LogDocMergePolicyProvider provider) {
-            super(provider);
-        }
-
-        @Override
-        public MergeSpecification findMerges(MergeTrigger trigger, SegmentInfos infos) throws IOException {
-            // we don't enable merges while indexing documents, we do them in the background
-            if (trigger == MergeTrigger.SEGMENT_FLUSH) {
-                return null;
-            }
-            return super.findMerges(trigger, infos);
-        }
-        
-        @Override
-        public MergePolicy clone() {
-            // Lucene IW makes a clone internally but since we hold on to this instance 
-            // the clone will just be the identity.
-            return this;
-        }
-    }
 }
diff --git a/src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java b/src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java
index d68e7f523951..06aef2548fdd 100644
--- a/src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java
+++ b/src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.index.merge.policy;
 
 import org.apache.lucene.index.MergePolicy;
-import org.apache.lucene.index.SegmentInfos;
 import org.apache.lucene.index.TieredMergePolicy;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.inject.Inject;
@@ -30,7 +29,6 @@
 import org.elasticsearch.index.settings.IndexSettingsService;
 import org.elasticsearch.index.store.Store;
 
-import java.io.IOException;
 import java.util.Set;
 import java.util.concurrent.CopyOnWriteArraySet;
 
@@ -47,7 +45,6 @@
     private volatile ByteSizeValue maxMergedSegment;
     private volatile double segmentsPerTier;
     private volatile double reclaimDeletesWeight;
-    private boolean asyncMerge;
 
     private final ApplySettings applySettings = new ApplySettings();
 
@@ -57,7 +54,6 @@
     public TieredMergePolicyProvider(Store store, IndexSettingsService indexSettingsService) {
         super(store);
         this.indexSettingsService = indexSettingsService;
-        this.asyncMerge = indexSettings.getAsBoolean("index.merge.async", true);
         this.forceMergeDeletesPctAllowed = componentSettings.getAsDouble("expunge_deletes_allowed", 10d); // percentage
         this.floorSegment = componentSettings.getAsBytesSize("floor_segment", new ByteSizeValue(2, ByteSizeUnit.MB));
         this.maxMergeAtOnce = componentSettings.getAsInt("max_merge_at_once", 10);
@@ -69,8 +65,8 @@ public TieredMergePolicyProvider(Store store, IndexSettingsService indexSettings
 
         fixSettingsIfNeeded();
 
-        logger.debug("using [tiered] merge policy with expunge_deletes_allowed[{}], floor_segment[{}], max_merge_at_once[{}], max_merge_at_once_explicit[{}], max_merged_segment[{}], segments_per_tier[{}], reclaim_deletes_weight[{}], async_merge[{}]",
-                forceMergeDeletesPctAllowed, floorSegment, maxMergeAtOnce, maxMergeAtOnceExplicit, maxMergedSegment, segmentsPerTier, reclaimDeletesWeight, asyncMerge);
+        logger.debug("using [tiered] merge policy with expunge_deletes_allowed[{}], floor_segment[{}], max_merge_at_once[{}], max_merge_at_once_explicit[{}], max_merged_segment[{}], segments_per_tier[{}], reclaim_deletes_weight[{}]",
+                forceMergeDeletesPctAllowed, floorSegment, maxMergeAtOnce, maxMergeAtOnceExplicit, maxMergedSegment, segmentsPerTier, reclaimDeletesWeight);
 
         indexSettingsService.addListener(applySettings);
     }
@@ -91,12 +87,7 @@ private void fixSettingsIfNeeded() {
 
     @Override
     public TieredMergePolicy newMergePolicy() {
-        CustomTieredMergePolicyProvider mergePolicy;
-        if (asyncMerge) {
-            mergePolicy = new EnableMergeTieredMergePolicyProvider(this);
-        } else {
-            mergePolicy = new CustomTieredMergePolicyProvider(this);
-        }
+        final CustomTieredMergePolicyProvider mergePolicy = new CustomTieredMergePolicyProvider(this);
         mergePolicy.setNoCFSRatio(noCFSRatio);
         mergePolicy.setForceMergeDeletesPctAllowed(forceMergeDeletesPctAllowed);
         mergePolicy.setFloorSegmentMB(floorSegment.mbFrac());
@@ -222,20 +213,4 @@ public MergePolicy clone() {
             return this;
         }
     }
-
-    public static class EnableMergeTieredMergePolicyProvider extends CustomTieredMergePolicyProvider {
-
-        public EnableMergeTieredMergePolicyProvider(TieredMergePolicyProvider provider) {
-            super(provider);
-        }
-
-        @Override
-        public MergePolicy.MergeSpecification findMerges(MergeTrigger trigger, SegmentInfos infos) throws IOException {
-            // we don't enable merges while indexing documents, we do them in the background
-            if (trigger == MergeTrigger.SEGMENT_FLUSH) {
-                return null;
-            }
-            return super.findMerges(trigger, infos);
-        }
-    }
 }
\ No newline at end of file
diff --git a/src/test/java/org/elasticsearch/index/engine/internal/InternalEngineIntegrationTest.java b/src/test/java/org/elasticsearch/index/engine/internal/InternalEngineIntegrationTest.java
index 25d1fee7ee90..6c348e641be1 100644
--- a/src/test/java/org/elasticsearch/index/engine/internal/InternalEngineIntegrationTest.java
+++ b/src/test/java/org/elasticsearch/index/engine/internal/InternalEngineIntegrationTest.java
@@ -152,4 +152,5 @@ private void assertTotalCompoundSegments(int i, int t, String index) {
         assertThat(total, Matchers.equalTo(t));
 
     }
+
 }
diff --git a/src/test/java/org/elasticsearch/index/engine/internal/InternalEngineMergeTests.java b/src/test/java/org/elasticsearch/index/engine/internal/InternalEngineMergeTests.java
new file mode 100644
index 000000000000..ab76b214a940
--- /dev/null
+++ b/src/test/java/org/elasticsearch/index/engine/internal/InternalEngineMergeTests.java
@@ -0,0 +1,94 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.engine.internal;
+
+import com.carrotsearch.randomizedtesting.annotations.Nightly;
+import com.carrotsearch.randomizedtesting.annotations.Seed;
+import com.google.common.base.Predicate;
+import org.apache.lucene.index.LogByteSizeMergePolicy;
+import org.apache.lucene.util.LuceneTestCase;
+import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;
+import org.elasticsearch.action.bulk.BulkRequestBuilder;
+import org.elasticsearch.action.bulk.BulkResponse;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.client.Requests;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.settings.ImmutableSettings;
+import org.elasticsearch.index.merge.policy.LogDocMergePolicyProvider;
+import org.elasticsearch.test.ElasticsearchIntegrationTest;
+import org.hamcrest.Matchers;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.ExecutionException;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+
+/**
+ */
+@ElasticsearchIntegrationTest.ClusterScope(numNodes = 1, scope = ElasticsearchIntegrationTest.Scope.SUITE)
+public class InternalEngineMergeTests extends ElasticsearchIntegrationTest {
+
+    @Test
+    @LuceneTestCase.Slow
+    public void testMergesHappening() throws InterruptedException, IOException, ExecutionException {
+        final int numOfShards = 5;
+        // some settings to keep num segments low
+        assertAcked(prepareCreate("test").setSettings(ImmutableSettings.builder()
+                .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, numOfShards)
+                .put(LogDocMergePolicyProvider.MIN_MERGE_DOCS_KEY, 10)
+                .put(LogDocMergePolicyProvider.MERGE_FACTORY_KEY, 5)
+                .put(LogByteSizeMergePolicy.DEFAULT_MIN_MERGE_MB, 0.5)
+                .build()));
+        long id = 0;
+        final int rounds = scaledRandomIntBetween(50, 300);
+        logger.info("Starting rounds [{}] ", rounds);
+        for (int i = 0; i < rounds; ++i) {
+            final int numDocs = scaledRandomIntBetween(100, 1000);
+            BulkRequestBuilder request = client().prepareBulk();
+            for (int j = 0; j < numDocs; ++j) {
+                request.add(Requests.indexRequest("test").type("type1").id(Long.toString(id++)).source(jsonBuilder().startObject().field("l", randomLong()).endObject()));
+            }
+            BulkResponse response = request.execute().actionGet();
+            refresh();
+            assertNoFailures(response);
+            IndicesStatsResponse stats = client().admin().indices().prepareStats("test").setSegments(true).setMerge(true).get();
+            logger.info("index round [{}] - segments {}, total merges {}, current merge {}", i, stats.getPrimaries().getSegments().getCount(), stats.getPrimaries().getMerge().getTotal(), stats.getPrimaries().getMerge().getCurrent());
+        }
+        awaitBusy(new Predicate<Object>() {
+            @Override
+            public boolean apply(Object input) {
+                IndicesStatsResponse stats = client().admin().indices().prepareStats().setSegments(true).setMerge(true).get();
+                logger.info("numshards {}, segments {}, total merges {}, current merge {}", numOfShards, stats.getPrimaries().getSegments().getCount(), stats.getPrimaries().getMerge().getTotal(), stats.getPrimaries().getMerge().getCurrent());
+                long current = stats.getPrimaries().getMerge().getCurrent();
+                long count = stats.getPrimaries().getSegments().getCount();
+                return count < 50 && current == 0;
+            }
+        });
+        IndicesStatsResponse stats = client().admin().indices().prepareStats().setSegments(true).setMerge(true).get();
+        logger.info("numshards {}, segments {}, total merges {}, current merge {}", numOfShards, stats.getPrimaries().getSegments().getCount(), stats.getPrimaries().getMerge().getTotal(), stats.getPrimaries().getMerge().getCurrent());
+        long count = stats.getPrimaries().getSegments().getCount();
+        assertThat(count, Matchers.lessThanOrEqualTo(50l));
+    }
+
+}
diff --git a/src/test/java/org/elasticsearch/test/cache/recycler/MockBigArrays.java b/src/test/java/org/elasticsearch/test/cache/recycler/MockBigArrays.java
index 873990492da1..435d1127eb4a 100644
--- a/src/test/java/org/elasticsearch/test/cache/recycler/MockBigArrays.java
+++ b/src/test/java/org/elasticsearch/test/cache/recycler/MockBigArrays.java
@@ -294,9 +294,7 @@ protected BigArray getDelegate() {
 
         @Override
         protected void randomizeContent(long from, long to) {
-            for (long i = from; i < to; ++i) {
-                set(i, (byte) random.nextInt(1 << 8));
-            }
+            fill(from, to, (byte) random.nextInt(1 << 8));
         }
 
         @Override
@@ -342,9 +340,7 @@ protected BigArray getDelegate() {
 
         @Override
         protected void randomizeContent(long from, long to) {
-            for (long i = from; i < to; ++i) {
-                set(i, random.nextInt());
-            }
+            fill(from, to, random.nextInt());
         }
 
         @Override
@@ -385,9 +381,7 @@ protected BigArray getDelegate() {
 
         @Override
         protected void randomizeContent(long from, long to) {
-            for (long i = from; i < to; ++i) {
-                set(i, random.nextLong());
-            }
+            fill(from, to, random.nextLong());
         }
 
         @Override
@@ -428,9 +422,7 @@ protected BigArray getDelegate() {
 
         @Override
         protected void randomizeContent(long from, long to) {
-            for (long i = from; i < to; ++i) {
-                set(i, (random.nextFloat() - 0.5f) * 1000);
-            }
+            fill(from, to, (random.nextFloat() - 0.5f) * 1000);
         }
 
         @Override
@@ -471,9 +463,7 @@ protected BigArray getDelegate() {
 
         @Override
         protected void randomizeContent(long from, long to) {
-            for (long i = from; i < to; ++i) {
-                set(i, (random.nextDouble() - 0.5) * 1000);
-            }
+            fill(from, to, (random.nextDouble() - 0.5) * 1000);
         }
 
         @Override
diff --git a/src/test/java/org/elasticsearch/test/cache/recycler/MockPageCacheRecycler.java b/src/test/java/org/elasticsearch/test/cache/recycler/MockPageCacheRecycler.java
index 25145bcb4c9a..e01774aaf1be 100644
--- a/src/test/java/org/elasticsearch/test/cache/recycler/MockPageCacheRecycler.java
+++ b/src/test/java/org/elasticsearch/test/cache/recycler/MockPageCacheRecycler.java
@@ -32,6 +32,7 @@
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.lang.reflect.Array;
+import java.util.Arrays;
 import java.util.Map;
 import java.util.Random;
 import java.util.concurrent.ConcurrentMap;
@@ -85,11 +86,21 @@ public boolean release() throws ElasticsearchException {
                     throw new IllegalStateException("Releasing a page that has not been acquired");
                 }
                 final T ref = v();
-                for (int i = 0; i < Array.getLength(ref); ++i) {
-                    if (ref instanceof Object[]) {
-                        Array.set(ref, i, null);
-                    } else {
-                        Array.set(ref, i, (byte) random.nextInt(256));
+                if (ref instanceof Object[]) {
+                    Arrays.fill((Object[])ref, 0, Array.getLength(ref), null);
+                } else if (ref instanceof byte[]) {
+                    Arrays.fill((byte[])ref, 0, Array.getLength(ref), (byte) random.nextInt(256));
+                } else if (ref instanceof long[]) {
+                    Arrays.fill((long[])ref, 0, Array.getLength(ref), random.nextLong());
+                } else if (ref instanceof int[]) {
+                    Arrays.fill((int[])ref, 0, Array.getLength(ref), random.nextInt());
+                } else if (ref instanceof double[]) {
+                    Arrays.fill((double[])ref, 0, Array.getLength(ref), random.nextDouble() - 0.5);
+                } else if (ref instanceof float[]) {
+                    Arrays.fill((float[])ref, 0, Array.getLength(ref), random.nextFloat() - 0.5f);
+                } else {
+                    for (int i = 0; i < Array.getLength(ref); ++i) {
+                            Array.set(ref, i, (byte) random.nextInt(256));
                     }
                 }
                 return v.release();
@@ -112,7 +123,7 @@ public boolean isRecycled() {
     public V<byte[]> bytePage(boolean clear) {
         final V<byte[]> page = super.bytePage(clear);
         if (!clear) {
-            random.nextBytes(page.v());
+            Arrays.fill(page.v(), 0, page.v().length, (byte)random.nextInt(1<<8));
         }
         return wrap(page);
     }
@@ -121,9 +132,7 @@ public boolean isRecycled() {
     public V<int[]> intPage(boolean clear) {
         final V<int[]> page = super.intPage(clear);
         if (!clear) {
-            for (int i = 0; i < page.v().length; ++i) {
-                page.v()[i] = random.nextInt();
-            }
+            Arrays.fill(page.v(), 0, page.v().length, random.nextInt());
         }
         return wrap(page);
     }
@@ -132,9 +141,7 @@ public boolean isRecycled() {
     public V<long[]> longPage(boolean clear) {
         final V<long[]> page = super.longPage(clear);
         if (!clear) {
-            for (int i = 0; i < page.v().length; ++i) {
-                page.v()[i] = random.nextLong();
-            }
+            Arrays.fill(page.v(), 0, page.v().length, random.nextLong());
         }
         return wrap(page);
     }
@@ -143,9 +150,7 @@ public boolean isRecycled() {
     public V<double[]> doublePage(boolean clear) {
         final V<double[]> page = super.doublePage(clear);
         if (!clear) {
-            for (int i = 0; i < page.v().length; ++i) {
-                page.v()[i] = random.nextDouble() - 0.5;
-            }
+            Arrays.fill(page.v(), 0, page.v().length, random.nextDouble() - 0.5);
         }
         return wrap(page);
     }
diff --git a/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java b/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
index 2bf525751042..4deb0e467dc4 100644
--- a/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
+++ b/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
@@ -32,6 +32,7 @@
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequestBuilder;
 import org.elasticsearch.action.admin.indices.delete.DeleteIndexResponse;
+import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.action.count.CountResponse;
 import org.elasticsearch.action.get.GetResponse;
 import org.elasticsearch.action.percolate.PercolateResponse;
@@ -202,6 +203,12 @@ public static void assertFailures(SearchResponse searchResponse) {
         assertVersionSerializable(searchResponse);
     }
 
+    public static void assertNoFailures(BulkResponse response) {
+        assertThat("Unexpected ShardFailures: " + response.buildFailureMessage(),
+                response.hasFailures(), is(false));
+        assertVersionSerializable(response);
+    }
+
     public static void assertFailures(SearchRequestBuilder searchRequestBuilder, RestStatus restStatus, Matcher<String> reasonMatcher) {
         //when the number for shards is randomized and we expect failures
         //we can either run into partial or total failures depending on the current number of shards
@@ -513,4 +520,5 @@ public boolean apply(Object input) {
             MockDirectoryHelper.wrappers.clear();
         }
     }
+
 }
