diff --git a/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java b/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
index 52ffb10fe070..9b17068c9219 100644
--- a/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
+++ b/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.snapshots.blobstore;
 
 import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.store.*;
@@ -49,10 +50,7 @@
 import java.io.FilterInputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicInteger;
@@ -716,9 +714,9 @@ public void restore() {
                 long totalSize = 0;
                 int numberOfReusedFiles = 0;
                 long reusedTotalSize = 0;
-                Map<String, StoreFileMetaData> metadata = Collections.emptyMap();
+                Store.MetadataSnapshot recoveryTargetMetadata = Store.MetadataSnapshot.EMPTY;
                 try {
-                    metadata = store.getMetadata().asMap();
+                    recoveryTargetMetadata = store.getMetadata();
                 } catch (CorruptIndexException e) {
                     logger.warn("{} Can't read metadata from store", e, shardId);
                     throw new IndexShardRestoreFailedException(shardId, "Can't restore corrupted shard", e);
@@ -727,33 +725,42 @@ public void restore() {
                     logger.warn("{} Can't read metadata from store", e, shardId);
                 }
 
-                List<FileInfo> filesToRecover = Lists.newArrayList();
-                for (FileInfo fileInfo : snapshot.indexFiles()) {
-                    String fileName = fileInfo.physicalName();
-                    final StoreFileMetaData md = metadata.get(fileName);
+                final List<FileInfo> filesToRecover = Lists.newArrayList();
+                final Map<String, StoreFileMetaData> snapshotMetaData = new HashMap<>();
+                final Map<String, FileInfo> fileInfos = new HashMap<>();
+
+                for (final FileInfo fileInfo : snapshot.indexFiles()) {
+                    snapshotMetaData.put(fileInfo.metadata().name(), fileInfo.metadata());
+                    fileInfos.put(fileInfo.metadata().name(), fileInfo);
+                }
+                final Store.MetadataSnapshot sourceMetaData = new Store.MetadataSnapshot(snapshotMetaData);
+                final Store.RecoveryDiff diff = sourceMetaData.recoveryDiff(recoveryTargetMetadata);
+                for (StoreFileMetaData md : diff.identical) {
+                    FileInfo fileInfo = fileInfos.get(md.name());
                     numberOfFiles++;
-                    if (md != null && fileInfo.isSame(md)) {
-                        totalSize += md.length();
-                        numberOfReusedFiles++;
-                        reusedTotalSize += md.length();
-                        recoveryState.getIndex().addReusedFileDetail(fileInfo.name(), fileInfo.length());
-                        if (logger.isTraceEnabled()) {
-                            logger.trace("[{}] [{}] not_recovering [{}] from [{}], exists in local store and is same", shardId, snapshotId, fileInfo.physicalName(), fileInfo.name());
-                        }
-                    } else {
-                        totalSize += fileInfo.length();
-                        filesToRecover.add(fileInfo);
-                        recoveryState.getIndex().addFileDetail(fileInfo.name(), fileInfo.length());
-                        if (logger.isTraceEnabled()) {
-                            if (md == null) {
-                                logger.trace("[{}] [{}] recovering [{}] from [{}], does not exists in local store", shardId, snapshotId, fileInfo.physicalName(), fileInfo.name());
-                            } else {
-                                logger.trace("[{}] [{}] recovering [{}] from [{}], exists in local store but is different", shardId, snapshotId, fileInfo.physicalName(), fileInfo.name());
-                            }
-                        }
+                    totalSize += md.length();
+                    numberOfReusedFiles++;
+                    reusedTotalSize += md.length();
+                    recoveryState.getIndex().addReusedFileDetail(fileInfo.name(), fileInfo.length());
+                    if (logger.isTraceEnabled()) {
+                        logger.trace("[{}] [{}] not_recovering [{}] from [{}], exists in local store and is same", shardId, snapshotId, fileInfo.physicalName(), fileInfo.name());
                     }
                 }
 
+                for (StoreFileMetaData md : Iterables.concat(diff.different, diff.missing)) {
+                    FileInfo fileInfo = fileInfos.get(md.name());
+                    numberOfFiles++;
+                    totalSize += fileInfo.length();
+                    filesToRecover.add(fileInfo);
+                    recoveryState.getIndex().addFileDetail(fileInfo.name(), fileInfo.length());
+                    if (logger.isTraceEnabled()) {
+                        if (md == null) {
+                            logger.trace("[{}] [{}] recovering [{}] from [{}], does not exists in local store", shardId, snapshotId, fileInfo.physicalName(), fileInfo.name());
+                        } else {
+                            logger.trace("[{}] [{}] recovering [{}] from [{}], exists in local store but is different", shardId, snapshotId, fileInfo.physicalName(), fileInfo.name());
+                        }
+                    }
+                }
                 final RecoveryState.Index index = recoveryState.getIndex();
                 index.totalFileCount(numberOfFiles);
                 index.totalByteCount(totalSize);
diff --git a/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java b/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
index e50a674ad865..b643bf234e50 100644
--- a/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
+++ b/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.snapshots.blobstore;
 
 import com.google.common.collect.ImmutableList;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.Nullable;
@@ -196,6 +197,7 @@ public boolean isSame(StoreFileMetaData md) {
             static final XContentBuilderString CHECKSUM = new XContentBuilderString("checksum");
             static final XContentBuilderString PART_SIZE = new XContentBuilderString("part_size");
             static final XContentBuilderString WRITTEN_BY = new XContentBuilderString("written_by");
+            static final XContentBuilderString META_HASH = new XContentBuilderString("meta_hash");
         }
 
         /**
@@ -221,6 +223,10 @@ public static void toXContent(FileInfo file, XContentBuilder builder, ToXContent
             if (file.metadata.writtenBy() != null) {
                 builder.field(Fields.WRITTEN_BY, file.metadata.writtenBy());
             }
+
+            if (file.metadata.hash() != null && file.metadata().hash().length > 0) {
+                builder.field(Fields.META_HASH, file.metadata.hash());
+            }
             builder.endObject();
         }
 
@@ -239,6 +245,7 @@ public static FileInfo fromXContent(XContentParser parser) throws IOException {
             String checksum = null;
             ByteSizeValue partSize = null;
             Version writtenBy = null;
+            BytesRef metaHash = new BytesRef();
             if (token == XContentParser.Token.START_OBJECT) {
                 while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                     if (token == XContentParser.Token.FIELD_NAME) {
@@ -257,6 +264,10 @@ public static FileInfo fromXContent(XContentParser parser) throws IOException {
                                 partSize = new ByteSizeValue(parser.longValue());
                             } else if ("written_by".equals(currentFieldName)) {
                                 writtenBy = Lucene.parseVersionLenient(parser.text(), null);
+                            } else if ("meta_hash".equals(currentFieldName)) {
+                                metaHash.bytes = parser.binaryValue();
+                                metaHash.offset = 0;
+                                metaHash.length = metaHash.bytes.length;
                             } else {
                                 throw new ElasticsearchParseException("unknown parameter [" + currentFieldName + "]");
                             }
@@ -269,7 +280,7 @@ public static FileInfo fromXContent(XContentParser parser) throws IOException {
                 }
             }
             // TODO: Verify???
-            return new FileInfo(name, new StoreFileMetaData(physicalName, length, checksum, writtenBy), partSize);
+            return new FileInfo(name, new StoreFileMetaData(physicalName, length, checksum, writtenBy, metaHash), partSize);
         }
 
     }
diff --git a/src/main/java/org/elasticsearch/index/store/Store.java b/src/main/java/org/elasticsearch/index/store/Store.java
index f1a399e74057..969a6c11d5fd 100644
--- a/src/main/java/org/elasticsearch/index/store/Store.java
+++ b/src/main/java/org/elasticsearch/index/store/Store.java
@@ -21,12 +21,12 @@
 
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Iterables;
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexCommit;
-import org.apache.lucene.index.SegmentCommitInfo;
-import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoFormat;
+import org.apache.lucene.index.*;
 import org.apache.lucene.store.*;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.ExceptionsHelper;
@@ -458,7 +458,17 @@ public String toString() {
      * @see StoreFileMetaData
      */
     public final static class MetadataSnapshot implements Iterable<StoreFileMetaData> {
-        private final ImmutableMap<String, StoreFileMetaData> metadata;
+        private final Map<String, StoreFileMetaData> metadata;
+
+        public static final MetadataSnapshot EMPTY = new MetadataSnapshot();
+
+        public MetadataSnapshot(Map<String, StoreFileMetaData> metadata) {
+            this.metadata = metadata;
+        }
+
+        MetadataSnapshot() {
+            this.metadata = Collections.emptyMap();
+        }
 
         MetadataSnapshot(IndexCommit commit, Directory directory, ESLogger logger) throws IOException {
             metadata = buildMetadata(commit, directory, logger);
@@ -485,7 +495,7 @@ public String toString() {
                     for (String file : info.files()) {
                         String legacyChecksum = checksumMap.get(file);
                         if (version.onOrAfter(Version.LUCENE_4_8) && legacyChecksum == null) {
-                            checksumFromLuceneFile(directory, file, builder, logger, version);
+                            checksumFromLuceneFile(directory, file, builder, logger, version, Lucene46SegmentInfoFormat.SI_EXTENSION.equals(IndexFileNames.getExtension(file)));
                         } else {
                             builder.put(file, new StoreFileMetaData(file, directory.fileLength(file), legacyChecksum, null));
                         }
@@ -494,7 +504,7 @@ public String toString() {
                 final String segmentsFile = segmentCommitInfos.getSegmentsFileName();
                 String legacyChecksum = checksumMap.get(segmentsFile);
                 if (maxVersion.onOrAfter(Version.LUCENE_4_8) && legacyChecksum == null) {
-                    checksumFromLuceneFile(directory, segmentsFile, builder, logger, maxVersion);
+                    checksumFromLuceneFile(directory, segmentsFile, builder, logger, maxVersion, true);
                 } else {
                     builder.put(segmentsFile, new StoreFileMetaData(segmentsFile, directory.fileLength(segmentsFile), legacyChecksum, null));
                 }
@@ -544,19 +554,28 @@ public String toString() {
             }
         }
 
-        private static void checksumFromLuceneFile(Directory directory, String file, ImmutableMap.Builder<String, StoreFileMetaData> builder,  ESLogger logger, Version version) throws IOException {
+        private static void checksumFromLuceneFile(Directory directory, String file, ImmutableMap.Builder<String, StoreFileMetaData> builder,  ESLogger logger, Version version, boolean readFileAsHash) throws IOException {
+            final String checksum;
+            BytesRef fileHash = new BytesRef();
             try (IndexInput in = directory.openInput(file, IOContext.READONCE)) {
                 try {
                     if (in.length() < CodecUtil.footerLength()) {
                         // truncated files trigger IAE if we seek negative... these files are really corrupted though
                         throw new CorruptIndexException("Can't retrieve checksum from file: " + file + " file length must be >= " + CodecUtil.footerLength() + " but was: " + in.length());
                     }
-                    String checksum = digestToString(CodecUtil.retrieveChecksum(in));
-                    builder.put(file, new StoreFileMetaData(file, directory.fileLength(file), checksum, version));
+                    if (readFileAsHash) {
+                        final int len = (int)Math.min(1024 * 1024, in.length()); // for safety we limit this to 1MB
+                        fileHash.bytes = new byte[len];
+                        in.readBytes(fileHash.bytes, 0, len);
+                        fileHash.length = len;
+                    }
+                    checksum = digestToString(CodecUtil.retrieveChecksum(in));
+
                 } catch (Throwable ex) {
                     logger.debug("Can retrieve checksum from file [{}]", ex, file);
                     throw ex;
                 }
+                builder.put(file, new StoreFileMetaData(file, directory.fileLength(file), checksum, version, fileHash));
             }
         }
 
@@ -573,6 +592,134 @@ public StoreFileMetaData get(String name) {
         public Map<String, StoreFileMetaData> asMap() {
             return metadata;
         }
+
+        private static final String DEL_FILE_EXTENSION = "del";  // TODO think about how we can detect if this changes?
+        private static final String FIELD_INFOS_FILE_EXTENSION = "fnm";
+
+        /**
+         * Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the
+         * recovery target and this snapshot as the source. The returned diff will hold a list of files that are:
+         *  <ul>
+         *      <li>identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered</li>
+         *      <li>different: they exist in both snapshots but their they are not identical</li>
+         *      <li>missing: files that exist in the source but not in the target</li>
+         *  </ul>
+         * This method groups file into per-segment files and per-commit files. A file is treated as
+         * identical if and on if all files in it's group are identical. On a per-segment level files for a segment are treated
+         * as identical iff:
+         * <ul>
+         *     <li>all files in this segment have the same checksum</li>
+         *     <li>all files in this segment have the same length</li>
+         *     <li>the segments <tt>.si</tt> files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the <tt>.si</tt> file content as it's hash</li>
+         * </ul>
+         *
+         * The <tt>.si</tt> file contains a lot of diagnostics including a timestamp etc. in the future there might be
+         * unique segment identifiers in there hardening this method further.
+         *
+         * The per-commit files handles very similar. A commit is composed of the <tt>segments_N</tt> files as well as generational files like
+         * deletes (<tt>_x_y.del</tt>) or field-info (<tt>_x_y.fnm</tt>) files. On a per-commit level files for a commit are treated
+         * as identical iff:
+         * <ul>
+         *     <li>all files belonging to this commit have the same checksum</li>
+         *     <li>all files belonging to this commit have the same length</li>
+         *     <li>the segments file <tt>segments_N</tt> files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the <tt>segments_N</tt> file content as it's hash</li>
+         * </ul>
+         *
+         * NOTE: this diff will not contain the <tt>segments.gen</tt> file. This file is omitted on recovery.
+         */
+        public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {
+            final ImmutableList.Builder<StoreFileMetaData> identical =  ImmutableList.builder();
+            final ImmutableList.Builder<StoreFileMetaData> different =  ImmutableList.builder();
+            final ImmutableList.Builder<StoreFileMetaData> missing =  ImmutableList.builder();
+            final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>();
+            final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>();
+
+            for (StoreFileMetaData meta : this) {
+                if (IndexFileNames.SEGMENTS_GEN.equals(meta.name())) {
+                    continue; // we don't need that file at all
+                }
+                final String segmentId = IndexFileNames.parseSegmentName(meta.name());
+                final String extension = IndexFileNames.getExtension(meta.name());
+                assert FIELD_INFOS_FILE_EXTENSION.equals(extension) == false || IndexFileNames.stripExtension(IndexFileNames.stripSegmentName(meta.name())).isEmpty() : "FieldInfos are generational but updateable DV are not supported in elasticsearch";
+                if (IndexFileNames.SEGMENTS.equals(segmentId) || DEL_FILE_EXTENSION.equals(extension)) {
+                        // only treat del files as per-commit files fnm files are generational but only for upgradable DV
+                    perCommitStoreFiles.add(meta);
+                } else {
+                    List<StoreFileMetaData> perSegStoreFiles = perSegment.get(segmentId);
+                    if (perSegStoreFiles == null) {
+                        perSegStoreFiles = new ArrayList<>();
+                        perSegment.put(segmentId, perSegStoreFiles);
+                    }
+                    perSegStoreFiles.add(meta);
+                }
+            }
+            final ArrayList<StoreFileMetaData> identicalFiles = new ArrayList<>();
+            for (List<StoreFileMetaData> segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {
+                identicalFiles.clear();
+                boolean consistent = true;
+                for (StoreFileMetaData meta : segmentFiles) {
+                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name());
+                    if (storeFileMetaData == null) {
+                        consistent = false;
+                        missing.add(meta);
+                    } else if (storeFileMetaData.isSame(meta) == false) {
+                        consistent = false;
+                        different.add(meta);
+                    } else {
+                        identicalFiles.add(meta);
+                    }
+                }
+                if (consistent) {
+                    identical.addAll(identicalFiles);
+                } else {
+                    // make sure all files are added - this can happen if only the deletes are different
+                    different.addAll(identicalFiles);
+                }
+            }
+            RecoveryDiff recoveryDiff = new RecoveryDiff(identical.build(), different.build(), missing.build());
+            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.SEGMENTS_GEN) ? 1: 0)
+                    : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size()  + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.SEGMENTS_GEN) + "]"   ;
+            return recoveryDiff;
+        }
+
+        /**
+         * Returns the number of files in this snapshot
+         */
+        public int size() {
+            return metadata.size();
+        }
+    }
+
+    /**
+     * A class representing the diff between a recovery source and recovery target
+     * @see MetadataSnapshot#recoveryDiff(org.elasticsearch.index.store.Store.MetadataSnapshot)
+     */
+    public static final class RecoveryDiff {
+        /**
+         *  Files that exist in both snapshots and they can be considered the same ie. they don't need to be recovered
+         */
+        public final List<StoreFileMetaData> identical;
+        /**
+         * Files that exist in both snapshots but their they are not identical
+         */
+        public final List<StoreFileMetaData> different;
+        /**
+         * Files that exist in the source but not in the target
+         */
+        public final List<StoreFileMetaData> missing;
+
+        RecoveryDiff(List<StoreFileMetaData> identical, List<StoreFileMetaData> different, List<StoreFileMetaData> missing) {
+            this.identical = identical;
+            this.different = different;
+            this.missing = missing;
+        }
+
+        /**
+         * Returns the sum of the files in this diff.
+         */
+        public int size() {
+            return identical.size() + different.size() + missing.size();
+        }
     }
 
     public final static class LegacyChecksums {
diff --git a/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java b/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java
index 3875c20be2a5..c25c7d5ef4f8 100644
--- a/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java
+++ b/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.store;
 
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -42,22 +43,35 @@
 
     private Version writtenBy;
 
+    private BytesRef hash;
+
     private StoreFileMetaData() {
     }
 
     public StoreFileMetaData(String name, long length) {
-        this(name, length, null, null);
+        this(name, length, null);
+    }
 
+    public StoreFileMetaData(String name, long length, String checksum) {
+        this(name, length, checksum, null, new BytesRef());
     }
 
     public StoreFileMetaData(String name, long length, String checksum, Version writtenBy) {
+        this(name, length, checksum, writtenBy, new BytesRef());
+    }
+
+    public StoreFileMetaData(String name, long length, String checksum, Version writtenBy, BytesRef hash) {
         this.name = name;
         this.length = length;
         this.checksum = checksum;
         this.writtenBy = writtenBy;
+        this.hash = hash;
     }
 
 
+    /**
+     * Returns the name of this file
+     */
     public String name() {
         return name;
     }
@@ -69,6 +83,12 @@ public long length() {
         return length;
     }
 
+    /**
+     * Returns a string representation of the files checksum. Since Lucene 4.8 this is a CRC32 checksum written
+     * by lucene. Previously we use Adler32 on top of Lucene as the checksum algorithm, if {@link #hasLegacyChecksum()} returns
+     * <code>true</code> this is a Adler32 checksum.
+     * @return
+     */
     @Nullable
     public String checksum() {
         return this.checksum;
@@ -81,7 +101,7 @@ public boolean isSame(StoreFileMetaData other) {
         if (checksum == null || other.checksum == null) {
             return false;
         }
-        return length == other.length && checksum.equals(other.checksum);
+        return length == other.length && checksum.equals(other.checksum) && hash.equals(other.hash);
     }
 
     public static StoreFileMetaData readStoreFileMetaData(StreamInput in) throws IOException {
@@ -104,6 +124,9 @@ public void readFrom(StreamInput in) throws IOException {
             String versionString = in.readOptionalString();
             writtenBy = Lucene.parseVersionLenient(versionString, null);
         }
+        if (in.getVersion().onOrAfter(org.elasticsearch.Version.V_1_4_0)) {
+            hash = in.readBytesRef();
+        }
     }
 
     @Override
@@ -114,6 +137,9 @@ public void writeTo(StreamOutput out) throws IOException {
         if (out.getVersion().onOrAfter(org.elasticsearch.Version.V_1_3_0)) {
             out.writeOptionalString(writtenBy == null ? null : writtenBy.name());
         }
+        if (out.getVersion().onOrAfter(org.elasticsearch.Version.V_1_4_0)) {
+            out.writeBytesRef(hash);
+        }
     }
 
     /**
@@ -130,4 +156,12 @@ public Version writtenBy() {
     public boolean hasLegacyChecksum() {
         return checksum != null && ((writtenBy != null  && writtenBy.onOrAfter(Version.LUCENE_4_8)) == false);
     }
+
+    /**
+     * Returns a variable length hash of the file represented by this metadata object. This can be the file
+     * itself if the file is small enough. If the length of the hash is <tt>0</tt> no hash value is available
+     */
+    public BytesRef hash() {
+        return hash;
+    }
 }
diff --git a/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java b/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
index 5f49c1a29f0d..9a8a415de42c 100644
--- a/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
+++ b/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.indices.recovery;
 
+import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
 import org.apache.lucene.index.CorruptIndexException;
@@ -141,35 +142,32 @@ public void phase1(final SnapshotIndexCommit snapshot) throws ElasticsearchExcep
                 store.incRef();
                 try {
                     StopWatch stopWatch = new StopWatch().start();
-                    final Store.MetadataSnapshot metadata;
-                    metadata = store.getMetadata(snapshot);
+                    final Store.MetadataSnapshot recoverySourceMetadata = store.getMetadata(snapshot);
                     for (String name : snapshot.getFiles()) {
-                        final StoreFileMetaData md = metadata.get(name);
+                        final StoreFileMetaData md = recoverySourceMetadata.get(name);
                         if (md == null) {
-                            logger.info("Snapshot differs from actual index for file: {} meta: {}", name, metadata.asMap());
-                            throw new CorruptIndexException("Snapshot differs from actual index - maybe index was removed metadata has " + metadata.asMap().size() + " files");
+                            logger.info("Snapshot differs from actual index for file: {} meta: {}", name, recoverySourceMetadata.asMap());
+                            throw new CorruptIndexException("Snapshot differs from actual index - maybe index was removed metadata has " + recoverySourceMetadata.asMap().size() + " files");
                         }
-                        boolean useExisting = false;
-                        if (request.existingFiles().containsKey(name)) {
-                            if (md.isSame(request.existingFiles().get(name))) {
-                                response.phase1ExistingFileNames.add(name);
-                                response.phase1ExistingFileSizes.add(md.length());
-                                existingTotalSize += md.length();
-                                useExisting = true;
-                                if (logger.isTraceEnabled()) {
-                                    logger.trace("[{}][{}] recovery [phase1] to {}: not recovering [{}], exists in local store and has checksum [{}], size [{}]", request.shardId().index().name(), request.shardId().id(), request.targetNode(), name, md.checksum(), md.length());
-                                }
-                            }
+                    }
+                    final Store.RecoveryDiff diff = recoverySourceMetadata.recoveryDiff(new Store.MetadataSnapshot(request.existingFiles()));
+                    for (StoreFileMetaData md : diff.identical) {
+                        response.phase1ExistingFileNames.add(md.name());
+                        response.phase1ExistingFileSizes.add(md.length());
+                        existingTotalSize += md.length();
+                        if (logger.isTraceEnabled()) {
+                            logger.trace("[{}][{}] recovery [phase1] to {}: not recovering [{}], exists in local store and has checksum [{}], size [{}]", request.shardId().index().name(), request.shardId().id(), request.targetNode(), md.name(), md.checksum(), md.length());
                         }
-                        if (!useExisting) {
-                            if (request.existingFiles().containsKey(name)) {
-                                logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], exists in local store, but is different: remote [{}], local [{}]", request.shardId().index().name(), request.shardId().id(), request.targetNode(), name, request.existingFiles().get(name), md);
-                            } else {
-                                logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], does not exists in remote", request.shardId().index().name(), request.shardId().id(), request.targetNode(), name);
-                            }
-                            response.phase1FileNames.add(name);
-                            response.phase1FileSizes.add(md.length());
+                        totalSize += md.length();
+                    }
+                    for (StoreFileMetaData md : Iterables.concat(diff.different, diff.missing)) {
+                        if (request.existingFiles().containsKey(md.name())) {
+                            logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], exists in local store, but is different: remote [{}], local [{}]", request.shardId().index().name(), request.shardId().id(), request.targetNode(), md.name(), request.existingFiles().get(md.name()), md);
+                        } else {
+                            logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], does not exists in remote", request.shardId().index().name(), request.shardId().id(), request.targetNode(), md.name());
                         }
+                        response.phase1FileNames.add(md.name());
+                        response.phase1FileSizes.add(md.length());
                         totalSize += md.length();
                     }
                     response.phase1TotalSize = totalSize;
@@ -199,7 +197,7 @@ public void phase1(final SnapshotIndexCommit snapshot) throws ElasticsearchExcep
                             public void run() {
                                 IndexInput indexInput = null;
                                 store.incRef();
-                                final StoreFileMetaData md = metadata.get(name);
+                                final StoreFileMetaData md = recoverySourceMetadata.get(name);
                                 try {
                                     final int BUFFER_SIZE = (int) recoverySettings.fileChunkSize().bytes();
                                     byte[] buf = new byte[BUFFER_SIZE];
diff --git a/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTest.java b/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTest.java
new file mode 100644
index 000000000000..d16a9db3fda4
--- /dev/null
+++ b/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTest.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.snapshots.blobstore;
+
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.common.xcontent.*;
+import org.elasticsearch.index.store.StoreFileMetaData;
+import org.elasticsearch.test.ElasticsearchTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+
+/**
+ */
+public class FileInfoTest extends ElasticsearchTestCase {
+
+    @Test
+    public void testToFromXContent() throws IOException {
+        final int iters = scaledRandomIntBetween(1, 10);
+        for (int iter = 0; iter < iters; iter++) {
+            final BytesRef hash = new BytesRef(scaledRandomIntBetween(0, 1024 * 1024));
+            hash.length = hash.bytes.length;
+            for (int i = 0; i < hash.length; i++) {
+                hash.bytes[i] = randomByte();
+            }
+            StoreFileMetaData meta = new StoreFileMetaData("foobar", randomInt(), randomAsciiOfLengthBetween(1, 10), TEST_VERSION_CURRENT, hash);
+            ByteSizeValue size = new ByteSizeValue(Math.max(0,Math.abs(randomLong())));
+            BlobStoreIndexShardSnapshot.FileInfo info = new BlobStoreIndexShardSnapshot.FileInfo("_foobar", meta, size);
+            XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON).prettyPrint();
+            BlobStoreIndexShardSnapshot.FileInfo.toXContent(info, builder, ToXContent.EMPTY_PARAMS);
+            byte[] xcontent = builder.bytes().toBytes();
+
+            final BlobStoreIndexShardSnapshot.FileInfo parsedInfo;
+            try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(xcontent)) {
+                parser.nextToken();
+                parsedInfo = BlobStoreIndexShardSnapshot.FileInfo.fromXContent(parser);
+            }
+            assertThat(info.name(), equalTo(parsedInfo.name()));
+            assertThat(info.physicalName(), equalTo(parsedInfo.physicalName()));
+            assertThat(info.length(), equalTo(parsedInfo.length()));
+            assertThat(info.checksum(), equalTo(parsedInfo.checksum()));
+            assertThat(info.partBytes(), equalTo(parsedInfo.partBytes()));
+            assertThat(parsedInfo.metadata().hash().length, equalTo(hash.length));
+            assertThat(parsedInfo.metadata().hash(), equalTo(hash));
+            assertThat(parsedInfo.metadata().writtenBy(), equalTo(TEST_VERSION_CURRENT));
+            assertThat(parsedInfo.isSame(info.metadata()), is(true));
+        }
+    }
+}
diff --git a/src/test/java/org/elasticsearch/index/store/StoreTest.java b/src/test/java/org/elasticsearch/index/store/StoreTest.java
index 0af6e4a0a937..2d31689c5b65 100644
--- a/src/test/java/org/elasticsearch/index/store/StoreTest.java
+++ b/src/test/java/org/elasticsearch/index/store/StoreTest.java
@@ -20,10 +20,7 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.document.TextField;
+import org.apache.lucene.document.*;
 import org.apache.lucene.index.*;
 import org.apache.lucene.store.*;
 import org.apache.lucene.util.BytesRef;
@@ -41,9 +38,7 @@
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.nio.file.NoSuchFileException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Map;
+import java.util.*;
 
 import static com.carrotsearch.randomizedtesting.RandomizedTest.randomInt;
 import static com.carrotsearch.randomizedtesting.RandomizedTest.randomIntBetween;
@@ -110,7 +105,7 @@ public void testVerifyingIndexOutputWithBogusInput() throws IOException {
     @Test
     public void testWriteLegacyChecksums() throws IOException {
         final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService();
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
         Store store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(directoryService));
         // set default codec - all segments need checksums
         IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(actualDefaultCodec()));
@@ -173,7 +168,7 @@ public void testWriteLegacyChecksums() throws IOException {
     @Test
     public void testNewChecksums() throws IOException {
         final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService();
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
         Store store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(directoryService));
         // set default codec - all segments need checksums
         IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(actualDefaultCodec()));
@@ -213,6 +208,9 @@ public void testNewChecksums() throws IOException {
                 assertThat("File: " + meta.name() + " has a different checksum", meta.checksum(), equalTo(checksum));
                 assertThat(meta.hasLegacyChecksum(), equalTo(false));
                 assertThat(meta.writtenBy(), equalTo(TEST_VERSION_CURRENT));
+                if (meta.name().endsWith(".si") || meta.name().startsWith("segments_")) {
+                    assertThat(meta.hash().length, greaterThan(0));
+                }
             }
         }
         assertConsistent(store, metadata);
@@ -225,7 +223,7 @@ public void testNewChecksums() throws IOException {
     @Test
     public void testMixedChecksums() throws IOException {
         final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService();
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
         Store store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(directoryService));
         // this time random codec....
         IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(actualDefaultCodec()));
@@ -311,7 +309,7 @@ public void testMixedChecksums() throws IOException {
     @Test
     public void testRenameFile() throws IOException {
         final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(false);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random(), false);
         Store store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(directoryService));
         {
             IndexOutput output = store.directory().createOutput("foo.bar", IOContext.DEFAULT);
@@ -450,20 +448,22 @@ public void assertDeleteContent(Store store,DirectoryService service) throws IOE
         }
     }
 
-    private  final class LuceneManagedDirectoryService implements DirectoryService {
+    private static final class LuceneManagedDirectoryService implements DirectoryService {
         private final Directory[] dirs;
+        private final Random random;
 
-        public LuceneManagedDirectoryService() {
-            this(true);
+        public LuceneManagedDirectoryService(Random random) {
+            this(random, true);
         }
-        public LuceneManagedDirectoryService(boolean preventDoubleWrite) {
-            this.dirs = new Directory[1 + random().nextInt(5)];
+        public LuceneManagedDirectoryService(Random random, boolean preventDoubleWrite) {
+            this.dirs = new Directory[1 + random.nextInt(5)];
             for (int i = 0; i < dirs.length; i++) {
-                dirs[i]  = newDirectory();
+                dirs[i]  = newDirectory(random);
                 if (dirs[i] instanceof MockDirectoryWrapper) {
                     ((MockDirectoryWrapper)dirs[i]).setPreventDoubleWrite(preventDoubleWrite);
                 }
             }
+            this.random = random;
         }
         @Override
         public Directory[] build() throws IOException {
@@ -472,7 +472,7 @@ public LuceneManagedDirectoryService(boolean preventDoubleWrite) {
 
         @Override
         public long throttleTimeInNanos() {
-            return random().nextInt(1000);
+            return random.nextInt(1000);
         }
 
         @Override
@@ -498,9 +498,159 @@ public static void assertConsistent(Store store, Store.MetadataSnapshot metadata
             }
         }
     }
-
     private Distributor randomDistributor(DirectoryService service) throws IOException {
-        return random().nextBoolean() ? new LeastUsedDistributor(service) : new RandomWeightedDistributor(service);
+        return randomDistributor(random(), service);
+    }
+
+    private Distributor randomDistributor(Random random, DirectoryService service) throws IOException {
+        return random.nextBoolean() ? new LeastUsedDistributor(service) : new RandomWeightedDistributor(service);
+    }
+
+
+    @Test
+    public void testRecoveryDiff() throws IOException, InterruptedException {
+        int numDocs = 2 + random().nextInt(100);
+        List<Document> docs = new ArrayList<>();
+        for (int i = 0; i < numDocs; i++) {
+            Document doc = new Document();
+            doc.add(new StringField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
+            docs.add(doc);
+        }
+        long seed = random().nextLong();
+        Store.MetadataSnapshot first;
+        {
+            Random random = new Random(seed);
+            IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(actualDefaultCodec());
+            iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+            iwc.setUseCompoundFile(random.nextBoolean());
+            iwc.setMaxThreadStates(1);
+            final ShardId shardId = new ShardId(new Index("index"), 1);
+            DirectoryService directoryService = new LuceneManagedDirectoryService(random);
+            Store store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(random, directoryService));
+            IndexWriter writer = new IndexWriter(store.directory(), iwc);
+            final boolean lotsOfSegments = rarely(random);
+            for (Document d : docs) {
+                writer.addDocument(d);
+                if (lotsOfSegments && random.nextBoolean()) {
+                    writer.commit();
+                } else if (rarely(random)) {
+                    writer.commit();
+                }
+            }
+            writer.close();
+            first = store.getMetadata();
+            assertDeleteContent(store, directoryService);
+            store.close();
+        }
+        long time = new Date().getTime();
+        while(time == new Date().getTime()) {
+            Thread.sleep(10); // bump the time
+        }
+        Store.MetadataSnapshot second;
+        Store store;
+        {
+            Random random = new Random(seed);
+            IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(actualDefaultCodec());
+            iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+            iwc.setUseCompoundFile(random.nextBoolean());
+            iwc.setMaxThreadStates(1);
+            final ShardId shardId = new ShardId(new Index("index"), 1);
+            DirectoryService directoryService = new LuceneManagedDirectoryService(random);
+            store = new Store(shardId, ImmutableSettings.EMPTY, null, null, directoryService, randomDistributor(random, directoryService));
+            IndexWriter writer = new IndexWriter(store.directory(), iwc);
+            final boolean lotsOfSegments = rarely(random);
+            for (Document d : docs) {
+                writer.addDocument(d);
+                if (lotsOfSegments && random.nextBoolean()) {
+                    writer.commit();
+                } else if (rarely(random)) {
+                    writer.commit();
+                }
+            }
+            writer.close();
+            second = store.getMetadata();
+        }
+        Store.RecoveryDiff diff = first.recoveryDiff(second);
+        assertThat(first.size(), equalTo(second.size()));
+        for (StoreFileMetaData md : first) {
+            assertThat(second.get(md.name()), notNullValue());
+            // si files are different - containing timestamps etc
+            assertThat(second.get(md.name()).isSame(md), equalTo(md.name().endsWith(".si") == false));
+        }
+        assertThat(diff.different.size(), equalTo(first.size()-1));
+        assertThat(diff.identical.size(), equalTo(1)); // commit point is identical
+        assertThat(diff.missing, empty());
+
+        // check the self diff
+        Store.RecoveryDiff selfDiff = first.recoveryDiff(first);
+        assertThat(selfDiff.identical.size(), equalTo(first.size()));
+        assertThat(selfDiff.different, empty());
+        assertThat(selfDiff.missing, empty());
+
+
+        // lets add some deletes
+        Random random = new Random(seed);
+        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(actualDefaultCodec());
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        iwc.setUseCompoundFile(random.nextBoolean());
+        iwc.setMaxThreadStates(1);
+        iwc.setOpenMode(IndexWriterConfig.OpenMode.APPEND);
+        IndexWriter writer = new IndexWriter(store.directory(), iwc);
+        writer.deleteDocuments(new Term("id", Integer.toString(random().nextInt(numDocs))));
+        writer.close();
+        Store.MetadataSnapshot metadata = store.getMetadata();
+        StoreFileMetaData delFile = null;
+        for (StoreFileMetaData md : metadata) {
+            if (md.name().endsWith(".del")) {
+                delFile = md;
+                break;
+            }
+        }
+        Store.RecoveryDiff afterDeleteDiff = metadata.recoveryDiff(second);
+        if (delFile != null) {
+            assertThat(afterDeleteDiff.identical.size(), equalTo(metadata.size()-2)); // segments_N + del file
+            assertThat(afterDeleteDiff.different.size(), equalTo(0));
+            assertThat(afterDeleteDiff.missing.size(), equalTo(2));
+        } else {
+            // an entire segment must be missing (single doc segment got dropped)
+            assertThat(afterDeleteDiff.identical.size(), greaterThan(0));
+            assertThat(afterDeleteDiff.different.size(), equalTo(0));
+            assertThat(afterDeleteDiff.missing.size(), equalTo(1)); // the commit file is different
+        }
+
+        // check the self diff
+        selfDiff = metadata.recoveryDiff(metadata);
+        assertThat(selfDiff.identical.size(), equalTo(metadata.size()));
+        assertThat(selfDiff.different, empty());
+        assertThat(selfDiff.missing, empty());
+
+        // add a new commit
+        iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(actualDefaultCodec());
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        iwc.setUseCompoundFile(true); // force CFS - easier to test here since we know it will add 3 files
+        iwc.setMaxThreadStates(1);
+        iwc.setOpenMode(IndexWriterConfig.OpenMode.APPEND);
+        writer = new IndexWriter(store.directory(), iwc);
+        writer.addDocument(docs.get(0));
+        writer.close();
+
+        Store.MetadataSnapshot newCommitMetaData = store.getMetadata();
+        Store.RecoveryDiff newCommitDiff = newCommitMetaData.recoveryDiff(metadata);
+        if (delFile != null) {
+            assertThat(newCommitDiff.identical.size(), equalTo(newCommitMetaData.size()-5)); // segments_N, del file, cfs, cfe, si for the new segment
+            assertThat(newCommitDiff.different.size(), equalTo(1)); // the del file must be different
+            assertThat(newCommitDiff.different.get(0).name(), endsWith(".del"));
+            assertThat(newCommitDiff.missing.size(), equalTo(4)); // segments_N,cfs, cfe, si for the new segment
+        } else {
+            assertThat(newCommitDiff.identical.size(), equalTo(newCommitMetaData.size() - 4)); // segments_N, cfs, cfe, si for the new segment
+            assertThat(newCommitDiff.different.size(), equalTo(0));
+            assertThat(newCommitDiff.missing.size(), equalTo(4)); // an entire segment must be missing (single doc segment got dropped)  plus the commit is different
+        }
+
+        store.deleteContent();
+        IOUtils.close(store);
     }
 
 
